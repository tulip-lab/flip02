{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 06: Apache Spark Platform)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n<font color = \"red\" size = 4>Python 2.7 </font>\n\n## Session C - SparkSQL and Data Understanding \n---\n\n### Table of Content\n\nPart A: SparkSQL\n\n* [Loading in a DataFrame](#loadingdataframe)\n\n2. [Creating SQLContext](#sqlcontext)\n\n3. [Creating DataFrame](#creatingdataframe)\n\n4. [Grouping Aggregation](#grouping)\n\n5. [Running SQL Queries](#sql)\n\nPart B: SparkSQL Application\n\n* [Getting the data and creating the RDD](#datardd)\n\n* [Getting a Data Frame](#getdataframe)\n\n* [Queries as DataFrame operations](#sqlquery)\n\n\n---\n\n## Introduction ##\n \nThis notebook will introduce Spark capabilities to deal with data in a structured way. Basically, everything turns around the concept of *Data Frame* and using *SQL language* to query them. We will see how the data frame abstraction, very popular in other data analytics ecosystems (e.g. R and Python/Pandas), it is very powerfull when performing exploratory data analysis. In fact, it is very easy to express data queries when used together with the SQL language. Moreover, Spark distributes this column-based data structure transparently, in order to make the querying process as efficient as possible.      \n \nThis lab session will assume that you have uploaded two data files into cloud, and note down the address:\n- mtcars.csv\n- kddcup_data_10_percent-d8e1d.gz\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"loadingdataframe\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1. Loading in a DataFrame\nTo create a Spark DataFrame we load an external DataFrame, called `mtcars`. This DataFrame includes 32 observations on 11 variables.\n\n[, 1]\tmpg\tMiles/(US) --> gallon  \n[, 2]\tcyl\t--> Number of cylinders  \n[, 3]\tdisp\t--> Displacement (cu.in.)  \n[, 4]\thp -->\tGross horsepower  \n[, 5]\tdrat -->\tRear axle ratio  \n[, 6]\twt -->\tWeight (lb/1000)  \n[, 7]\tqsec -->\t1/4 mile time  \n[, 8]\tvs -->\tV/S  \n[, 9]\tam -->\tTransmission (0 = automatic, 1 = manual)  \n[,10]\tgear -->\tNumber of forward gears  \n[,11]\tcarb -->\tNumber of carburetors  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/mtcars.csv'\nDataSet = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\nmtcars = pd.read_csv('mtcars.csv')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "mtcars.head()"
        }, 
        {
            "source": "<a id = \"sqlcontext\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2. Initialize SQLContext\nTo work with dataframes we need a SQLContext which is created using `SQLContext(sc)`. SQLContext uses SparkContext which has been already created in Data Scientist Workbench, named `sc`. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# sqlContext = SQLContext(sc)"
        }, 
        {
            "source": "<a id = \"creatingdataframe\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3. Creating Spark DataFrames\nWith SQLContext and a loaded local DataFrame, we create a Spark DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf = sqlContext.createDataFrame(mtcars) \nsdf.printSchema()"
        }, 
        {
            "source": "You can also directly load this csv file into a Spark DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf2 = sqlContext.read.format(\"com.databricks.spark.csv\").load(\"mtcars.csv\")"
        }, 
        {
            "source": "#### (3a) Displays the content of the DataFrame \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "sdf.show(5)"
        }, 
        {
            "source": "#### (3b) Selecting columns", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "sdf.select('mpg').show(5)"
        }, 
        {
            "source": "#### (3c)  Filtering Data\nFilter the DataFrame to only retain rows with `mpg` less than 18", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf.filter(sdf['mpg'] < 18).show(5)"
        }, 
        {
            "source": "#### (3d)  Operating on Columns\nSparkR also provides a number of functions that can directly applied to columns for data processing and aggregation. The example below shows the use of basic arithmetic functions to convert lb to metric ton.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf.withColumn('wtTon', sdf['wt'] * 0.45).show(6)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf.show(6)"
        }, 
        {
            "source": "<a id = \"grouping\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 4. Grouping, Aggregation\nSpark DataFrames support a number of commonly used functions to aggregate data after grouping. For example we can compute the average weight of cars by their cylinders as shown below:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sdf.groupby(['cyl'])\\\n.agg({\"wt\": \"AVG\"})\\\n.show(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# We can also sort the output from the aggregation to get the most common cars\ncar_counts = sdf.groupby(['cyl'])\\\n.agg({\"wt\": \"count\"})\\\n.sort(\"count(wt)\", ascending=False)\\\n.show(5)\n"
        }, 
        {
            "source": "<a id = \"sql\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 5. Running SQL Queries from Spark DataFrames\nA Spark DataFrame can also be registered as a temporary table in Spark SQL and registering a DataFrame as a table allows you to run SQL queries over its data. The `sql` function enables applications to run SQL queries programmatically and returns the result as a DataFrame.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Register this DataFrame as a table.\nsdf.registerTempTable(\"cars\")\n\n# SQL statements can be run by using the sql method\nhighgearcars = sqlContext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\nhighgearcars.show(6)\n    "
        }, 
        {
            "source": "## Part B: SparkSQL Application", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"datardd\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 6. Getting the data and creating the RDD", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As we did in previous notebooks, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# import urllib\n# f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\n#link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/kddcup_data_10_percent.gz'\n\nlink_to_data = 'https://github.com/tuliplab/mds/blob/master/Jupyter/data/kddcup.data_10_percent.gz?raw=true'\n#link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/kddcup_data_10_percent-d8e1d.gz'\nDataSet = wget.download(link_to_data, out='kdd.gz')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -l\n!pwd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data_file = \"kdd.gz\"\n\n# assume that the data file has been uploaded to DBFS\n#data_file = \"kddcup_data_10_percent-d8e1d.gz\"\nraw_data = sc.textFile(data_file).cache()"
        }, 
        {
            "source": "<a id = \"getdataframe\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 7. Getting a Data Frame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A Spark `DataFrame` is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as a existing RDD in our case.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The entry point into all SQL functionality in Spark is the `SQLContext` class. To create a basic instance, all we need is a `SparkContext` reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object `sc` for this purpose.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)"
        }, 
        {
            "source": "#### (7a) Inferring the schema", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "With a `SQLContext`, we are ready to create a `DataFrame` from our existing RDD. But first we need to tell Spark SQL the schema in our data.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark SQL can convert an RDD of `Row` objects to a `DataFrame`. Rows are constructed by passing a list of key/value pairs as *kwargs* to the `Row` class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In our case, we first need to split the comma separated data, and then use the information in KDD's 1999 task description to obtain the [column names](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import Row\n\ncsv_data = raw_data.map(lambda l: l.split(\",\"))\nrow_data = csv_data.map(lambda p: Row(\n    duration=int(p[0]), \n    protocol_type=p[1],\n    service=p[2],\n    flag=p[3],\n    src_bytes=int(p[4]),\n    dst_bytes=int(p[5])\n    )\n)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "help(sqlContext)"
        }, 
        {
            "source": "Once we have our RDD of `Row` we can infer and register the schema.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "interactions_df = sqlContext.createDataFrame(row_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "interactions_df.registerTempTable(\"interactions\")"
        }, 
        {
            "source": "Now we can run SQL queries over our data frame that has been registered as a table.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Select tcp network interactions with more than 1 second duration and no transfer from destination\ntcp_interactions = sqlContext.sql(\"\"\"\n    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n\"\"\")\ntcp_interactions.show()"
        }, 
        {
            "source": "The results of SQL queries are RDDs and support all the normal RDD operations.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Output duration together with dst_bytes\ntcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\nfor ti_out in tcp_interactions_out.collect():\n  print ti_out"
        }, 
        {
            "source": "We can easily have a look at our data frame schema using `printSchema`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "interactions_df.printSchema()"
        }, 
        {
            "source": "<a id = \"sqlquery\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 8. Queries as `DataFrame` operations", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark `DataFrame` provides a domain-specific language for structured data manipulation. This language includes methods we can concatenate in order to do selection, filtering, grouping, etc. For example, let's say we want to count how many interactions are there for each protocol type. We can proceed as follows.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from time import time\n\nt0 = time()\ninteractions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()\ntt = time() - t0\n\nprint \"Query performed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "Now imagine that we want to count how many interactions last more than 1 second, with no data transfer from destination, grouped by protocol type. We can just add to filter calls to the previous.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ninteractions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0).groupBy(\"protocol_type\").count().show()\ntt = time() - t0\n\nprint \"Query performed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "We can use this to perform some [exploratory data analysis](http://en.wikipedia.org/wiki/Exploratory_data_analysis). Let's count how many attack and normal interactions we have. First we need to add the label column to our data.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_label_type(label):\n    if label!=\"normal.\":\n        return \"attack\"\n    else:\n        return \"normal\"\n    \nrow_labeled_data = csv_data.map(lambda p: Row(\n    duration=int(p[0]), \n    protocol_type=p[1],\n    service=p[2],\n    flag=p[3],\n    src_bytes=int(p[4]),\n    dst_bytes=int(p[5]),\n    label=get_label_type(p[41])\n    )\n)\ninteractions_labeled_df = sqlContext.createDataFrame(row_labeled_data)"
        }, 
        {
            "source": "This time we don't need to register the schema since we are going to use the OO query interface.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's check the previous actually works by counting attack and normal data in our data frame.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ninteractions_labeled_df.select(\"label\").groupBy(\"label\").count().show()\ntt = time() - t0\n\nprint \"Query performed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "Now we want to count them by label and protocol type, in order to see how important the protocol type is to detect when an interaction is or not an attack.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ninteractions_labeled_df.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\ntt = time() - t0\n\nprint \"Query performed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "At first sight it seems that *udp* interactions are in lower proportion between network attacks versus other protocol types.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "And we can do much more sofisticated groupings. For example, add to the previous a \"split\" based on data transfer from target.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ninteractions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0).count().show()\ntt = time() - t0\n\nprint \"Query performed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "We see how relevant is this new split to determine if a network interaction is an attack.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We will stop here, but we can see how powerfull this type of queries are in order to explore our data. Actually we can replicate all the splits we saw in previous notebooks, when introducing classification trees, just by selecting, groping, and filtering our dataframe. For a more detailed (but less real-world) list of Spark's `DataFrame` operations and data sources, have a look at the oficial documentation [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations).    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}