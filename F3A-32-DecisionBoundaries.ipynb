{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session B - Decision Boundaries\n\n\n### Bayes' Rule:\n\n\n$P(\\omega_j|x) = \\frac{p(x|\\omega_j) * P(\\omega_j)}{p(x)}$ \n\n### Discriminant Functions:\n\nThe goal is to maximize the discriminant function, which we define as the posterior probability here to perform a **minimum-error classification** (Bayes classifier).\n\n$g_1(\\vec{x}) = P(\\omega_1 | \\; \\vec{x}), \\quad  g_2(\\vec{x}) = P(\\omega_2 | \\; \\vec{x})$\n\n$\\Rightarrow g_1(\\vec{x}) = P(\\vec{x}|\\;\\omega_1) \\;\\cdot\\; P(\\omega_1) \\quad | \\; ln \\\\\n\\quad g_2(\\vec{x}) = P(\\vec{x}|\\;\\omega_2) \\;\\cdot\\; P(\\omega_2) \\quad | \\; ln$\n\n<br>\nWe can drop the prior probabilities (since we have equal priors in this case): \n\n$\\Rightarrow g_1(\\vec{x}) = ln(P(\\vec{x}|\\;\\omega_1))\\\\\n\\quad g_2(\\vec{x}) = ln(P(\\vec{x}|\\;\\omega_2))$\n$\\Rightarrow g_1(\\vec{x}) = \\frac{1}{2\\sigma^2} \\bigg[\\; \\vec{x}^{\\,t} - 2 \\vec{\\mu_1}^{\\,t} \\vec{x} + \\vec{\\mu_1}^{\\,t} \\bigg] \\mu_1 \\\\ \n= - \\frac{1}{2} \\bigg[ \\vec{x}^{\\,t} \\vec{x} -2 \\; [0 \\;\\; 0] \\;\\; \\vec{x} +  [0 \\;\\; 0] \\;\\; \\bigg[ \n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array} \\bigg] \\bigg] \\\\\n= -\\frac{1}{2} \\vec{x}^{\\,t} \\vec{x}$\n\n$\\Rightarrow g_2(\\vec{x}) = \\frac{1}{2\\sigma^2} \\bigg[\\; \\vec{x}^{\\,t} - 2 \\vec{\\mu_2}^{\\,t} \\vec{x} + \\vec{\\mu_2}^{\\,t} \\bigg] \\mu_2 \\\\ \n= - \\frac{1}{2} \\bigg[ \\vec{x}^{\\,t} \\vec{x} -2 \\; 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} +  [1 \\;\\; 1] \\;\\; \\bigg[ \n\\begin{array}{c}\n1 \\\\\n1 \\\\\n\\end{array} \\bigg] \\bigg] \\\\\n= -\\frac{1}{2} \\; \\bigg[ \\; \\vec{x}^{\\,t} \\vec{x} - 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} + 2\\; \\bigg] \\;$\n\n### Decision Boundary\n\n$g_1(\\vec{x}) = g_2(\\vec{x})$ \n\n$\\Rightarrow  -\\frac{1}{2} \\vec{x}^{\\,t} \\vec{x} = -\\frac{1}{2} \\; \\bigg[ \\; \\vec{x}^{\\,t} \\vec{x} - 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} + 2\\; \\bigg] \\;$ \n\n$\\Rightarrow -2[1\\;\\; 1] \\vec{x} + 2 = 0$\n\n$\\Rightarrow [-2\\;\\; -2] \\;\\;\\vec{x} + 2 = 0$\n\n$\\Rightarrow -2x_1 - 2x_2 + 2 = 0$\n\n$\\Rightarrow -x_1 - x_2 + 1 = 0$|", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Linear Discriminant Analysis\nIn this part, we will do lda on a synthetic data set. That means we will generate the data ourselves and then fit a linear classifier to this data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step1: Create data set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We are going to sample 500 points each from three 2d gaussian distributions. The means of the three gaussians are $\\mu_1 = [a, b]^T$, $\\mu_2 = [a+2, b+4]^T$ and $\\mu_3 = [a+4, b]^T$ respectively, where **a** is *the last digit of your roll number* and **b** is *second last digit of your roll number*. <br>\nSimilarly the covariance matrices are $\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = I$ <br>\nTo generate points from 2d gaussians, we should first know how to generate random numbers.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### How to generate random numbers?\nuse numpy random package.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\n# code to sample a random number between 0 & 1\n# Try running this multiple times by pressing Ctrl-Enter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom sympy import *\nfrom numpy.linalg import solve\n\nprint (np.random.random())"
        }, 
        {
            "source": "##### How to sample from a gaussian?\nUse randn function to sample from a 1D gaussian with mean 0 and variance 1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (np.random.randn())"
        }, 
        {
            "source": "##### Let's sample 1000 points!\nUse random.normal(mu, sigma, number of points). Let'us assume mean is 3.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "points = np.random.normal(3, 1, 1000)\n# A histogram plot. It looks like a gaussian distribution centered around 3\nplt.hist(points)\nplt.show()"
        }, 
        {
            "source": "##### Generate samples from a 2D gaussian\nUse random.multivariate_normal(mean, cov, 100) to generate 100 points from a multivariate gaussian", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "mean = np.array([3, 3])\ncov = np.eye(2) # the identity matrix\n\npoints = np.random.multivariate_normal(mean, cov, 100)\n# scatter plot with x axis as the first column of points and y axis as the second column\nplt.scatter(points[:, 0], points[:, 1])\nplt.show()"
        }, 
        {
            "source": "#### Sample from three different 2D gaussians\nThe means of the three gaussians should be $\\mu_1 = [a, b]^T$, $\\mu_2 = [a+2, b+4]^T$ and $\\mu_3 = [a+4, b]^T$ respectively, where **a** is *the last digit of your roll number* and **b** is * the second last digit of your roll number*. <br>\nSimilarly the covariance matrices are $\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = I$ <br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#covariance matrix for all 3 distributions\ncov = np.eye(2)\n\nd1 = np.random.multivariate_normal([7, 5], cov, 500)\nd2 = np.random.multivariate_normal([9, 9], cov, 500)\nd3 = np.random.multivariate_normal([11, 5], cov, 500)\n\ndata = np.vstack([d1, d2, d3])\nplt.scatter(d1[:, 0], d1[:, 1], color='red')\nplt.scatter(d2[:, 0], d2[:, 1], color='blue')\nplt.scatter(d3[:, 0], d3[:, 1], color='green')\nplt.show()"
        }, 
        {
            "source": "### Step2: Estimate the Parameters\n##### Estimate 3 means and a covariance matrix from data\nWe have assumed that $\\Sigma = \\sigma^2 I$. <br>\nConvince yourself that the Maximum Likelihood Estimate for $\\sigma^2$ is $\\frac{1}{2n}\\sum\\limits_{i=1}^n (x_i-\\mu)^T(x_i-\\mu)$, where $n$ is the number of samples. <br>\n\nLet's compute the maximum likelihood estimates for the three sets of data points (generated from 3 different gaussians) separately, denote them as $\\hat\\sigma_1^2$, $\\hat\\sigma_2^2$ and $\\hat\\sigma_3^2$ and then take the combined estimate as the averae of the three estimates.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#MLE of mean of the 3 distributions\nm1 = np.mean(d1, axis = 0)\nm2 = np.mean(d2, axis = 0)\nm3 = np.mean(d3, axis = 0)\n\nprint(m1)\nprint(m2)\nprint(m3)\n\n#MLE of covariance of the 3 distributions\n\nt1 = d1 - m1\ns1 = np.trace(np.dot(np.transpose(t1), t1)) / (2*500)\n\nt2 = d2 - m2\ns2 = np.trace(np.dot(np.transpose(t2), t2)) / (2*500)\n\nt3 = d3 - m3\ns3 = np.trace(np.dot(np.transpose(t3), t3)) / (2*500)\n\nprint(s1)\nprint(s2)\nprint(s3)\n\n#Combined estimate - the average of the 3 estimates\ns = (s1 + s2 + s3)/3\nprint(s)"
        }, 
        {
            "source": "### Step3: Draw the Decision Boundaries\nRefer your notes/textbook to convince yourself that in the particular case where all the normal distributions have the same prior and the same covariance matrix of the form $\\sigma^2I$, the discriminant functions are given by $$g_i(x) = \\mu_i^Tx - \\frac{1}{2}\\mu_i^T\\mu_i$$Find the point at which $g_1(x) = g_2(x) = g_3(x)$ <br>\nDraw the three decision boundaries by solving $g_1(x) = g_2(x)$, $g_1(x) =  g_3(x)$ and $g_2(x) = g_3(x)$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.scatter(d1[:, 0], d1[:, 1], color='red')\nplt.scatter(d2[:, 0], d2[:, 1], color='blue')\nplt.scatter(d3[:, 0], d3[:, 1], color='green')\n\n#Solving g1(x)=g2(x) to get the common point of intersection\na = np.array([[m1.item(0)-m2.item(0), m1.item(1)-m2.item(1)],\n              [m1.item(0)-m3.item(0), m1.item(1)-m3.item(1)]])\nb = np.array([0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n                     (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1))),\n              0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n                     (m3.item(0) * m3.item(0)) - (m3.item(1) * m3.item(1)))])\nsol = np.linalg.solve(a, b)\nprint('Common point :')\nprint (sol)\nplt.scatter(sol.item(0), sol.item(1), color='black')\n\n#Plot the decision boundary g1(x)=g2(x)\na = m1.item(0)-m2.item(0)\nb = m1.item(1)-m2.item(1)\nc = 0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n           (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1)))\nx = np.linspace(2, sol.item(0), 20)\ny = (c - (a*x)) /b\nplt.plot(x, y, color='blue')   \n\n#Plot the decision boundary g1(x)=g3(x)\na = m1.item(0)-m3.item(0)\nb = m1.item(1)-m3.item(1)\nc = 0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) -\n           (m3.item(0) * m3.item(0)) - (m3.item(1) * m3.item(1)))\nx = np.linspace(sol.item(0), 9.2, 10)\ny = (c - (a*x)) /b\nplt.plot(x, y, color='red')  \n\n#Plot the decision boundary g3(x)=g2(x)\na = m3.item(0)-m2.item(0)\nb = m3.item(1)-m2.item(1)\nc = 0.5 * ((m3.item(0) * m3.item(0)) + (m3.item(1) * m3.item(1)) -\n           (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1)))\nx = np.linspace(sol.item(0), 15, 20)\ny = (c - (a*x)) /b\nplt.plot(x, y, color='green')   \n\nplt.axis([3,15,0,13])\nplt.show()"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}