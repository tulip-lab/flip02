{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session I - Using optimization routines from scipy and statsmodels", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Finding roots\nFor root finding, we generally need to provide a starting point in the vicinity of the root. For iD root finding, this is often provided as a bracket (a, b) where a and b have opposite signs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import scipy.linalg as la\nimport numpy as np\nimport scipy.optimize as opt\nimport matplotlib.pyplot as plt\nimport pandas as pd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.set_printoptions(precision=3, suppress=True)"
        }, 
        {
            "source": "#### Univariate roots and fixed points", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x):\n    return x**3-3*x+1"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.linspace(-3,3,100)\nplt.axhline(0, c='red')\nplt.plot(x, f(x));"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import brentq, newton"
        }, 
        {
            "source": "brentq is the recommended method", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brentq(f, -3, 0), brentq(f, 0, 1), brentq(f, 1,3)"
        }, 
        {
            "source": "Secant method", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "newton(f, -3), newton(f, 0), newton(f, 3)"
        }, 
        {
            "source": "Newton-Raphson method", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fprime = lambda x: 3*x**2 - 3\nnewton(f, -3, fprime), newton(f, 0, fprime), newton(f, 3, fprime)"
        }, 
        {
            "source": "Analytical solution using sympy to find roots of a polynomial", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sympy import symbols, N, real_roots"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = symbols('x', real=True)\nsol = real_roots(x**3 - 3*x + 1)\nlist(map(N, sol))"
        }, 
        {
            "source": "#### Finding fixed points\n\nFinding the fixed points of a function $g(x)=x$ is the same as finding the roots of $g(x)\u2212x$. However, specialized algorithms also exist - e.g. using scipy.optimize.fixedpoint.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import fixed_point"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.linspace(-3,3,100)\nplt.plot(x, f(x), color='red')\nplt.plot(x, x)\npass"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fixed_point(f, 0), fixed_point(f, -3), fixed_point(f, 3)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "newton(lambda x: f(x) - x, 0), newton(lambda x: f(x) - x, -3), newton(lambda x: f(x) - x, 3)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x, r):\n    \"\"\"Discrete logistic equation.\"\"\"\n    return r*x*(1-x)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "n = 100\nfps = np.zeros(n)\nfor i, r in enumerate(np.linspace(0, 4, n)):\n    fps[i] = fixed_point(f, 0.5, args=(r, ))\n\nplt.plot(np.linspace(0, 4, n), fps)\nplt.axis([0,4,-0.1, 1.1])\npass"
        }, 
        {
            "source": "##### Note that we don\u2019t know anything about the stability of the fixed point\n\nBeyond $r=3$, the fixed point is unstable, even chaotic, but we would never know that just from the plot above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "xs = []\nfor i, r in enumerate(np.linspace(0, 4, 400)):\n    x = 0.5\n    for j in range(10000):\n        x = f(x, r)\n    for j in range(50):\n        x = f(x, r)\n        xs.append((r, x))\nxs = np.array(xs)\nplt.scatter(xs[:,0], xs[:,1], s=1)\nplt.axis([0,4,-0.1, 1.1])\npass"
        }, 
        {
            "source": "### Mutlivariate roots and fixed points\n\nUse root to solve polynomial equations. Use fsolve for non-polynomial equations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import root, fsolve"
        }, 
        {
            "source": "Suppose we want to solve a sysetm of m equations with $n$ unknowns\n\nNote that the equations are non-linear and there can be multiple solutions. These can be interpreted as fixed points of a system of differential equations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x):\n    return [x[1] - 3*x[0]*(x[0]+1)*(x[0]-1),\n            .25*x[0]**2 + x[1]**2 - 1]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sol = root(f, (0.5, 0.5))\nsol.x"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fsolve(f, (0.5, 0.5))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "r0 = opt.root(f,[1,1])\nr1 = opt.root(f,[0,1])\nr2 = opt.root(f,[-1,1.1])\nr3 = opt.root(f,[-1,-1])\nr4 = opt.root(f,[2,-0.5])\n\nroots = np.c_[r0.x, r1.x, r2.x, r3.x, r4.x]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Y, X = np.mgrid[-3:3:100j, -3:3:100j]\nU = Y - 3*X*(X + 1)*(X-1)\nV = .25*X**2 + Y**2 - 1\n\nplt.streamplot(X, Y, U, V, color=U, linewidth=2, cmap=plt.cm.autumn)\nplt.scatter(roots[0], roots[1], s=50, c='none', edgecolors='k', linewidth=2)\npass"
        }, 
        {
            "source": "We can also give the jacobian", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def jac(x):\n    return [[-6*x[0], 1], [0.5*x[0], 2*x[1]]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sol = root(f, (0.5, 0.5), jac=jac)\nsol.x, sol.fun"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Check that values found are really roots\nnp.allclose(f(sol.x), 0)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Starting from other initial conditions, different roots may be found\nsol = root(f, (12,12))\nsol.x"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.allclose(f(sol.x), 0)"
        }, 
        {
            "source": "### Optimization Primer\nWe will assume that our optimization problem is to minimize some univariate or multivariate function $f(x)$. This is without loss of generality, since to find the maximum, we can simply minimize $\u2212f(x)$. We will also assume that we are dealing with multivariate or real-valued smooth functions - non-smooth or discrete functions (e.g. integer-valued) are outside the scope of this course.\n\nTo find the minimum of a function, we first need to be able to express the function as a mathematical expresssion. For example, in least squares regression, the function that we are optimizing is of the form $y_i\u2212f(x_i,\u03b8)$ for some parameter(s) $\u03b8$. To choose an appropirate optimization algorithm, we should at least answer these two questions if possible:\n\n      1.Is the function convex?\n      2.Are there any constraints that the solution must meet?\n\nFinally, we need to realize that optimization methods are nearly always designed to find local optima. For convex problems, there is only one minimum and so this is not a problem. However, if there are multiple local minima, often heuristics such as multiple random starts must be adopted to find a \u201cgood\u201d enough solution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Is the function convex?\n\nConvex functions are very nice because they have a single global minimum, and there are very efficient algorithms for solving large convex systems.\n\nIntuitively, a function is convex if every chord joining two points on the function lies above the function. More formally, a function is convex if\n$$ f(ta+(1\u2212t)b)<tf(a)+(1\u2212t)f(b) $$\n\nfor some t between $0$ and $1$ - this is shown in the figure below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x):\n    return (x-4)**2 + x + 1\n\nwith plt.xkcd():\n    x = np.linspace(0, 10, 100)\n\n    plt.plot(x, f(x))\n    ymin, ymax = plt.ylim()\n    plt.axvline(2, ymin, f(2)/ymax, c='red')\n    plt.axvline(8, ymin, f(8)/ymax, c='red')\n    plt.scatter([4, 4], [f(4), f(2) + ((4-2)/(8-2.))*(f(8)-f(2))],\n                 edgecolor=['blue', 'red'], facecolor='none', s=100, linewidth=2)\n    plt.plot([2,8], [f(2), f(8)])\n    plt.xticks([2,4,8], ('a', 'ta + (1-t)b', 'b'), fontsize=20)\n    plt.text(0.2, 40, 'f(ta + (1-t)b) < tf(a) + (1-t)f(b)', fontsize=20)\n    plt.xlim([0,10])\n    plt.yticks([])\n    plt.suptitle('Convex function', fontsize=20)"
        }, 
        {
            "source": "Warm up exercise\n\nShow that $f(x)=\u2212log(x)$ is a convex function.\nChecking if a function is convex using the Hessian\n\nThe formal definition is only useful for checking if a function is convex if you can find a counter-example. More practically, a twice differentiable function is convex if its Hessian is positive semi-definite, and strictly convex if the Hessian is positive definite.\n\nFor example, suppose we want to minimize the scalar-valued function\n$$f(x_1,x_2,x_3)=x_2^1+2x_2^2+3x^2_3+2x_1x_2+2x_1x_3$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sympy import symbols, hessian, Function, init_printing, expand, Matrix, diff\nx, y, z = symbols('x y z')\nf = symbols('f', cls=Function)\ninit_printing()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "f = x**2 + 2*y**2 + 3*z**2 + 2*x*y + 2*x*z\nf"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "H = hessian(f, (x, y, z))\nH"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "np.real_if_close(la.eigvals(np.array(H).astype('float')))"
        }, 
        {
            "source": "Since the matrix is symmetric and all eigenvalues are positive, the Hessian is positive defintie and the function is convex.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Combining convex functions\n\nThe following rules may be useful to determine if more complex functions are convex:\n\n    1.The intersection of convex functions is convex\n    2.If the functions f and g are convex and a\u22650 and b\u22650 then the function af+bg is convex.\n    3.If the function U is convex and the function g is nondecreasing and convex then the function f defined by f(x)=g(U(x)) is convex.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n##### Are there any constraints that the solution must meet?\n\nIn general, optimization without constraints is easier to perform than optimization in the presence of constraints. The solutions may be very different in the presence or absence of constraints, so it is important to know if there are any constraints.\n\nWe will see some examples of two general strategies: - convert a problem with constraints into one without constraints or - use an algorithm that can optimize with constraints.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Using scipy.optimize\nOne of the most convenient libraries to use is scipy.optimize, since it is already part of the Anaconda installation and it has a fairly intuitive interface.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy import optimize as opt"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x):\n    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.linspace(-8, 5, 100)\nplt.plot(x, f(x));"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "opt.minimize_scalar(f, method='Brent')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "opt.minimize_scalar(f, method='bounded', bounds=[0, 6])"
        }, 
        {
            "source": "##### Local and global minima", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x, offset):\n    return -np.sinc(x-offset)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.linspace(-20, 20, 100)\nplt.plot(x, f(x, 5));"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# note how additional function arguments are passed in\nsol = opt.minimize_scalar(f, args=(5,))\nsol"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot(x, f(x, 5))\nplt.axvline(sol.x, c='red')\npass"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# We can try multiple random starts to find the global minimum\nlower = np.random.uniform(-20, 20, 100)\nupper = lower + 1\nsols = [opt.minimize_scalar(f, args=(5,), bracket=(l, u)) for (l, u) in zip(lower, upper)]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "idx = np.argmin([sol.fun for sol in sols])\nsol = sols[idx]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot(x, f(x, 5))\nplt.axvline(sol.x, c='red');"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import basinhopping"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x0 = 0\nsol = basinhopping(f, x0, stepsize=1, minimizer_kwargs={'args': (5,)})\nsol"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot(x, f(x, 5))\nplt.axvline(sol.x, c='red');"
        }, 
        {
            "source": "Conditioning of optimization problem\n\nWith these values for a\nand b, the problem is ill-conditioned. As we shall see, one of the factors affecting the ease of optimization is the condition number of the curvature (Hessian). When the condition number is high, the gradient may not point in the direction of the minimum, and simple gradient descent methods may be inefficient since they may be forced to take many sharp turns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sympy import symbols, hessian, Function, N\n\nx, y = symbols('x y')\nf = symbols('f', cls=Function)\n\nf = 100*(y - x**2)**2 + (1 - x)**2\n\nH = hessian(f, [x, y]).subs([(x,1), (y,1)])\nH, N(H.condition_number())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# As pointed out in the previous lecture, the condition number is basically the ratio of largest to smallest eigenvalue of the Hessian\nimport scipy.linalg as la\n\nmu = la.eigvals(np.array([802, -400, -400, 200]).reshape((2,2)))\nnp.real_if_close(mu[0]/mu[1])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def rosen(x):\n    \"\"\"Generalized n-dimensional version of the Rosenbrock function\"\"\"\n    return sum(100*(x[1:]-x[:-1]**2.0)**2.0 +(1-x[:-1])**2.0)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Why is the condition number so large?\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = rosen(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Note: the global minimum is at (1,1) in a tiny contour island\nplt.contour(X, Y, Z, np.arange(10)**5, cmap='jet')\nplt.text(1, 1, 'x', va='center', ha='center', color='red', fontsize=20);"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Zooming in to the global minimum at (1,1)\nx = np.linspace(0, 2, 100)\ny = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x, y)\nZ = rosen(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.contour(X, Y, Z, [rosen(np.array([k, k])) for k in np.linspace(1, 1.5, 10)], cmap='jet')\nplt.text(1, 1, 'x', va='center', ha='center', color='red', fontsize=20);"
        }, 
        {
            "source": "### Gradient descent\nThe gradient (or Jacobian) at a point indicates the direction of steepest ascent. Since we are looking for a minimum, one obvious possibility is to take a step in the opposite direction to the gradient. We weight the size of the step by a factor $\u03b1$ known in the machine learning literature as the learning rate. If $\u03b1$ is small, the algorithm will eventually converge towards a local minimum, but it may take long time. If $\u03b1$ is large, the algorithm may converge faster, but it may also overshoot and never find the minimum. Gradient descent is also known as a first order method because it requires calculation of the first derivative at each iteration.\n\nSome algorithms also determine the appropriate value of $\u03b1$ at each stage by using a line search, i.e.,\n$$\u03b1^\u2217=argmin_\u03b1f(x_k\u2212\u03b1\u2207f(x_k))$$\n\nwhich is a 1D optimization problem.\n\nAs suggested above, the problem is that the gradient may not point towards the global minimum especially when the condition number is large, and we are forced to use a small $\u03b1$ for convergence.\n\nLet\u2019s warm up by minimizing a trivial function $f(x,y)=x^2+y^2$ to illustrate the basic idea of gradient descent.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\na = 0.1 # learning rate\nx0 = np.array([1.0,1.0])\nprint('Start', x0)\nfor i in range(41):\n    x0 -= a * grad(x0)\n    if i%5 == 0:\n        print(i, x0)"
        }, 
        {
            "source": "#### Gradient descent for least squares minimization\n\nUsually, when we optimize, we are not just finding the minimum, but also want to know the parameters that give us the minimum. As a simple example, suppose we want to find parameters that minimize the least squares difference between a linear model and some data. Suppose we have some data $(0,1),(1,2),(2,3),(3,3.5),(4,6),(5,9),(6,8)$ and want to find a line $y=\u03b2_0+\u03b2_1x$ that is the best least squares fit. One way to do this is to solve $X^TX\u03b2^^=X^Ty$, but we want to show how this can be formulated as a gradient descent problem.\nWe want to find $\u03b2=(\u03b2_0,\u03b2_1)$ that minimize the squared differences\n$$r=\u2211(\u03b2_0+\u03b2_1x\u2212y)^2$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x, y, b):\n    \"\"\"Helper function.\"\"\"\n    return (b[0] + b[1]*x - y)\n\ndef grad(x, y, b):\n    \"\"\"Gradient of objective function with respect to parameters b.\"\"\"\n    n = len(x)\n    return np.array([\n            sum(f(x, y, b)),\n            sum(x*f(x, y, b))\n    ])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x, y = map(np.array, zip((0,1), (1,2), (2,3), (3,3.5), (4,6), (5,9), (6,8)))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = 0.001 # learning rate\nb = np.zeros(2)\nfor i in range(10000):\n    b -= a * grad(x, y, b)\nb"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.scatter(x, y, s=30)\nplt.plot(x, b[0] + b[1]*x, color='red')\npass"
        }, 
        {
            "source": "### Gradient descent to minimize the Rosen function using scipy.optimize\n\nBecause gradient descent is unreliable in practice, it is not part of the scipy optimize suite of functions, but we will write a custom function below to illustrate how to use gradient descent while maintaining the scipy.optimize interface.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def rosen_der(x):\n    \"\"\"Derivative of generalized Rosen function.\"\"\"\n    xm = x[1:-1]\n    xm_m1 = x[:-2]\n    xm_p1 = x[2:]\n    der = np.zeros_like(x)\n    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n    der[-1] = 200*(x[-1]-x[-2]**2)\n    return der"
        }, 
        {
            "source": "Warning One of the most common causes of failure of optimization is because the gradient or Hessian function is specified incorrectly. You can check for this using check_grad which compares the analytical gradient with one calculated using finite differences.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import check_grad\n\nfor x in np.random.uniform(-2,2,(10,2)):\n    print(x, check_grad(rosen, rosen_der, x))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Writing a custom function for the scipy.optimize interface.\ndef custmin(fun, x0, args=(), maxfev=None, alpha=0.0002,\n        maxiter=100000, tol=1e-10, callback=None, **options):\n    \"\"\"Implements simple gradient descent for the Rosen function.\"\"\"\n    bestx = x0\n    bestf = fun(x0)\n    funcalls = 1\n    niter = 0\n    improved = True\n    stop = False\n\n    while improved and not stop and niter < maxiter:\n        niter += 1\n        # the next 2 lines are gradient descent\n        step = alpha * rosen_der(bestx)\n        bestx = bestx - step\n\n        bestf = fun(bestx)\n        funcalls += 1\n\n        if la.norm(step) < tol:\n            improved = False\n        if callback is not None:\n            callback(bestx)\n        if maxfev is not None and funcalls >= maxfev:\n            stop = True\n            break\n\n    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,\n                              nfev=funcalls, success=(niter > 1))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reporter(p):\n    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n    global ps\n    ps.append(p)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Initial starting position\nx0 = np.array([4,-4.1])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = [x0]\nopt.minimize(rosen, x0, method=custmin, callback=reporter)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = rosen(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = np.array(ps)\nplt.figure(figsize=(12,4))\nplt.subplot(121)\nplt.contour(X, Y, Z, np.arange(10)**5, cmap='jet')\nplt.plot(ps[:, 0], ps[:, 1], '-ro')\nplt.subplot(122)\nplt.semilogy(range(len(ps)), rosen(ps.T));"
        }, 
        {
            "source": "#### First order methods\n\nAs calculating the Hessian is computationally expensive, sometimes first order methods that only use the first derivatives are preferred. Quasi-Newton methods use functions of the first derivatives to approximate the inverse Hessian. A well know example of the Quasi-Newoton class of algorithjms is BFGS, named after the initials of the creators. As usual, the first derivatives can either be provided via the jac= argument or approximated by finite difference methods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = [x0]\nopt.minimize(rosen, x0, method='BFGS', callback=reporter)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = np.array(ps)\nplt.figure(figsize=(12,4))\nplt.subplot(121)\nplt.contour(X, Y, Z, np.arange(10)**5, cmap='jet')\nplt.plot(ps[:, 0], ps[:, 1], '-ro')\nplt.subplot(122)\nplt.semilogy(range(len(ps)), rosen(ps.T));"
        }, 
        {
            "source": "#### Second order methods\n\nSecond order methods solve for H^\u22121 and so require calculation of the Hessian (either provided or approximated using finite differences). For efficiency reasons, the Hessian is not directly inverted, but solved for using a variety of methods such as conjugate gradient. An example of a second order method in the optimize package is Newton-GC.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import rosen, rosen_der, rosen_hess"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = [x0]\nopt.minimize(rosen, x0, method='Newton-CG', jac=rosen_der, hess=rosen_hess, callback=reporter)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = np.array(ps)\nplt.figure(figsize=(12,4))\nplt.subplot(121)\nplt.contour(X, Y, Z, np.arange(10)**5, cmap='jet')\nplt.plot(ps[:, 0], ps[:, 1], '-ro')\nplt.subplot(122)\nplt.semilogy(range(len(ps)), rosen(ps.T));"
        }, 
        {
            "source": "#### Zeroth order methods\n\nFinally, there are some optimization algorithms not based on the Newton method, but on other heuristic search strategies that do not require any derivatives, only function evaluations. One well-known example is the Nelder-Mead simplex algorithm.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = [x0]\nopt.minimize(rosen, x0, method='nelder-mead', callback=reporter)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ps = np.array(ps)\nplt.figure(figsize=(12,4))\nplt.subplot(121)\nplt.contour(X, Y, Z, np.arange(10)**5, cmap='jet')\nplt.plot(ps[:, 0], ps[:, 1], '-ro')\nplt.subplot(122)\nplt.semilogy(range(len(ps)), rosen(ps.T));"
        }, 
        {
            "source": "### Lagrange multipliers and constrained optimization\nRecall why Lagrange multipliers are useful for constrained optimization - a stationary point must be where the constraint surface $g$ touches a level set of the function $f$ (since the value of $f$ does not change on a level set). At that point, $f$ and $g$ are parallel, and hence their gradients are also parallel (since the gradient is normal to the level set). So we want to solve\n$$\u2207f=\u2212\u03bb\u2207g$$\n\nor equivalently,\n$$\u2207f+\u03bb\u2207g=0$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Numerical example of using Lagrange multipliers\n\nMaximize $f(x,y,z)=xy+yz$ subject to the constraints $x+2y=6$ and $x\u22123z=0$\n\nWe set up the equations\n$$F(x,y,z,\u03bb,\u03bc)=xy+yz\u2212\u03bb(x+2y\u22126)\u2212\u03bc(x\u22123z)$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "A = np.array([\n    [0, 1, 0, -1, -1],\n    [1, 0, 1, -2, 0],\n    [0, 1, 0, 0, 3],\n    [1, 2, 0, 0, 0],\n    [1, 0,-3, 0, 0]])\n\nb = np.array([0,0,0,6,0])\n\nsol = la.solve(A, b)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sol"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f(x, y, z):\n    return x*y + y*z"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "f(*sol[:3])"
        }, 
        {
            "source": "### Check using scipy.optimize", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Convert to minimization problem by negating function\ndef f(x):\n    return -(x[0]*x[1] + x[1]*x[2])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cons = ({'type': 'eq',\n         'fun' : lambda x: np.array([x[0] + 2*x[1] - 6, x[0] - 3*x[2]])})"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x0 = np.array([2,2,0.67])\ncx = opt.minimize(f, x0, constraints=cons)\ncx"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}