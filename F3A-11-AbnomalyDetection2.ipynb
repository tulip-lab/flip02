{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "## Modern Data Science \n**(Module 01: A Touch of Data Science)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session I - Abnormality Analytics (2)\n\n**The purpose of this session, which is continued from practical session 3, is to demonstrate advanced methods of abnormality analytics:**\n\n1. Principal component analysis (PCA) based methods\n2. Probabilistic Approach - Gaussian mixture model based methods\n\n** References and additional reading and resources**\n- [Novelty and outlier detection with scikit-learn](http://scikit-learn.org/stable/modules/outlier_detection.html)\n---\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " # <span style=\"color:#0b486b\">1. Revisiting EMNIST dataset</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " *__EMNIST__* dataset contains many of digit images and a few of non-digit images. Our aim is to train a model using this dataset to detect non-digit images. We can apply this model to build a machine to verify a valid phone number written by hand. You now can load EMNIST data (in csv format) and view data properties using the following code. The first column represents labels of data instances. The rest are feature vectors of data instances.\n\nNow you can load data and get some basic information of dataset using <code>info()</code> function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nwget.download('https://github.com/tuliplab/mds/raw/master/Jupyter/data/emnist.digits_letters.small.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('emnist.digits_letters.small.csv',index_col=0)\ndf = df.sort_values(['0'])  # for further visualization\ndf.info()"
        }, 
        {
            "source": "Converting the data frame *df* to a numpy array object *data_array* for using *numpy* array utility methods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data_array = df.as_matrix()\nx = data_array[:,1:]\ny_true = data_array[:,0]\nnum_samples = x.shape[0]\nprint(\"\\nNumber of samples: {}\".format(num_samples))\n"
        }, 
        {
            "source": "Since the dataset contains images, you can sample and plot some digits images (labeled as 1) in the dataset. Note that you need to reshape to a matrix before using `imshow()` to view this image because the feature vector is flattened in 1D.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6) # configure the size of images\n\nimport matplotlib.pyplot as plt \nnum_dig_subplots = 5                                 # the number of digit images plotted\nnum_let_subplots = 5                                 # the number of letter images plotted                                     \n# the number of images plotted\nfig, ax = plt.subplots(1,num_dig_subplots+num_let_subplots)\n# Plotting digits\nfor idx in range(num_dig_subplots):\n    n = np.random.randint(np.sum(y_true < 0), len(y_true))      # randomly choose an image index\n    img1 = x[n,:].reshape((28,28)).T                  # reshape the vector into the image size 28x28\n    ax[idx].imshow(img1, cmap = plt.get_cmap('gray')) # show the selected image\n    \n# Plotting letters\nfor idx in range(num_let_subplots):\n    n = np.random.randint(0,np.sum(y_true < 0))      # randomly choose an image index\n    img1 = x[n,:].reshape((28,28)).T                  # reshape the vector into the image size 28x28\n    ax[idx+num_dig_subplots].imshow(img1, cmap = plt.get_cmap('gray')) # show the selected image\n\nplt.show()\n"
        }, 
        {
            "source": "## Dataset Statistics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "First, we examine the ratio between normal data (labeled as **'1'**) and abnormal data (labeled as **'-1'**). Intuitively, we can see it is an imbalanced dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\n(counts, _) = np.histogram(y_true,2)\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.bar([0,1], counts)\nclasslabels=[\"abnormal\",\"normal\"]\n\nrects =ax.patches\n\n# Now make labels of percentage\nlabels = ['{:.3f}%'.format(i*100) for i in counts/np.sum(counts)]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 2, label, ha='center', va='bottom')\n\nplt.ylabel(\"Count\")\nplt.xticks(np.arange(2),classlabels)\nplt.show()"
        }, 
        {
            "source": "# <span style=\"color:#0b486b\"> 2.   Advanced Methods for Anomaly Detection Systems</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In addition to the algorithms being introduced in previous practical session, we present here two advanced approaches for abnormality  analytics:\n - PCA-based approach\n - Probabilistic approach using Gaussian mixture models (GMM)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## II.1 PCA-based approach", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In  $PCA$  algorithm, we need to find a subspace of  $\ud835\udc51$ principal components to approximate dataset $X$. This original matrix $X$ now can be seen  as the sum of two orthogonal parts: an approximated data matrix $\\tilde{X}$ and a residual data matrix $E$, which is intuitively represented in the following figure.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/PCAResidual.png\" width=\"600\">\n\nIn PCA-based algorithm for abnormality analytics, we use residual data matrix $E$ to detect anomalies. The pseudo-code of the algorithms is\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/PCAResidual_algo.png\" width=\"600\">\n\nNow we step-by-step implement the algorithm. Firstly, we compute covariance matrix", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cov_mat = np.cov(x[y_true > 0,:].T)  # only consider normal data"
        }, 
        {
            "source": "and decompose covariance matrix using SVD method.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "[U, S, V] = np.linalg.svd(cov_mat)"
        }, 
        {
            "source": "We can find the number of principal components with a certain of info kept.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# set model hyper-parameters\nfind_threshold_type = 'percentile'\nkeep_info = 0.999\nalpha = 0.3  # use for find_threshold_type = 'keepinfo'\nnormal_percent = 90  # use for find_threshold_type = 'percentile'\n\ncs = S.cumsum()\nnum_principal_components = int(np.where(cs >= cs[-1]*keep_info)[0][0]+1)\nprint('Number of principal components:', num_principal_components)"
        }, 
        {
            "source": "We calculate the principal subspace and residual subspace, and project data onto residual subspace to get the residual signal.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "U1 = U[:, :num_principal_components]    # principal subspace\nU2 = U[:, num_principal_components:]     # residual subspace\n\nresid = np.power(x.dot(U2.dot(U2.T)),2).sum(axis=1)   # project onto residual subspace"
        }, 
        {
            "source": "A data point is abnormal if residual signal is very high. Thus, by defining a threshold, we can detect anomaly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "threshold = 1.0\n# now we visualize residual signal and threshold\n# we mark anomalies as red data points \nimport matplotlib.pyplot as plt\ndata_idx = np.arange(num_samples)\nplt.scatter(data_idx, resid,s=3,label='normal')\nplt.scatter(data_idx[resid>threshold], resid[resid>threshold], color='red',s=3,label='anomalous')\nthreshold_line = np.ones(num_samples) * threshold\nplt.plot(data_idx, threshold_line, color='green')\nplt.yscale('log')\nplt.xlabel('Data point',fontsize=18)\nplt.ylabel('Residual signal',fontsize=18)\nplt.title('PCA residual signals of data',fontsize=20)\nplt.legend()\nplt.show()"
        }, 
        {
            "source": "Finally, we can report the prediction performance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import metrics\n\ny_predict = np.ones(num_samples)\ny_predict[resid > threshold] = -1\nprint('Classification results:')\nprint(metrics.classification_report(y_true, y_predict))\n\nconfusion_mat = metrics.confusion_matrix(y_true, y_predict, [1, -1])\nprint('Confusion matrix')\ndf_confusion = pd.DataFrame(confusion_mat, columns=['Prediction Positive ','Prediction Negative'])\ndf_confusion.index = ['Original Positive','Original Negative']\ndf_confusion"
        }, 
        {
            "source": "Our question is whether we can determine automatically threshold. What following is about choosing a threshold automatically.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def find_threshold_percentile(resid, args):\n    percent = args\n    return np.percentile(resid, percent)\n\ndef find_threshold_keepinfo(resid, args):\n    (alpha, S, num_principal_components) = args\n    c_alpha = scipy.stats.norm.ppf(1-alpha)\n    phi1 = np.sum(S[num_principal_components+1:])\n    phi2 = np.sum(S[num_principal_components+1:]**2)\n    phi3 = np.sum(S[num_principal_components+1:]**3)\n    h0 = 1 - (2*phi1*phi3)/(3*phi2*phi2)\n    threshold = phi1 * (c_alpha*np.sqrt(2*phi2*h0*h0)/phi1 + 1 + (phi2*h0*(h0-1))/(phi1*phi1))**(1/h0)\n    return threshold\n\nif find_threshold_type == 'keepinfo':\n    find_threshold = find_threshold_keepinfo\n    args = (alpha, S, num_principal_components)\nelif find_threshold_type == 'percentile':\n    find_threshold = find_threshold_percentile\n    args = normal_percent\n\n# automatically compute threshold\nthreshold = find_threshold(resid, args)\nprint(\"Auto threshold = {}\".format(threshold))"
        }, 
        {
            "source": "We mark data points as anomalies if their residual signals are greater than the threshold. The residual signals and threshold are visualized in the following code.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\ndata_idx = np.arange(num_samples)\nplt.scatter(data_idx, resid,s=3,label='normal')\nplt.scatter(data_idx[resid>threshold], resid[resid>threshold], color='red',s=3,label='anomalous')\nthreshold_line = np.ones(num_samples) * threshold\nplt.plot(data_idx, threshold_line, color='green')\nplt.yscale('log')\nplt.xlabel('Data point',fontsize=18)\nplt.ylabel('Residual signal',fontsize=18)\nplt.title('PCA residual signals of data',fontsize=20)\nplt.legend()\nplt.show()"
        }, 
        {
            "source": "Finally, we can report the prediction performance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "from sklearn import metrics\n\ny_predict = np.ones(num_samples)\ny_predict[resid > threshold] = -1\nprint('Classification results:')\nprint(metrics.classification_report(y_true, y_predict))\n\nconfusion_mat = metrics.confusion_matrix(y_true, y_predict, [1, -1])\nprint('Confusion matrix')\ndf_confusion = pd.DataFrame(confusion_mat, columns=['Prediction Positive ','Prediction Negative'])\ndf_confusion.index = ['Original Positive','Original Negative']\ndf_confusion"
        }, 
        {
            "source": "We also have a ROC plot.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import metrics\nFPR, TPR, _ = metrics.roc_curve(y_true, resid, pos_label=[1])\nplt.scatter(TPR,FPR)\nplt.show()"
        }, 
        {
            "source": "## 2.2 Gaussian Mixture Model Approach\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Gaussian Mixture model (GMM) is usually an unsupervised clustering model that is similar to k-means but has more robustness than k-means. However, Gaussian mixture model (GMM) can be used to detect anomalies in a dataset. We can use GMM to capture the density function of the dataset. If one data point receives a low probability assignment from the GMM learned from dataset, it can be classified as an anomalous item. In this figure below, we can learnt 2-component GMMs from given dataset, the red data points possess very low likelihoods (low log-likelihoods) then can be classified as anomalies.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/GMM.png\" width=\"600\">\n\nThe pseudo-code of the algorithms is\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/GMM_algo.png\" width=\"600\">\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now, we build a class GMM for modelling data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import mixture\n\nclass clsGMM:\n    def __init__(self, train_data, num_mixtures=5, percent_abnormal=0.005):\n        self.num_mixtures = num_mixtures\n        self.train_data= train_data\n        self.percent_abnormal= percent_abnormal \n    \n    def train(self):\n        # Init Gaussian Mixture Model with 5 components\n        self.g = mixture.GaussianMixture(self.num_mixtures)        # creating and fitting GMM model using sklearn library\n        self.g.fit(self.train_data) \n        \n    def predict(self, data):\n        # Estimate log probability\n        logprob = self.g.score_samples(data)\n        logprob = logprob*(-1)\n        logprob = logprob + abs(np.min(logprob)) + 0.5\n        residual_signal = logprob;\n        \n        a = np.sort(residual_signal)\n        threshold = a[int((1-self.percent_abnormal)*residual_signal.size)]\n        \n        num_samples = data.shape[0]\n        y_predict = np.ones(num_samples)\n        y_predict[residual_signal > threshold] = -1\n        return y_predict\n"
        }, 
        {
            "source": "Using dataset loaded in the previous section, we can train the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# set model hyper-parameters\nnum_mixtures = 5\npercent_abnormal =0.1\n\n# train on normal data\ngmm = clsGMM(train_data=x[y_true > 0,:], num_mixtures=num_mixtures, percent_abnormal=percent_abnormal)\ngmm.train()\n"
        }, 
        {
            "source": "The trained model is used to predict anomalous data points,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_predict = gmm.predict(x)"
        }, 
        {
            "source": "and compute the predicting performance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "print('Classification results:')\nprint(metrics.classification_report(y_true, y_predict))\n\nconfusion_mat = metrics.confusion_matrix(y_true, y_predict, [1, -1])\nprint('Confusion matrix')\ndf_confusion = pd.DataFrame(confusion_mat, columns=['Prediction Positive ','Prediction Negative'])\ndf_confusion.index = ['Original Positive','Original Negative']\ndf_confusion"
        }, 
        {
            "source": "We can plot discovered data as follows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nx_reduced = PCA(n_components=4).fit_transform(x)\n\n# train on normal data\ngmm = clsGMM(train_data=x[y_true > 0,:], num_mixtures=num_mixtures, percent_abnormal=percent_abnormal)\ngmm.train()\ny_predict = gmm.predict(x)\n\nx_labels=gmm.g.predict(x)\nfig = plt.figure(2, figsize=(15, 10))\n# fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nx_normal=x_reduced[y_predict>0,:]\nx_normal_labels=x_labels[y_predict>0]\nax.scatter(x_normal[:, 0], x_normal[:, 1], x_normal[:, 2], c=x_normal_labels, cmap='Accent')  # plot 2-d data where each data point is decorated with its label.\n\nx_anormaly=x_reduced[y_predict<0,:]\nax.scatter(x_anormaly[:, 0], x_anormaly[:, 1],x_anormaly[:, 2], color='red',zdir='x')\n\n\nax.view_init(elev=10,azim=352)\nplt.show()"
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">3. Practical Coding Exercises</span>\n\n1. You can try to vary  *threshold,keep_info, alpha, normal_percent* values in the **PCA-based** algorithm and *number of clusters, percent_abnormal* for the **GMM-based** algorithm and report the best values for each algorithm in terms of F-measure.\n2. We provide you a subset of [Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)) in [**german.csv**](link_to_data = 'https://github.com/tuliplab/mds/blob/master/Jupyter/data/emnist.digits_letters.small.csv?raw=true'\n). You can try to understand the data statistics and use two algorithms presented in Section II to dectect anomolies and report the results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}