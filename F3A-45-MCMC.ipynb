{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session O - Metropolis and Gibbs Sampling\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "[Kruschke's book](https://sites.google.com/site/doingbayesiandataanalysis/) begins with a fun example of a politician visiting a chain of islands to canvas support - being callow, the politician uses a simple rule to determine which island to visit next. Each day, the politician chooses a neighboring island and compares the populations there with the population of the current island. If the neighboring island has a larger population, the politician goes over. If the neighboring island has a smaller population, then the politician visits with probability $p = p_\\text{neighbor} / p_\\text{current}$; otherwise the politician stays on the same island. After doing this for many days, the politician will end up spending time on each island proportional to the population of each island - in other words, estimating the distribution of island populations correctly. How a simple comparison of only two states at a time can lead to accurate estimation of a probability density is the topic of the next few lectures.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom functools import partial"
        }, 
        {
            "source": "Island hopping\n----", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_islands(n, low=10, high=101):\n    islands = np.random.randint(low, high, n+2)\n    islands[0] = 0\n    islands[-1] = 0\n    return islands"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def hop(islands, start=1, niter=1000):\n    pos = start\n    pop = islands[pos]\n    thetas = np.zeros(niter+1, dtype='int')\n    thetas[0] = pos\n    for i in range(niter):\n        # generate sample from proposal distribution\n        k = np.random.choice([-1, 1], 1)\n        next_pos = pos + k\n        # evaluate unnormalized target distribution at proposed position\n        next_pop = islands[next_pos]\n        # calculate acceptance probability\n        p = min(1, next_pop/pop)\n        # use uniform random to decide accept/reject proposal\n        if np.random.random() < p:\n            pos = next_pos\n            pop = next_pop\n        thetas[i+1] = pos\n    return thetas"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "islands = make_islands(10)\nthetas = hop(islands, start=1, niter=10000)"
        }, 
        {
            "source": "### True population proportions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data = islands[1:-1]\ndata = data/data.sum()\nsns.barplot(x=np.arange(len(data)), y=data)\npass"
        }, 
        {
            "source": "### Estimated population proportions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data = np.bincount(thetas)[1:]\ndata = data/data.sum()\nsns.barplot(x=np.arange(len(data)), y=data)\npass"
        }, 
        {
            "source": "### Generic Metropolis scheme", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def metroplis(start, target, proposal, niter, nburn=0):\n    current = start\n    post = [current]\n    for i in range(niter):\n        proposed = proposal(current)\n        p = min(target(proposed)/target(current), 1)\n        if np.random.random() < p:\n            current = proposed\n        post.append(current)\n    return post[nburn:]"
        }, 
        {
            "source": "### Apply to island hooper", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "target = lambda x: islands[x]\nproposal = lambda x: x + np.random.choice([-1, 1])\npost = metroplis(1, target, proposal, 2000)\ndata = np.bincount(post)[1:]\ndata = data/data.sum()\nsns.barplot(x=np.arange(len(data)), y=data)\npass"
        }, 
        {
            "source": "Bayesian Data Analysis\n----\n\nThe fundamental objective of Bayesian data analysis is to determine the posterior distribution\n\n$$\np(\\theta \\ | \\ X) = \\frac{p(X \\ | \\ \\theta) p(\\theta)}{p(X)}\n$$\n\nwhere the denominator is\n\n$$\np(X) = \\int d\\theta^* p(X \\ | \\ \\theta^*) p(\\theta^*) \n$$\n\nHere, \n\n- $p(X \\ | \\ \\theta)$ is the likelihood, \n- $p(\\theta)$ is the prior and \n- $p(X)$ is a normalizing constant also known as the evidence or marginal likelihood\n\nThe computational issue is the difficulty of evaluating the integral in the denominator. There are many ways to address this difficulty, including:\n\n- In cases with conjugate priors (with conjugate priors, the posterior has the same distribution family as the prior), we can get closed form solutions\n- We can use numerical integration\n- We can approximate the functions used to calculate the posterior with simpler functions and show that the resulting  approximate posterior is \"close\" to true posterior (variational Bayes)\n- We can use Monte Carlo methods, of which the most important is Markov Chain Monte Carlo (MCMC). In simple Monte Carlo inegration, we want to estimate the integral $f(x) \\, p(x) dx$. Wtih Bayesian models, the distribution $p(x)$ in the integral is the posterior\n\n$$\np(x) = p(\\theta \\ | \\ X) = \\frac{p(X \\ | \\ \\theta) p(\\theta)}{\\int d\\theta^* p(X \\ | \\ \\theta^*) p(\\theta^*) }\n$$\n- MCMC allows to sample from the posterior distribution - the samples will not be independent unlike simple Monte Carlo integration, but this is OK as we can compensate for the auto-correlation by drawing a larger number  of samples.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Motivating example\n\nWe will use the toy example of estimating the bias of a coin given a sample consisting of $n$ tosses to illustrate a few of the approaches.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Analytical solution\n\nIf we use a beta distribution as the prior, then the posterior distribution has a closed form solution. This is shown in the example below. Some general points:\n\n- We need to choose a prior distribution family (i.e. the beta here) as well as its parameters (here a=10, b=10)\n    - The prior distribution may be relatively uninformative (i.e. more flat) or informative (i.e. more peaked)\n- The posterior depends on both the prior and the data\n    - As the amount of data becomes large, the posterior approximates the MLE\n    - An informative prior takes more data to shift than an uninformative one\n- Of course, it is also important the model used (i.e. the likelihood) is appropriate for the fitting the data\n- The mode of the posterior distribution is known as the maximum a posteriori (MAP) estimate (cf MLE which is the mode of the likelihood)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import scipy.stats as stats"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "n = 100\nh = 61\np = h/n\nrv = stats.binom(n, p)\nmu = rv.mean()\n\na, b = 10, 10\nprior = stats.beta(a, b)\npost = stats.beta(h+a, n-h+b)\nci = post.interval(0.95)\n\nthetas = np.linspace(0, 1, 200)\nplt.plot(thetas, prior.pdf(thetas), label='Prior', c='blue')\nplt.plot(thetas, post.pdf(thetas), label='Posterior', c='red')\nplt.plot(thetas, n*stats.binom(n, thetas).pmf(h), label='Likelihood', c='green')\nplt.axvline((h+a-1)/(n+a+b-2), c='red', linestyle='dashed', alpha=0.4, label='MAP')\nplt.axvline(mu/n, c='green', linestyle='dashed', alpha=0.4, label='MLE')\nplt.xlim([0, 1])\nplt.axhline(0.3, ci[0], ci[1], c='black', linewidth=2, label='95% CI');\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel('Density', fontsize=16)\nplt.legend(loc='upper left')\npass"
        }, 
        {
            "source": "#### Numerical integration\n\nOne simple way of numerical integration is to estimate the values on a grid of values for $\\theta$. To calculate the posterior, we find the prior and the likelihood for each value of $\\theta$, and for the marginal likelihood, we replace the integral with the equivalent sum\n\n$$\np(X) = \\sum_{\\theta^*} p(X | \\theta^*) p(\\theta^*) \n$$\n\nOne advantage of this is that the prior does not have to be conjugate (although the example below uses the same beta prior for ease of comparison), and so we are not restricted in our choice of an appropriate prior distribution. For example, the prior can be a mixture distribution or estimated empirically from data. The disadvantage, of course, is that this is computationally very expensive when we need to estimate multiple parameters, since the number of grid points grows as $\\mathcal{O}(n^d)$, where $n$ defines the grid resolution and $d$ is the size of $\\theta$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "thetas = np.linspace(0, 1, 200)\nprior = stats.beta(a, b)\n\npost = prior.pdf(thetas) * stats.binom(n, thetas).pmf(h)\n# Normalzie so volume is 1\npost /= (post.sum() / len(thetas))\n\nplt.plot(thetas, prior.pdf(thetas), label='Prior', c='blue')\nplt.plot(thetas, n*stats.binom(n, thetas).pmf(h), label='Likelihood', c='green')\nplt.plot(thetas, post, label='Posterior', c='red')\nplt.xlim([0, 1])\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel('Density', fontsize=16)\nplt.legend()\npass"
        }, 
        {
            "source": "### Markov Chain Monte Carlo (MCMC)\n\nThis lecture will only cover the basic ideas of MCMC and the 3 common variants - Metroplis, Metropolis-Hastings and Gibbs sampling. All code will be built from the ground up to illustrate what is involved in fitting an MCMC model, but only toy examples will be shown since the goal is conceptual understanding. More realistic computational examples will be shown in coming lectures using the `pymc3` and `pystan` packages.\n\nIn Bayesian statistics, we want to estimate the posterior distribution, but this is often intractable due to the high-dimensional integral in the denominator (marginal likelihood). A few other ideas we have encountered that are also relevant here are Monte Carlo integration with independent samples and the use of proposal distributions (e.g. rejection and importance sampling). As we have seen from the Monte Carlo integration lectures, we can approximate the posterior $p(\\theta | X)$ if we can somehow draw many samples that come from the posterior distribution. With vanilla Monte Carlo integration, we need the samples to be independent draws from the posterior distribution, which is a problem if we do not actually know what the posterior distribution is (because we cannot integrate the marginal likelihood). \n\nWith MCMC, we draw samples from a (simple) proposal distribution so that each draw depends only on the state of the previous draw (i.e. the samples form a Markov chain). Under certain conditions, the Markov chain will have a unique stationary distribution. In addition, not all samples are used - instead we set up acceptance criteria for each draw based on comparing successive states with respect to a target distribution that ensure that the stationary distribution is the posterior distribution of interest. The nice thing is that this target distribution only needs to be proportional to the posterior distribution, which means we don't need to evaluate the potentially intractable marginal likelihood, which is just a normalizing constant. We can find such a target distribution easily, since `posterior` $\\propto$ `likelihood` $\\times$ `prior`. After some time, the Markov chain of accepted draws will converge to the stationary distribution, and we can use those samples as (correlated) draws from the posterior distribution, and find functions of the posterior distribution in the same way as for vanilla Monte Carlo integration. \n\nThere are several flavors of MCMC, but the simplest to understand is the Metropolis-Hastings random walk algorithm, and we will start there.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Metropolis-Hastings random walk algorithm for estimating the bias of a coin\n\nTo carry out the Metropolis-Hastings algorithm, we need to draw random samples from the following distributions\n\n- the standard uniform distribution\n- a proposal distribution $p(x)$ that we choose to be $\\mathcal{N}(0, \\sigma)$\n- the target distribution $g(x)$ which is proportional to the posterior probability\n\nGiven an initial guess for $\\theta$ with positive probability of being drawn, the Metropolis-Hastings algorithm proceeds as follows\n\n- Choose a new proposed value ($\\theta_p$) such that  $\\theta_p = \\theta + \\Delta\\theta$ where $\\Delta \\theta \\sim \\mathcal{N}(0, \\sigma)$\n- Caluculate the ratio\n\n$$\n\\rho = \\frac{g(\\theta_p \\ | \\ X)}{g(\\theta \\ | \\ X)} \n$$\n\nwhere $g$ is the posterior probability. \n\n- If the proposal distribution is not symmetrical, we need to weight the acceptance probability to maintain detailed balance (reversibility) of the stationary distribution, and instead calculate\n\n$$\n\\rho = \\frac{g(\\theta_p \\ | \\ X) p(\\theta \\ | \\ \\theta_p)}{g(\\theta \\ | \\ X) p(\\theta_p \\ | \\ \\theta)} \n$$\n\nSince we are taking ratios, the denominator cancels any distribution proportional to $g$ will also work - so we can use \n\n$$\n\\rho = \\frac{p(X | \\theta_p ) p(\\theta_p)}{p(X | \\theta ) p(\\theta)}\n$$\n \n- If $\\rho \\ge 1$, then set $\\theta = \\theta_p$\n- If $\\rho \\lt 1$, then set $\\theta = \\theta_p$ with probability $\\rho$, otherwise set $\\theta = \\theta$ (this is where we use the standard uniform distribution)\n- Repeat the earlier steps\n\nAfter some number of iterations $k$, the samples $\\theta_{k+1}, \\theta_{k+2}, \\dots$ will be samples from the posterior distributions. Here are initial concepts to help your intuition about why this is so:\n\n- We accept a proposed move to $\\theta_{k+1}$ whenever the density of the (unnormalized) target distribution at $\\theta_{k+1}$ is larger than the value of $\\theta_k$ - so $\\theta$ will more often be found in places where the target distribution is denser\n- If this was all we accepted, $\\theta$ would get stuck at a local mode of the target distribution, so we also accept occasional moves to lower density regions - it turns out that the correct probability of doing so is given by the ratio $\\rho$\n- The acceptance criteria only looks at ratios of the target distribution, so the denominator cancels out and does not matter - that is why we only need samples from a distribution proportional to the posterior distribution\n- So, $\\theta$ will be expected to bounce around in such a way that its spends its time in places proportional to the density of the posterior distribution - that is, $\\theta$ is a draw from the posterior distribution. \n\nAdditional notes:\n\nDifferent proposal distributions can be used for Metropolis-Hastings:\n\n- The independence sampler uses a proposal distribution that is independent of the current value of $\\theta$. In this case the proposal distribution needs to be similar to the posterior distribution for efficiency, while ensuring that the acceptance ratio is bounded in the tail region of the posterior.\n- The random walk sampler (used in this example) takes a random step centered at the current value of $\\theta$ - efficiency is a trade-off between small step size with high probability of acceptance and large step sizes with low probability of acceptance. Note (picture will be sketched in class) that the random walk may take a long time to traverse narrow regions of the probability distribution. Changing the step size (e.g. scaling $\\Sigma$ for a multivariate normal proposal distribution) so that a target proportion of proposals are accepted is known as *tuning*.\n- Much research is being conducted on different proposal distributions for efficient sampling of the posterior distribution.\n\nWe will first see a numerical example and then try to understand why it works.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def target(lik, prior, n, h, theta):\n    if theta < 0 or theta > 1:\n        return 0\n    else:\n        return lik(n, theta).pmf(h)*prior.pdf(theta)\n\nn = 100\nh = 61\na = 10\nb = 10\nlik = stats.binom\nprior = stats.beta(a, b)\nsigma = 0.3\n\nnaccept = 0\ntheta = 0.1\nniters = 10000\nsamples = np.zeros(niters+1)\nsamples[0] = theta\nfor i in range(niters):\n    theta_p = theta + stats.norm(0, sigma).rvs() \n    rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, theta ))\n    u = np.random.uniform()\n    if u < rho:\n        naccept += 1\n        theta = theta_p\n    samples[i+1] = theta\nnmcmc = len(samples)//2\nprint(\"Efficiency = \", naccept/niters)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "post = stats.beta(h+a, n-h+b)\n\nplt.hist(samples[nmcmc:], 40, histtype='step', normed=True, linewidth=1, label='Prior');\nplt.hist(prior.rvs(nmcmc), 40, histtype='step', normed=True, linewidth=1, label='Posterior');\nplt.plot(thetas, post.pdf(thetas), c='red', linestyle='--', alpha=0.5, label='True posterior')\nplt.xlim([0,1]);\nplt.legend(loc='upper left')\npass"
        }, 
        {
            "source": "#### Assessing for convergence\n\nTrace plots are often used to informally assess for stochastic convergence. Rigorous demonstration of convergence is an unsolved problem, but simple ideas such as running multiple chains and checking that they are converging to similar distributions are often employed in practice.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def mh_coin(niters, n, h, theta, lik, prior, sigma):\n    samples = [theta]\n    while len(samples) < niters:\n        theta_p = theta + stats.norm(0, sigma).rvs() \n        rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, theta ))\n        u = np.random.uniform()\n        if u < rho:\n            theta = theta_p\n        samples.append(theta)\n    return samples"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "n = 100\nh = 61\nlik = stats.binom\nprior = stats.beta(a, b)\nsigma = 0.05\nniters = 100\n\nsampless = [mh_coin(niters, n, h, theta, lik, prior, sigma) for theta in np.arange(0.1, 1, 0.2)]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Convergence of multiple chains\n\nfor samples in sampless:\n    plt.plot(samples, '-o')\nplt.xlim([0, niters])\nplt.ylim([0, 1]);"
        }, 
        {
            "source": "#### Why does Metropolis-Hastings work?\n\nThere are two main ideas - first that the samples generated by MCMC constitute a Markov chain, and that this Markov chain has a unique stationary distribution that is always reached if we generate a very large number of samples. The second idea is to show that this stationary distribution is exactly the posterior distribution that we are looking for. We will only give the intuition here as a refresher.\n\n#### One: There is a unique stationary state\n\nSince possible transitions depend only on the current and the proposed values of $\\theta$, the successive values of $\\theta$ in a Metropolis-Hastings sample constitute a Markov chain. Recall that for a Markov chain with a transition matrix $T$\n\n$$\n\\pi = \\pi T\n$$\n\nmeans that $\\pi$ is a stationary distribution. If it is possible to go from any state to any other state, then the matrix is irreducible. If in addition, it is not possible to get stuck in an oscillation, then the matrix is also aperiodic or mixing. For finite state spaces, irreducibility and aperiodicity guarantee the existence of a unique stationary state. For continuous state space, we need an additional property of positive recurrence - starting from any state, the expected time to come back to the original state must be finite. If we have all 3 properties of irreducibility, aperiodicity and positive recurrence, then there is a unique stationary distribution. The term ergodic is a little confusing - most standard definitions take ergodicity to be equivalent to irreducibility, but often Bayesian texts take ergodicity to mean irreducibility, aperiodicity and positive recurrence, and we will follow the latter convention. For another intuitive perspective, the random walk Metropolis-Hasting algorithm is analogous to a diffusion process. Since all states are communicating (by design), eventually the system will settle into an equilibrium state. This is analogous to converging on the stationary state. \n\n#### Two: The stationary state is the posterior probability distribution\n\nWe will consider the simplest possible scenario for an explicit calculation. Suppose we have a two-state system where the posterior probabilities are $\\theta$ and $1 - \\theta$. Suppose $\\theta \\lt 0.5$. So we have the following picture with the Metropolis-Hastings algorithm:\n\n![Markov chain](https://github.com/tulip-lab/mds/raw/master/Jupyter/image/mcmc.png)\n\nand we find the stationary distribution $\\pi = \\left( \\begin{array}{cc} p & 1-p \\end{array} \\right)$ by solving\n\n$$\n\\begin{align}\n\\left( \\begin{array}{cc} p & 1-p \\end{array} \\right) &=\n\\left( \\begin{array}{cc} p & 1-p \\end{array} \\right) \\left( \n\\begin{array}{cc}\n0 & 1  \\\\\n\\frac{\\theta}{1-\\theta} & 1-\\frac{\\theta}{1-\\theta} \n\\end{array} \n\\right)\n\\end{align}\n$$\n\nto be $\\pi = \\left( \\begin{array}{cc} \\theta & 1-\\theta \\end{array} \\right)$, which is the posterior distribution.\n\nThe final point is that a stationary distribution has to follow the detailed balance (reversibility) criterion that says that the probability of being in state $x$ and moving to state $y$ must be the same as the probability of being in state $y$ and moving to state $x$. Or, more briefly,\n\n$$\n\\pi(x)T(x \\to y) = \\pi(y)T(y \\to x)\n$$\n\nand the need to make sure that this condition is true accounts for the strange looking acceptance criterion\n\n$$\n\\min \\left(1, \\frac{g(\\theta_p \\ | \\ X) p(\\theta \\ | \\ \\theta_p)}{g(\\theta \\ | \\ X) p(\\theta_p \\ | \\ \\theta)} \\right)\n$$\n\n### Intuition\n\nWe want the stationary distribution $\\pi(x)$ to be the posterior distribution $P(x)$. So we set\n\n$$\nP(x)T(x \\to y) = P(y)T(y \\to x)\n$$\n\nRearranging, we get\n\n$$\n\\frac{T(x \\to y)}{T(y \\to x)} = \\frac{P(y)}{P(x)}\n$$\n\nWe split the transition probability into separate proposal $q$ and acceptance $A$ parts, and after a little algebraic rearrangement get\n\n$$\n\\frac{A(x \\to y)}{A(y \\to x)} = \\frac{P(y) \\, q(y \\to x)}{P(x) \\, q(x \\to y)}\n$$\n\nAn acceptance probability that meets this condition is\n$$\nA(x \\to y) = \\min \\left(1, \\frac{P(y) \\, q(y \\to x)}{P(x) \\, q(x \\to y)} \\right)\n$$\n\nsince $A$ in the numerator and denominator are both bounded above by 1.\n\nSee [Chib and Greenberg](https://eml.berkeley.edu/reprints/misc/understanding.pdf) for algebraic details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### The Gibbs sampler\n\nSuppose we have a vector of parameters $\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_k)$, and we want to estimate the joint posterior distribution $p(\\theta | X)$. Suppose we can find and draw random samples from all the conditional distributions \n\n$$\np(\\theta_1 | \\theta_2, \\dots \\theta_k, X) \\\\\np(\\theta_2 | \\theta_1, \\dots \\theta_k, X) \\\\\n\\dots \\\\\np(\\theta_k | \\theta_1, \\theta_2, \\dots, X) \n$$\n\nWith Gibbs sampling, the Markov chain is constructed by sampling from the conditional distribution for each parameter $\\theta_i$ in turn, treating all other parameters as observed. When we have finished iterating over all parameters, we are said to have completed one cycle of the Gibbs sampler. Since hierarchical models are typically set up as products of conditional distributions, the Gibbs sampler is ubiquitous in Bayesian modeling. Where it is difficult to sample from a conditional distribution, we can sample using a Metropolis-Hastings algorithm instead - this is known as Metropolis within Gibbs. \n\nGibbs sampling is a type of random walk through parameter space, and hence can be thought of as a Metropolis-Hastings algorithm with a special proposal distribution. At each iteration in the cycle, we are drawing a proposal for a new value of a particular parameter, where the proposal distribution *is* the conditional posterior probability of that parameter. This means that the proposal move is *always* accepted. Hence, if we can draw samples from the conditional distributions, Gibbs sampling can be much more efficient than regular Metropolis-Hastings. \nMore formally, we want to show that \n\n$$\n\\frac{P(y) \\, q(y \\to x)}{P(x) \\, q(x \\to y)} = 1\n$$\n\nWe start by noting that $P(x_{-i}$ is the same as $P(y_{-i})$ since apart from the component $i$, the old state and the proposed new state are identical in Gibbs sampling. We also recall that \n\n$$P(x_i \\mid x_{-i}) \\, P(x_{-i}) = P(x_i, x_{-i} = P(x)$$ \n\nby definition of conditional probability. So we have\n\n$$\n\\begin{align}\n\\frac{P(y) \\, q(y \\to x)}{P(x) \\, q(x \\to y)} &= \\frac{P(y_i \\mid y_{-1}) \\, P(y_{-i})\\, P(x_i \\mid x_{-i}) }{P(x_i \\mid x_{-i}) \\, P(x_{-i})\\, P(y_i \\mid y_{-1})} &= 1\n\\end{align}\n$$\n\n\n**Advantages of Gibbs sampling**\n\n- No need to tune proposal distribution\n- Proposals are always accepted\n\n**Disadvantages of Gibbs sampling**\n\n- Need to be able to derive conditional probability distributions \n- Need to be able to (cheaply) draw random samples from conditional probability distributions\n- Can be very slow if parameters are correlated because you cannot take \"diagonal\" steps (draw picture to illustrate)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Motivating example\n \nWe will use the toy example, familiar from the EM lecture, of estimating the bias of two coins given sample pairs $(z_1, n_1)$ and $(z_2, n_2)$ where $z_i$ is the number of heads in $n_i$ tosses for coin $i$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Setup", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def bern(theta, z, N):\n    \"\"\"Bernoulli likelihood with N trials and z successes.\"\"\"\n    return np.clip(theta**z * (1-theta)**(N-z), 0, 1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def bern2(theta1, theta2, z1, z2, N1, N2):\n    \"\"\"Bernoulli likelihood with N trials and z successes.\"\"\"\n    return bern(theta1, z1, N1) * bern(theta2, z2, N2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_thetas(xmin, xmax, n):\n    xs = np.linspace(xmin, xmax, n)\n    widths =(xs[1:] - xs[:-1])/2.0\n    thetas = xs[:-1]+ widths\n    return thetas"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from mpl_toolkits.mplot3d import Axes3D\n\ndef make_plots(X, Y, prior, likelihood, posterior, projection=None):\n    fig, ax = plt.subplots(1,3, subplot_kw=dict(projection=projection, aspect='equal'), figsize=(12,3))\n    if projection == '3d':\n        ax[0].plot_surface(X, Y, prior, alpha=0.3, cmap=plt.cm.jet)\n        ax[1].plot_surface(X, Y, likelihood, alpha=0.3, cmap=plt.cm.jet)\n        ax[2].plot_surface(X, Y, posterior, alpha=0.3, cmap=plt.cm.jet)\n        for ax_ in ax: ax_._axis3don = False\n    else:\n        ax[0].contour(X, Y, prior, cmap=plt.cm.jet)\n        ax[1].contour(X, Y, likelihood, cmap=plt.cm.jet)\n        ax[2].contour(X, Y, posterior, cmap=plt.cm.jet)\n    ax[0].set_title('Prior')\n    ax[1].set_title('Likelihood')\n    ax[2].set_title('Posteior')     \n    plt.tight_layout()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "thetas1 = make_thetas(0, 1, 101)\nthetas2 = make_thetas(0, 1, 101)\nX, Y = np.meshgrid(thetas1, thetas2)"
        }, 
        {
            "source": "#### Analytic solution", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = 2\nb = 3\n\nz1 = 11\nN1 = 14\nz2 = 7\nN2 = 14\n\nprior = stats.beta(a, b).pdf(X) * stats.beta(a, b).pdf(Y)\nlikelihood = bern2(X, Y, z1, z2, N1, N2)\nposterior = stats.beta(a + z1, b + N1 - z1).pdf(X) * stats.beta(a + z2, b + N2 - z2).pdf(Y)\nmake_plots(X, Y, prior, likelihood, posterior)\nmake_plots(X, Y, prior, likelihood, posterior, projection='3d')"
        }, 
        {
            "source": "#### Grid approximation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def c2d(thetas1, thetas2, pdf):\n    width1 = thetas1[1] - thetas1[0]\n    width2 = thetas2[1] - thetas2[0]\n    area = width1 * width2\n    pmf = pdf * area\n    pmf /= pmf.sum()\n    return pmf"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "_prior = bern2(X, Y, 2, 8, 10, 10) + bern2(X, Y, 8, 2, 10, 10)\nprior_grid = c2d(thetas1, thetas2, _prior)\n_likelihood = bern2(X, Y, 1, 1, 2, 3)\nposterior_grid = _likelihood * prior_grid\nposterior_grid /= posterior_grid.sum()\nmake_plots(X, Y, prior_grid, likelihood, posterior_grid)\nmake_plots(X, Y, prior_grid, likelihood, posterior_grid, projection='3d')"
        }, 
        {
            "source": "#### Metropolis", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = 2\nb = 3\n\nz1 = 11\nN1 = 14\nz2 = 7\nN2 = 14\n\nprior = lambda theta1, theta2: stats.beta(a, b).pdf(theta1) * stats.beta(a, b).pdf(theta2)\nlik = partial(bern2, z1=z1, z2=z2, N1=N1, N2=N2)\ntarget = lambda theta1, theta2: prior(theta1, theta2) * lik(theta1, theta2)\n\ntheta = np.array([0.5, 0.5])\nniters = 10000\nburnin = 500\nsigma = np.diag([0.2,0.2])\n\nthetas = np.zeros((niters-burnin, 2), np.float)\nfor i in range(niters):\n    new_theta = stats.multivariate_normal(theta, sigma).rvs()\n    p = min(target(*new_theta)/target(*theta), 1)\n    if np.random.rand() < p:\n        theta = new_theta\n    if i >= burnin:\n        thetas[i-burnin] = theta"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "kde = stats.gaussian_kde(thetas.T)\nXY = np.vstack([X.ravel(), Y.ravel()])\nposterior_metroplis = kde(XY).reshape(X.shape)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_metroplis)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_metroplis, projection='3d')"
        }, 
        {
            "source": "#### Gibbs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = 2\nb = 3\n\nz1 = 11\nN1 = 14\nz2 = 7\nN2 = 14\n\nprior = lambda theta1, theta2: stats.beta(a, b).pdf(theta1) * stats.beta(a, b).pdf(theta2)\nlik = partial(bern2, z1=z1, z2=z2, N1=N1, N2=N2)\ntarget = lambda theta1, theta2: prior(theta1, theta2) * lik(theta1, theta2)\n\ntheta = np.array([0.5, 0.5])\nniters = 10000\nburnin = 500\nsigma = np.diag([0.2,0.2])\n\nthetas = np.zeros((niters-burnin,2), np.float)\nfor i in range(niters):\n    theta = [stats.beta(a + z1, b + N1 - z1).rvs(), theta[1]]\n    theta = [theta[0], stats.beta(a + z2, b + N2 - z2).rvs()]\n    \n    if i >= burnin:\n        thetas[i-burnin] = theta"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "kde = stats.gaussian_kde(thetas.T)\nXY = np.vstack([X.ravel(), Y.ravel()])\nposterior_gibbs = kde(XY).reshape(X.shape)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_gibbs)\nmake_plots(X, Y, prior(X, Y), lik(X, Y), posterior_gibbs, projection='3d')"
        }, 
        {
            "source": "Hierarchical models\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Hierarchical models have the following structure - first we specify that the data come from a distribution with parameters $\\theta$\n\n$$\nX \\sim f(X\\ | \\ \\theta)\n$$\n\nand that the parameters themselves come from another distribution with hyperparameters $\\lambda$\n\n$$\n\\theta \\sim g(\\theta \\ | \\ \\lambda)\n$$\n\nand finally that $\\lambda$ comes from a prior distribution\n\n$$ \n\\lambda \\sim h(\\lambda)\n$$\n\nMore levels of hierarchy are possible - i.e you can specify hyper-hyperparameters for the distribution of $\\lambda$ and so on.\n\nThe essential idea of the hierarchical model is because the $\\theta$s are not independent but rather are drawn from a common distribution with parameter $\\lambda$, we can share information across the $\\theta$s by also estimating $\\lambda$ at the same time. \n\nAs an example, suppose have data about the proportion of heads after some number of tosses from several coins, and we want to estimate the bias of each coin. We also know that the coins come from the same mint and so might share some common manufacturing defect. There are two extreme approaches - we could estimate the bias of each coin from its coin toss data independently of all the others, or we could pool the results together and estimate the same bias for all coins. Hierarchical models provide a compromise where we shrink individual estimates towards a common estimate.\n\nNote that because of the conditionally independent structure of hierarchical models, Gibbs sampling is often a natural choice for the MCMC sampling strategy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Gibbs sampler example from [Robert and Casella, 10.17](http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-21239-5)\n\nSuppose we have data of the number of failures ($y_i$) for each of 10 pumps in a nuclear plant. We also have the times ($_i$) at which each pump was observed. We want to model the number of failures with a Poisson likelihood, where the expected number of failure $\\lambda_i$ differs for each pump. Since the time which we observed each pump is different, we need to scale each $\\lambda_i$ by its observed time $t_i$.\n\nWe now specify the hierarchical model - note change of notation from the overview above - that $\\theta$ is $\\lambda$ (parameter) and $\\lambda$ is $\\beta$ (hyperparameter) simply because $\\lambda$ is traditional for the Poisson distribution parameter. \n\nThe likelihood $f$ is \n$$\n\\prod_{i=1}^{10} \\text{Poisson}(\\lambda_i t_i)\n$$\n\nWe let the prior $g$ for $\\lambda$ be \n\n$$\n\\lambda \\sim \\text{Gamma}(\\alpha, \\beta)\n$$\nwith $\\alpha = 1.8$ (an improper prior whose integral does not sum to 1)\n\nand let the hyperprior $h$ for $\\beta$ to be \n\n$$\n\\beta \\sim \\text{Gamma}(\\gamma, \\delta)\n$$\n\nwith $\\gamma = 0.01$ and $\\delta = 1$.\n\nThere are 11 unknown parameters (10 $\\lambda$s and $\\beta$) in this hierarchical model.\n\nThe posterior is \n$$\np(\\lambda, \\beta \\ | \\ y, t) = \\prod_{i=1}^{10} \\text{Poisson}(\\lambda_i t_i) \\times \\text{Gamma}(\\alpha, \\beta) \\times \\text{Gamma}(\\gamma, \\delta)\n$$\n\nwith the conditional distributions needed for Gibbs sampling given by\n\n$$\np(\\lambda_i \\ | \\ \\lambda_{-i}, \\beta, y, t) = \\text{Gamma}(y_i + \\alpha, t_i + \\beta)\n$$\n\nand \n\n$$\np(\\beta \\ | \\ \\lambda, y, t) = \\text{Gamma}(10\\alpha + \\gamma, \\delta + \\sum_{i=1}^10 \\lambda_i)\n$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from numpy.random import gamma as rgamma # rename so we can use gamma for parameter name"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def lambda_update(alpha, beta, y, t):\n    return rgamma(size=len(y), shape=y+alpha, scale=1.0/(t+beta))\n\ndef beta_update(alpha, gamma, delta, lambd, y):\n    return rgamma(size=1, shape=len(y) * alpha + gamma, scale=1.0/(delta + lambd.sum()))\n\ndef gibbs(niter, y, t, alpha, gamma, delta):\n    lambdas_ = np.zeros((niter, len(y)), np.float)\n    betas_ = np.zeros(niter, np.float)\n    \n    lambda_ = y/t\n\n    for i in range(niter):\n        beta_ = beta_update(alpha, gamma, delta, lambda_, y)\n        lambda_ = lambda_update(alpha, beta_, y, t)\n\n        betas_[i] = beta_\n        lambdas_[i,:] = lambda_\n        \n    return betas_, lambdas_"
        }, 
        {
            "source": "#### Setup", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "alpha = 1.8\ngamma = 0.01\ndelta = 1.0\nbeta0 = 1\ny = np.array([5, 1, 5, 14, 3, 19, 1, 1, 4, 22], np.int)\nt = np.array([94.32, 15.72, 62.88, 125.76, 5.24, 31.44, 1.05, 1.05, 2.10, 10.48], np.float)\nniter = 1000"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "betas, lambdas = gibbs(niter, y, t, alpha, gamma, delta)\nprint('%.3f' % betas.mean())\nprint('%.3f' % betas.std(ddof=1))\nprint(lambdas.mean(axis=0))\nprint(lambdas.std(ddof=1, axis=0))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.figure(figsize=(8, 16))\nfor i in range(len(lambdas.T)):\n    plt.subplot(5,2,i+1)\n    plt.plot(lambdas[::10, i]);\n    plt.title('Trace for $\\lambda$%d' % i)\nplt.tight_layout()"
        }, 
        {
            "source": "### Bandits\n\nOne-arm bandit refers to slot machines in a casino. The multi-arm bandit problem is that you are faced with multiple slot machines, and they each have potentially different payoffs - perhaps there is one machine where you could potentially win a lot of money and others where you would lose money in the long run. You need to balance exploration (trying different machines)  against exploitation (sticking to the best machine found so far) in an optimal way to maximize your gains. Suppose you were looking for a mate among N candidates - how do you optimize your dating strategy as a good Bayesian? \n\nAdapted from [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities)", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "k = 6\nbandits = np.random.uniform(0, 1, k)\n\na = np.ones(k)\nb = np.ones(k)\nwins = np.zeros(k)\ntrials = np.zeros(k)\ndist = stats.beta(a, b)\n\nn = 1000\ndists = [dist]\nfor _ in range(n-1):\n    i = np.argmax(dist.rvs())\n    print(i, end=', ')\n    payoff = np.random.rand() < bandits[i]\n    wins[i] += payoff\n    trials[i] += 1 \n    dist = stats.beta(a + wins, b + trials - wins)\n    dists.append(dist)\n\nplt.figure(figsize=(4*3, 5*2))\nx = np.repeat(np.linspace(0, 1, 100), k).reshape(-1,k)\nfor i, dist in enumerate(dists[::(n//20)], 1):\n    plt.subplot(20//4,4,i)\n    plt.plot(x, dist.pdf(x))\n    if i==1: plt.legend(range(k))\n    plt.yticks([])\n    plt.xticks([])\nplt.tight_layout()"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "latex_envs": {
            "current_citInitial": 1, 
            "eqLabelWithNumbers": true, 
            "cite_by": "apalike", 
            "bibliofile": "biblio.bib", 
            "eqNumInitial": 0
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}