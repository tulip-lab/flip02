{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session K - Classifying and Calculating ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n\n# Catalogue\n\n\n<p>&#8226; <a href=\"#given\">Given information</a><br>\n&#8226; <a href=\"#classify_rand\">Classifying some random example data</a><br>\n&#8226; <a href=\"#chern_err\">Calculating the Chernoff theoretical bounds for P(error)</a><br>\n&#8226; <a href=\"#emp_err\">Calculating the empirical error rate</a><br>\n\n  \n\n  \n  \n\n\n\n<hr>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Given information:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n\n\n#### model: continuous univariate normal (Gaussian) model for the class-conditional densities\n\n\n$ p(\\vec{x} | \\omega_j) \\sim N(\\vec{\\mu}|\\Sigma) $ \n\n$ p(\\vec{x} | \\omega_j) \\sim \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp{ \\bigg[-\\frac{1}{2} (\\vec{x}-\\vec{\\mu})^t \\Sigma^{-1}(\\vec{x}-\\vec{\\mu}) \\bigg] } $\n\n\n\n\n#### Prior probabilities:\n\n$ P(\\omega_1) = P(\\omega_2) = 0.5 $\n\n\n ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The samples are of 2-dimensional feature vectors:\n\n$\\vec{x} = \\bigg[ \n\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\end{array} \\bigg]$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Means of the sample distributions for 2-dimensional features:\n\n$\\vec{\\mu}_{\\,1} = \\bigg[ \n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array} \\bigg]$,\n$\\; \\vec{\\mu}_{\\,2} = \\bigg[ \n\\begin{array}{c}\n1 \\\\\n1 \\\\\n\\end{array} \\bigg]$\n\n#### Covariance matrices for the statistically independend and identically distributed ('i.i.d') features: \n\n$\\Sigma_i = \\bigg[ \n\\begin{array}{cc}\n\\sigma_{11}^2 & \\sigma_{12}^2\\\\\n\\sigma_{21}^2 & \\sigma_{22}^2 \\\\\n\\end{array} \\bigg], \\; \n\\Sigma_1 = \\Sigma_2 = I = \\bigg[ \n\\begin{array}{cc}\n1 & 0\\\\\n0 & 1 \\\\\n\\end{array} \\bigg], \\;$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Class conditional probabilities:\n\n$ p(\\vec{x}\\;|\\;\\omega_1) \\sim N \\bigg( \\vec{\\mu_1} = \\;  \\bigg[ \n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array} \\bigg], \\Sigma = I \\bigg) $\n\n$ p(\\vec{x}\\;|\\;\\omega_2) \\sim N \\bigg( \\vec{\\mu_2} = \\;  \\bigg[ \n\\begin{array}{c}\n1 \\\\\n1 \\\\\n\\end{array} \\bigg], \\Sigma = I \\bigg) $", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<p><a name=\"deriving_db\"></a>\n<br></p>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n## Classifying some random example data\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%pylab inline\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef decision_boundary(x_1):\n    \"\"\" Calculates the x_2 value for plotting the decision boundary.\"\"\"\n    return -x_1 + 1\n\n# Generate 100 random patterns for class1\nmu_vec1 = np.array([0,0])\ncov_mat1 = np.array([[1,0],[0,1]])\nx1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\nmu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector\n\n# Generate 100 random patterns for class2\nmu_vec2 = np.array([1,1])\ncov_mat2 = np.array([[1,0],[0,1]])\nx2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100)\nmu_vec2 = mu_vec2.reshape(1,2).T # to 1-col vector\n\n# Scatter plot\nf, ax = plt.subplots(figsize=(7, 7))\nax.scatter(x1_samples[:,0], x1_samples[:,1], marker='o', color='green', s=40, alpha=0.5)\nax.scatter(x2_samples[:,0], x2_samples[:,1], marker='^', color='blue', s=40, alpha=0.5)\nplt.legend(['Class1 (w1)', 'Class2 (w2)'], loc='upper right') \nplt.title('Densities of 2 classes with 100 bivariate random patterns each')\nplt.ylabel('x2')\nplt.xlabel('x1')\nftext = 'p(x|w1) ~ N(mu1=(0,0)^t, cov1=I)\\np(x|w2) ~ N(mu2=(1,1)^t, cov2=I)'\nplt.figtext(.15,.8, ftext, fontsize=11, ha='left')\nplt.ylim([-3,4])\nplt.xlim([-3,4])\n\n\n\n# Plot decision boundary\nx_1 = np.arange(-5, 5, 0.1)\nbound = decision_boundary(x_1)\nplt.annotate('R1', xy=(-2, 2), xytext=(-2, 2), size=20)\nplt.annotate('R2', xy=(2.5, 2.5), xytext=(2.5, 2.5), size=20)\nplt.plot(x_1, bound, color='r', alpha=0.8, linestyle=':', linewidth=3)\n\nx_vec = np.linspace(*ax.get_xlim())\nx_1 = np.arange(0, 100, 0.05)\n\nplt.show()"
        }, 
        {
            "source": "<p><a name=\"chern_err\"></a>\n<br></p>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n## Calculating the Chernoff theoretical bounds for P(error)\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "$P(error) \\le p^{\\beta}(\\omega_1) \\; p^{1-\\beta}(\\omega_2) \\; e^{-(\\beta(1-\\beta))}$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "$\\Rightarrow 0.5^\\beta \\cdot 0.5^{(1-\\beta)} \\; e^{-(\\beta(1-\\beta))}$\n\n$\\Rightarrow 0.5 \\cdot e^{-\\beta(1-\\beta)}$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "$min[P(\\omega_1), \\; P(\\omega_2)] \\le 0.5 \\; e^{-(\\beta(1-\\beta))} \\quad for \\; P(\\omega_1), \\; P(\\omega_2) \\ge \\; 0 \\; and \\; 0 \\; \\le \\; \\beta \\; \\le 1$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Plotting the Chernoff Bound for $0 \\le \\beta \\le 1$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def chernoff_bound(beta):\n    return 0.5 * np.exp(-beta * (1-beta))\n\nbetas = np.arange(0, 1, 0.01)\nc_bound = chernoff_bound(betas)\n\nplt.plot(betas, c_bound)\nplt.title('Chernoff Bound')\nplt.ylabel('P(error)')\nplt.xlabel('parameter beta')\n\nplt.show()"
        }, 
        {
            "source": "#### Finding the global minimum: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from scipy.optimize import minimize\n\nx0 = [0.39] # initial guess (here: guessed based on the plot)\nres = minimize(chernoff_bound, x0, method='Nelder-Mead')\nprint(res)"
        }, 
        {
            "source": "<p><a name=\"emp_err\"></a>\n<br></p>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n## Calculating the empirical error rate\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def decision_rule(x_vec):\n    \"\"\" Returns value for the decision rule of 2-d row vectors \"\"\"\n    x_1 = x_vec[0]\n    x_2 = x_vec[1]\n    return -x_1 - x_2 + 1\n\nw1_as_w2, w2_as_w1 = 0, 0\n\nfor x in x1_samples:\n    if decision_rule(x) < 0:\n        w1_as_w2 += 1\nfor x in x2_samples:\n    if decision_rule(x) > 0:\n        w2_as_w1 += 1\n\nemp_err = (w1_as_w2 + w2_as_w1) / float(len(x1_samples) + len(x2_samples))\n    \nprint('Empirical Error: {}%'.format(emp_err * 100))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}