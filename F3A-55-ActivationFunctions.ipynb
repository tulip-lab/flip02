{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session B - Activation Functions \n\nActivation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input -- usually passing this input through it and generating an output through its result. To\nbegin the process of having a more intuitive understanding, let's go through some of the most commonly used functions.\n\n<font size = 3><strong>This lesson covers the following concepts of Activation Functions:</strong></font>\n<br>\n- <p><a href=\"#ref1\">The Step Functions</a></p>\n- <p><a href=\"#ref2\">The Sigmoid Functions</a></p>\n- <p><a href=\"#ref3\">The Linear Unit Functions</a></p>\n- <p><a href=\"#ref3\">Activation Functions in TensorFlow</a></p>\n<p></p>\n\n\n\n** References and additional reading and resources**\n* This material is from [CognitiveClass.AI](https://cognitiveclass.ai), created by: <a href=\"https://ca.linkedin.com/in/rafaelblsilva\"> Rafael Belo Da Silva </a> \n* This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n\n\n---\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Activation Function\n\n### 1.1 Importing Dependencies", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n%matplotlib inline"
        }, 
        {
            "source": "The next cell implements a basic function that plots a surface for an arbitrary activation function. The plot is done for all possible values of weight and bias between -0.5 and 0.5 with a step of 0.05. The input, the weight, and the bias are one-dimensional. Additionally, the input can be passed as an argument.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def plot_act(i=1.0, actfunc=lambda x: x):\n    ws = np.arange(-0.5, 0.5, 0.05)\n    bs = np.arange(-0.5, 0.5, 0.05)\n\n    X, Y = np.meshgrid(ws, bs)\n\n    os = np.array([actfunc(tf.constant(w*i + b)).eval(session=sess) \\\n                   for w,b in zip(np.ravel(X), np.ravel(Y))])\n\n    Z = os.reshape(X.shape)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, Y, Z, rstride=1, cstride=1)"
        }, 
        {
            "source": "### 1.2 Basic Structure\n\nIn this example we illustrate how, in Tensorflow, to compute the weighted sum that goes into the neuron and direct it to the activation function. For further details, read the code comments below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#start a session\nsess = tf.Session();\n#create a simple input of 3 real values\ni = tf.constant([1.0, 2.0, 3.0], shape=[1, 3])\n#create a matrix of weights\nw = tf.random_normal(shape=[3, 3])\n#create a vector of biases\nb = tf.random_normal(shape=[1, 3])\n#dummy activation function\ndef func(x): return x\n#tf.matmul will multiply the input(i) tensor and the weight(w) tensor then sum the result with the bias(b) tensor.\nact = func(tf.matmul(i, w) + b)\n#Evaluate the tensor to a numpy array\nact.eval(session=sess)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plot_act(1.0, func)"
        }, 
        {
            "source": "<a href=\"#ref1\"></a>\n## 2. The Step Functions\nThe Step function was the first one designed for Machine Learning algorithms. It consists of a simple threshold function that varies the Y value from 0 to 1. This function has been historically utilized for classification problems, like Logistic Regression with two classes.\n\n<img width=\"250\" alt=\"Activation binary step\" src=\"https://ibm.box.com/shared/static/kqect7hdbnpzb6ylnauimr1uxmx5634k.png\">\n\nThe Step Function simply functions as a limiter. Every input that goes through this function will be applied to gets either assigned a value of 0 or 1. As such, it is easy to see how it can be handy in classification problems.\n\nThere are other variations of the Step Function such as the Rectangle Step and others, but those are seldom used.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Tensorflow doesn't have a Step Function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a href=\"#ref2\"></a>\n## 3. The Sigmoid Functions\nThe next in line for Machine Learning problems is the family of the ever-present Sigmoid functions. Sigmoid functions are called that due to their shape in the Cartesian plane, which resembles an \"S\" shape.\n\nSigmoid functions are very useful in the sense that they \"squash\" their given inputs into a bounded interval. This is exceptionally handy when combining these functions with others such as the Step function.\n\nMost of the Sigmoid functions you should find in applications will be the Logistic, Arctangent, and Hyperbolic Tangent functions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.1 Logistic Regression (sigmoid)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The Logistic function, as its name implies, is widely used in Logistic Regression. It is defined as $f(x) = \\dfrac{1}{1 + e^{-x}}$. Effectively, this makes it so you have a Sigmoid over the $(0,1)$ interval, like so:\n\n<img width=\"384\" alt=\"Activation - Binary Step\" src=\"https://ibm.box.com/shared/static/eryl0bvmczgfzemarakzgvgvgwe0c135.png\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plot_act(1, tf.sigmoid)"
        }, 
        {
            "source": "3D sigmoid plot. The x-axis is the weight, the y-axis is the bias.\n\n### 3.2 Using sigmoid in a neural net layer ####", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "act = tf.sigmoid(tf.matmul(i, w) + b)\nact.eval(session=sess)"
        }, 
        {
            "source": "The Arctangent and Hyperbolic Tangent functions on the other hand, as the name implies, are based on the Tangent function. Arctangent is defined by $f(x) = tan^{-1}x$, and produces a sigmoid over the $(\\dfrac{-\\pi}{2},\\dfrac{\\pi}{2})$ interval.\n\n<img width=\"384\" alt=\"Activation - Atan\" src=\"https://ibm.box.com/shared/static/ajw9xtnsux7hc4fi5pghatwak425xk7f.png\">\n\nIt has no implementation in Tensorflow", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.3 Tanh", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The Hyperbolic Tangent, or TanH as it's usually called, is defined as $f(x) = \\dfrac{2}{1 + e^{-2x}} - 1$. It produces a sigmoid over the $(-1,1)$ interval. TanH is widely used in a wide range of applications, and is probably the most used function of the Sigmoid family.\n\n<img width=\"384\" alt=\"Activation - TanH\" src=\"https://ibm.box.com/shared/static/ylr2so4fnhwszxnw9hfmar9rphy4qlpn.png\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plot_act(1, tf.tanh)"
        }, 
        {
            "source": "3D tanh plot. The x-axis is the weight, the y-axis is the bias.\n\n### 3.4 Using tanh in a neural net layer ####", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "act = tf.tanh(tf.matmul(i, w) + b)\nact.eval(session=sess)"
        }, 
        {
            "source": "<a href=\"#ref3\"></a>\n## 4. The Linear Unit functions\nLinear Units are the next step in activation functions. They take concepts from both Step and Sigmoid functions and behave within the best of the two types of functions. Linear Units in general tend to be variation of what is called the Rectified Linear Unit, or ReLU for short.\n\nThe ReLU is a simple function which operates within the $[0,\\infty)$ interval. For the entirety of the negative value domain, it returns a value of 0, while on the positive value domain, it returns $x$ for any $f(x)$.\n\n<img width=\"384\" alt=\"Activation - ReLU\" src=\"https://ibm.box.com/shared/static/kblfxapmiioh20q6vmtd6qu579w689nz.png\">\n\nWhile it may seem counterintuitive to utilize a pseudo-linear function instead of something like Sigmoids, ReLUs provide some benefits which might not be understood at first glance. For example, during the initialization process of a Neural Network model, in which weights are distributed at random for each unit, ReLUs will only activate approximately only in 50% of the times -- which saves some processing power. Additionally, the ReLU structure takes care of what is called the **Vanishing and Exploding Gradient** problem by itself. Another benefit -- if not only marginally relevant to us -- is that this kind of activation function is directly relatable to the nervous system analogy of Neural Networks (this is called *Biological Plausibility*).\n\nThe ReLU structure has also has many variations optimized for certain applications, but those are implemented on a case-by-case basis and therefore aren't in the scope of this notebook. If you want to know more, search for *Parametric Rectified Linear Units* or maybe *Exponential Linear Units*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plot_act(1, tf.nn.relu)"
        }, 
        {
            "source": "3D relu plot. The x-axis is the weight, the y-axis is the bias.\n\n### 4.1 Using relu in a neural net layer ####\n\nTensorFlow has ReLU and some other variants of this function. Take a look:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "act = tf.nn.relu(tf.matmul(i, w) + b)\nact.eval(session=sess)"
        }, 
        {
            "source": "---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This is the end of the **Activation Functions** notebook. Hopefully, now you have a deeper understanding of what activation functions are and what they are used for. Thank you for reading this notebook, and good luck on your studies.\n You can take a look at all TensorFlow Activation Functions in [its reference](https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#activation-functions).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6 (Unsupported)", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}