{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 06: Apache Spark Platform)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n<font color='red' size = 4> Python 2.7 </font>\n## Session R - Case Study: Network Attack Detection\n---\n\nIn this notebook we will use Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) to build a classifier for network attack detection. We will use the  [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets in order to test Spark capabilities with large datasets. \n\n\n### Content\n\n### Part 1  Data Manipulation\n\n1.1 [RDD creation](#rdd)\n\n1.2 [RDD Basics](#rddbasics)\n\n1.3 [Sampling RDD](#samplingrdd)\n\n1.4 [Set operations on RDDs](#set)\n\n1.5 [Data aggregations on RDDs](#aggregation)\n\n1.6 [Working with key/value pair RDDs](#pairrdd)\n\n### Part 2 Data Analytics\n\n2.1 [MLlib: Basic Statistics and Exploratory Data Analysis](#stats)\n\n2.2 [MLlib: Classification with Decision Tree](#dt)\n\n2.3 [MLlib: Classification with Logistic Regression](#lr)\n\n2.4 [Model Selection](#ms)\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n---\n## <span style=\"color:#0b486b\">1. Data Manipulation</span>\n\n\n<a id = \"rdd\"></a>\n### <span style=\"color:#0b486b\">1.1 RDD creation</span>\n\n\nIn this notebook we will introduce two different ways of getting data into the basic Spark data structure, the **Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, transforming existing RDDs, or calling actions on RDDs to compute a result. Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nThe KDD Cup 1999 competition dataset is described in detail [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99).\n\n#### (1.1a) Getting the data files  \n\nIn this section we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million nework interactions. The file is provided as a *Gzip* file that we will download locally.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import urllib\nf = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -l"
        }, 
        {
            "source": "#### (1.1b) Creating a RDD from a file  \n\nThe most common way of creating an RDD is to load it from a file. Notice that Spark's `textFile` can handle compressed files directly.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#/FileStore/tables/kpsy4z8b1491439369206/corrected.gz\n#/FileStore/tables/kpsy4z8b1491439369206/kddcup_data-0c964.gz\n\ndata_file = \"./kddcup.data_10_percent.gz\"\n\nraw_data = sc.textFile(data_file)"
        }, 
        {
            "source": "Now we have our data file loaded into the `raw_data` RDD.\n\nWithout getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw_data.count()"
        }, 
        {
            "source": "We can also check the first few entries in our data.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw_data.take(5)"
        }, 
        {
            "source": "In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"rddbasics\"></a>\n### <span style=\"color:#0b486b\">1.2 RDD Basics</span>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This part will introduce three basic but essential Spark opeations. Two of them are the *transformations* `map` and `filter`. The other is the *action* `collect`. At the same time we will intrudce the concept of *persistence* in Spark.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.2a) The `filter` transformation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a functions is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For example, imagine we want to count how many `normal.` interactions we have in our dataset. We can filter our `raw_data` RDD as follows.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
        }, 
        {
            "source": "Now we can count how many elements do we have in the new RDD.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from time import time\nt0 = time()\nnormal_count = normal_raw_data.count()\ntt = time() - t0\nprint \"There are {} 'normal' interactions\".format(normal_count)\nprint \"Count completed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "Remember from notebook 1 that we have a total of 494021 in our 10 percent dataset. Here we can see that 97278 contain the `normal.` tag word.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Notice that we have measure the elapsed time for counting the elements in the RDD. We have done this because we wanted to point out that actual (distributed) computations in Spark take place when we execute *actions* and not *transformations*. In this case `count` is the action we execute on the RDD. We can apply as many transformations as we want on a our RDD and no computation will take place until we call the first action that, in this case takes a few seconds to complete.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.2b) The `map` transformation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pprint import pprint\ncsv_data = raw_data.map(lambda x: x.split(\",\"))\nt0 = time()\nhead_rows = csv_data.take(5)\ntt = time() - t0\nprint \"Parse completed in {} seconds\".format(round(tt,3))\npprint(head_rows[0])"
        }, 
        {
            "source": "Again, all action happens once we call the first Spark *action* (i.e. *take* in this case). What if we take a lot of elements instead of just the first few?  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\nhead_rows = csv_data.take(100000)\ntt = time() - t0\nprint \"Parse completed in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "We can see that it takes longer. The `map` function is applied now in a  distributed way to a lot of elements on the RDD, hence the longer execution time.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Using `map` and predefined functions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Of course we can use predefined functions with `map`. Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. *normal*) and the value is the whole list of elements that represents the row in the CSV formatted file. We could proceed as follows.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse_interaction(line):\n    elems = line.split(\",\")\n    tag = elems[41]\n    return (tag, elems)\n\nkey_csv_data = raw_data.map(parse_interaction)\nhead_rows = key_csv_data.take(5)\npprint(head_rows[0])"
        }, 
        {
            "source": "That was easy, wasn't it?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In our notebook about working with key-value pairs we will use this type of RDDs to do data aggregations (e.g. count by key).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.2c) The `collect` action", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So far we have used the actions `count` and `take`. Another basic action we need to learn is `collect`. Basically it will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "An example using our raw data.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\nall_raw_data = raw_data.collect()\ntt = time() - t0\nprint \"Data collected in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "That took longer as any other action we used before, of course. Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As a last example combining all the previous, we want to collect all the `normal` interactions as key-value pairs.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get data from file\n#data_file = \"./kddcup.data_10_percent.gz\"\n\n#data_file = \"/FileStore/tables/kpsy4z8b1491439369206/kddcup_data_10_percent-d8e1d.gz\"\n#raw_data = sc.textFile(data_file)\n\n\n# parse into key-value pairs\nkey_csv_data = raw_data.map(parse_interaction)\n\n# filter normal key interactions\nnormal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n\n# collect all\nt0 = time()\nall_normal = normal_key_interactions.collect()\ntt = time() - t0\nnormal_count = len(all_normal)\nprint \"Data collected in {} seconds\".format(round(tt,3))\nprint \"There are {} 'normal' interactions\".format(normal_count)"
        }, 
        {
            "source": "This count matches with the previous count for `normal` interactions. The new procedure is more time consuming. This is because we retrieve all the data with `collect` and then use Python's `len` on the resulting list. Before we were just counting the total number of elements in the RDD by using `count`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"samplingrdd\"></a>\n### <span style=\"color:#0b486b\">1.3 Sampling RDD</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So far we have introduced RDD creation together with some basic transformations such as `map` and `filter` and some actions such as `count`, `take`, and `collect`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This notebook will show how to sample RDDs. Regarding transformations, `sample` will be introduced since it will be useful in many statistical learning scenarios. The we will compare results with the `takeSample` action.      ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.3a) Sampling RDDs   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In Spark, there are two sampling operations, the transformation `sample` and the action `takeSample`. By using a transformation we can tell Spark to apply successive transformation on a sample of a given RDD. By using an action we retrieve a given sample and we can have it in local memory to be used by any other standar library (e.g. Scikit-learn).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### The `sample` transformation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The `sample` transformation takes up to three parameters. First is wether the sampling is done with replacement or not. Second is the sample size as a fraction. Finally we can optionally provide a *random seed*.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "raw_data_sample = raw_data.sample(False, 0.1, 1234)\nsample_size = raw_data_sample.count()\ntotal_size = raw_data.count()\nprint \"Sample size is {} of {}\".format(sample_size, total_size)"
        }, 
        {
            "source": "But the power of sampling as a transformation comes from doing it as part of a sequence of additional transformations. This will show more powerful once we start doing aggregations and key-value pairs operations, and will be specially useful when using Spark's machine learning library MLlib.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In the meantime, imagine we want to have an approximation of the proportion of `normal.` interactions in our dataset. We could do this by counting the total number of tags as we did in previous notebooks. However we want a quicker response and we don't need the exact answer but just an approximation. We can do it as follows.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from time import time\n\n# transformations to be applied\nraw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\nsample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n\n# actions + time\nt0 = time()\nsample_normal_tags_count = sample_normal_tags.count()\ntt = time() - t0\n\nsample_normal_ratio = sample_normal_tags_count / float(sample_size)\nprint \"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)) \nprint \"Count done in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "Let's compare this with calculating the ratio without sampling.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# transformations to be applied\nraw_data_items = raw_data.map(lambda x: x.split(\",\"))\nnormal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n\n# actions + time\nt0 = time()\nnormal_tags_count = normal_tags.count()\ntt = time() - t0\n\nnormal_ratio = normal_tags_count / float(total_size)\nprint \"The ratio of 'normal' interactions is {}\".format(round(normal_ratio,3)) \nprint \"Count done in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "We can see a gain in time. The more transformations we apply after the sampling the bigger this gain. This is because without sampling all the transformations are applied to the complete set of data.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### The `takeSample` action  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "If what we need is to grab a sample of raw data from our RDD into local memory in order to be used by other non-Spark libraries, `takeSample` can be used.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The syntax is very similar, but in this case we specify the number of items instead of the sample size as a fraction of the complete data size.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\nraw_data_sample = raw_data.takeSample(False, 400000, 1234)\nnormal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\ntt = time() - t0\n\nnormal_sample_size = len(normal_data_sample)\n\nnormal_ratio = normal_sample_size / 400000.0\nprint \"The ratio of 'normal' interactions is {}\".format(normal_ratio)\nprint \"Count done in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "The process was very similar as before. We obtained a sample of about 10 percent of the data, and then filter and split.  \n\nHowever, it took longer, even with a slightly smaller sample. The reason is that Spark just distributed the execution of the sampling process. The filtering and splitting of the resuls were done locally in a single node.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"set\"></a>\n### <span style=\"color:#0b486b\">1.4 Set operations on RDDs</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark support many of the operations we have in mathematical sets, such as union and intersection, even when the RDDs themselves are not properly sets. It is important to note that these operations require that the RDDs being operated on are of the same type.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Set operations are quite straighforward to understand as it work as expected. The only consideration comes from the fact that RDDs are not real sets, and therefore operations such as the union of RDDs doesn't remove duplicates. In this notebook we will have a brief look at `substract`, `distinct`, and `cartesian`.       ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.4a) Getting attack interactions using `substract`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For illustrative purposes, imagine we already have our RDD with non attack (normal) interactions from some previous analysis.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
        }, 
        {
            "source": "We can obtain attack interactions by substracting normal ones from the original unfiltered RDD as follows.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "attack_raw_data = raw_data.subtract(normal_raw_data)"
        }, 
        {
            "source": "Let's do some counts to check our results.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from time import time\n\n# count all\nt0 = time()\nraw_data_count = raw_data.count()\ntt = time() - t0\nprint \"All count in {} secs\".format(round(tt,3))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# count normal\nt0 = time()\nnormal_raw_data_count = normal_raw_data.count()\ntt = time() - t0\nprint \"Normal count in {} secs\".format(round(tt,3))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# count attacks\nt0 = time()\nattack_raw_data_count = attack_raw_data.count()\ntt = time() - t0\nprint \"Attack count in {} secs\".format(round(tt,3))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"There are {} normal interactions and {} attacks, \\\nfrom a total of {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count)"
        }, 
        {
            "source": "So now we have two RDDs, one with normal interactions and another one with attacks.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.4b) Protocol and service combinations using `cartesian`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can compute the Cartesian product between two RDDs by using the `cartesian` transformation. It returns all possible pairs of elements between two RDDs. In our case we will use it to generate all the possible combinations between service and protocol in our network interactions.  \n\nFirst of all we need to isolate each collection of values in two separate RDDs. For that we will use `distinct` on the CSV-parsed dataset. From the [dataset description](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) we know that protocol is the second column and service is the third (tag is the last one and not the first as appears in the page).   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So first, let's get the protocols.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csv_data = raw_data.map(lambda x: x.split(\",\"))\nprotocols = csv_data.map(lambda x: x[1]).distinct()\nprotocols.collect()"
        }, 
        {
            "source": "Now we do the same for services.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "services = csv_data.map(lambda x: x[2]).distinct()\nservices.collect()"
        }, 
        {
            "source": "A longer list in this case.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now we can do the cartesian product.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "product = protocols.cartesian(services).collect()\nprint \"There are {} combinations of protocol X service\".format(len(product))"
        }, 
        {
            "source": "Obviously, for such small RDDs doesn't really makes sense to use Spark cartesian product. We could have perfectly collected the values after using `distinct` and do the cartesian product locally. Moreover, `distinct` and `cartesian` are expensive operations so they must be used with care when the operating datasets are large.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"aggregation\"></a>\n### <span style=\"color:#0b486b\">1.5 Data aggregations on RDDs</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can aggregate RDD data in Spark by using three different actions: `reduce`, `fold`, and `aggregate`. The last one is the more general one and someway includes the first two.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.5a) Inspecting interaction duration by tag", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Both `fold` and `reduce` take a function as an argument that is applied to two elements of the RDD. The `fold` action differs from `reduce` in that it gets and additional initial *zero value* to be used for the initial call. This value should be the identity element for the function provided.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As an example, imagine we want to know the total duration of our interactions for normal and attack interactions. We can use `reduce` as follows.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# parse data\ncsv_data = raw_data.map(lambda x: x.split(\",\"))\n\n# separate into different RDDs\nnormal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\nattack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
        }, 
        {
            "source": "The function that we pass to `reduce` gets and returns elements of the same type of the RDD. If we want to sum durations we need to extract that element into a new RDD.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\nattack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
        }, 
        {
            "source": "Now we can reduce these new RDDs.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\ntotal_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n\nprint \"Total duration for 'normal' interactions is {}\".\\\n    format(total_normal_duration)\nprint \"Total duration for 'attack' interactions is {}\".\\\n    format(total_attack_duration)"
        }, 
        {
            "source": "We can go further and use counts to calculate duration means.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_count = normal_duration_data.count()\nattack_count = attack_duration_data.count()\n\nprint \"Mean duration for 'normal' interactions is {}\".\\\n    format(round(total_normal_duration/float(normal_count),3))\nprint \"Mean duration for 'attack' interactions is {}\".\\\n    format(round(total_attack_duration/float(attack_count),3))"
        }, 
        {
            "source": "We have a first (and too simplistic) approach to identify attack interactions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.5b) A better way, using `aggregate`  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The `aggregate` action frees us from the constraint of having the return be the same type as the RDD we are working on. Like with `fold`, we supply an initial zero value of the type we want to return. Then we provide two functions. The first one is used to combine the elements from our RDD with the accumulator. The second function is needed to merge two accumulators. Let's see it in action calculating the mean we did before.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_sum_count = normal_duration_data.aggregate(\n    (0,0), # the initial value\n    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n)\n\nprint \"Mean duration for 'normal' interactions is {}\".\\\n    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3))"
        }, 
        {
            "source": "In the previous aggregation, the accumulator first element keeps the total sum, while the second element keeps the count. Combining an accumulator with an RDD element consists in suming up the value and incrementing the count. Combining two accumulators requires just a pairwise sum.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can do the same with attack type interactions.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "attack_sum_count = attack_duration_data.aggregate(\n    (0,0), # the initial value\n    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n)\n\nprint \"Mean duration for 'attack' interactions is {}\".\\\n    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3))"
        }, 
        {
            "source": "\n<a id = \"pairrdd\"></a>\n### <span style=\"color:#0b486b\">1.6 Working with key/value pair RDDs</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark provides specific functions to deal with RDDs which elements are key/value pairs. They are sually used to perform aggregations and other processings by key.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this notebook we will show how, by working with key/value pairs, we can process our network interactions dataset in a more practical and powerful way than that used in previous notebooks. Key/value pair aggregations will show to be particularly effective when trying to explore each type of tag in our network attacks, in an individual way.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (1.6a) Creating a pair RDD for interaction types", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this notebook we want to do some exploratory data analysis on our network interactions dataset. More concretly we want to profile each network interaction type in terms of some of its variables such as duration. In order to do so, we first need to create the RDD suitable for that, where each interaction is parsed as a CSV row representing the value, and is put together with its corresponding tag as a key.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Normally we create key/value pair RDDs by applying a function using `map` to the original data. This function returns the corresponding pair for a given RDD element. We can proceed as follows.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csv_data = raw_data.map(lambda x: x.split(\",\"))\nkey_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag"
        }, 
        {
            "source": "We have now our key/value pair data ready to be used. Let's get the first element in order to see how it looks like.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "key_value_data.take(1)"
        }, 
        {
            "source": "#### (1.6b) Data aggregations with key/value pair RDDs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements. Additionally, Spark provides specific functions to work with RDDs containing pair elements. They are very similar to those available for general RDDs.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For example, we have a `reduceByKey` transformation that we can use as follows to calculate the total duration of each network interaction type.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \ndurations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n\ndurations_by_key.collect()"
        }, 
        {
            "source": "We have a specific counting action for key/value pairs.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "counts_by_key = key_value_data.countByKey()\ncounts_by_key"
        }, 
        {
            "source": "##### Using `combineByKey`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. We can think about it as the `aggregate` equivlent since it allows the user to return values that are not the same type as our input data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For example, we can use it to calculate per-type average durations as follows.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sum_counts = key_value_duration.combineByKey(\n    (lambda x: (x, 1)), # the initial value, with value x and count 1\n    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n)\n\nsum_counts.collectAsMap()"
        }, 
        {
            "source": "We can see that the arguments are pretty similar to those passed to `aggregate` in the previous notebook. The result associated to each type is in the form of a pair. If we want to actually get the averages, we need to do the division before collecting the results.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "duration_means_by_type = sum_counts.map(lambda (key,value): (key, round(value[0]/value[1],3))).collectAsMap()\n\n# Print them sorted\nfor tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n    print tag, duration_means_by_type[tag]"
        }, 
        {
            "source": "A small step into understanding what makes a network interaction be considered an attack.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"stats\"></a>\n\n### <span style=\"color:#0b486b\">2.1 MLlib: Basic Statistics and Exploratory Data Analysis</span>\n\n\nSo far we have used different map and aggregation functions, on simple and key/value pair RDD's, in order to get simple statistics that help us understand our datasets. In this notebook we will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) through its basic statistics functionality in order to better understand our dataset. We will use the reduced 10-percent [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets through the notebook.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.1a) Getting the data and creating the RDD", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import urllib\nf = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n\ndata_file = \"./kddcup.data_10_percent.gz\"\n\nraw_data = sc.textFile(data_file)"
        }, 
        {
            "source": "#### (2.1b) Local vectors", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A [local vector](https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector) is often used as a base type for RDDs in Spark MLlib. A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For dense vectors, MLlib uses either Python *lists* or the *NumPy* `array` type. The later is recommended, so you can simply pass NumPy arrays around.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For sparse vectors, users can construct a `SparseVector` object from MLlib or pass *SciPy* `scipy.sparse` column vectors if SciPy is available in their environment. The easiest way to create sparse vectors is to use the factory methods implemented in `Vectors`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### An RDD of dense vectors", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's represent each network interaction in our dataset as a dense vector. For that we will use the *NumPy* `array` type.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\n\ndef parse_interaction(line):\n    line_split = line.split(\",\")\n    # keep just numeric and logical values\n    symbolic_indexes = [1,2,3,41]\n    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n    return np.array([float(x) for x in clean_line_split])\n\nvector_data = raw_data.map(parse_interaction)"
        }, 
        {
            "source": "#### (2.1c) Summary statistics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark's MLlib provides column summary statistics for `RDD[Vector]` through the function [`colStats`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics.colStats) available in [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics). The method returns an instance of [`MultivariateStatisticalSummary`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateStatisticalSummary), which contains the column-wise *max*, *min*, *mean*, *variance*, and *number of nonzeros*, as well as the *total count*.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.stat import Statistics \nfrom math import sqrt \n\n# Compute column summary statistics.\nsummary = Statistics.colStats(vector_data)\n\nprint \"Duration Statistics:\"\nprint \" Mean: {}\".format(round(summary.mean()[0],3))\nprint \" St. deviation: {}\".format(round(sqrt(summary.variance()[0]),3))\nprint \" Max value: {}\".format(round(summary.max()[0],3))\nprint \" Min value: {}\".format(round(summary.min()[0],3))\nprint \" Total value count: {}\".format(summary.count())\nprint \" Number of non-zero values: {}\".format(summary.numNonzeros()[0])"
        }, 
        {
            "source": "##### Summary statistics by label  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The interesting part of summary statistics, in our case, comes from being able to obtain them by the type of network attack or 'label' in our dataset. By doing so we will be able to better characterise our dataset dependent variable in terms of the independent variables range of values.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "If we want to do such a thing we could filter our RDD containing labels as keys and vectors as values. For that we just need to adapt our `parse_interaction` function to return a tuple with both elements.     ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse_interaction_with_key(line):\n    line_split = line.split(\",\")\n    # keep just numeric and logical values\n    symbolic_indexes = [1,2,3,41]\n    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n    return (line_split[41], np.array([float(x) for x in clean_line_split]))\n\nlabel_vector_data = raw_data.map(parse_interaction_with_key)"
        }, 
        {
            "source": "The next step is not very sofisticated. We use `filter` on the RDD to leave out other labels but the one we want to gather statistics from.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_label_data = label_vector_data.filter(lambda x: x[0]==\"normal.\")"
        }, 
        {
            "source": "Now we can use the new RDD to call `colStats` on the values.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_summary = Statistics.colStats(normal_label_data.values())"
        }, 
        {
            "source": "And collect the results as we did before.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"Duration Statistics for label: {}\".format(\"normal\")\nprint \" Mean: {}\".format(normal_summary.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(normal_summary.variance()[0]),3))\nprint \" Max value: {}\".format(round(normal_summary.max()[0],3))\nprint \" Min value: {}\".format(round(normal_summary.min()[0],3))\nprint \" Total value count: {}\".format(normal_summary.count())\nprint \" Number of non-zero values: {}\".format(normal_summary.numNonzeros()[0])"
        }, 
        {
            "source": "Instead of working with a key/value pair we could have just filter our raw data split using the label in column 41. Then we can parse the results as we did before. This will work as well. However having our data organised as key/value pairs will open the door to better manipulations. Since `values()` is a transformation on an RDD, and not an action, we don't perform any computation until we call `colStats` anyway.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "But lets wrap this within a function so we can reuse it with any label.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def summary_by_label(raw_data, label):\n    label_vector_data = raw_data.map(parse_interaction_with_key).filter(lambda x: x[0]==label)\n    return Statistics.colStats(label_vector_data.values())"
        }, 
        {
            "source": "Let's give it a try with the \"normal.\" label again.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "normal_sum = summary_by_label(raw_data, \"normal.\")\n\nprint \"Duration Statistics for label: {}\".format(\"normal\")\nprint \" Mean: {}\".format(normal_sum.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(normal_sum.variance()[0]),3))\nprint \" Max value: {}\".format(round(normal_sum.max()[0],3))\nprint \" Min value: {}\".format(round(normal_sum.min()[0],3))\nprint \" Total value count: {}\".format(normal_sum.count())\nprint \" Number of non-zero values: {}\".format(normal_sum.numNonzeros()[0])"
        }, 
        {
            "source": "Let's try now with some network attack. We have all of them listed [here](http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "guess_passwd_summary = summary_by_label(raw_data, \"guess_passwd.\")\n\nprint \"Duration Statistics for label: {}\".format(\"guess_password\")\nprint \" Mean: {}\".format(guess_passwd_summary.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(guess_passwd_summary.variance()[0]),3))\nprint \" Max value: {}\".format(round(guess_passwd_summary.max()[0],3))\nprint \" Min value: {}\".format(round(guess_passwd_summary.min()[0],3))\nprint \" Total value count: {}\".format(guess_passwd_summary.count())\nprint \" Number of non-zero values: {}\".format(guess_passwd_summary.numNonzeros()[0])"
        }, 
        {
            "source": "We can see that this type of attack is shorter in duration than a normal interaction. We could build a table with duration statistics for each type of interaction in our dataset. First we need to get a list of labels as described in the first line [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).      ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "label_list = [\"back.\",\"buffer_overflow.\",\"ftp_write.\",\"guess_passwd.\",\n              \"imap.\",\"ipsweep.\",\"land.\",\"loadmodule.\",\"multihop.\",\n              \"neptune.\",\"nmap.\",\"normal.\",\"perl.\",\"phf.\",\"pod.\",\"portsweep.\",\n              \"rootkit.\",\"satan.\",\"smurf.\",\"spy.\",\"teardrop.\",\"warezclient.\",\n              \"warezmaster.\"]"
        }, 
        {
            "source": "Then we get a list of statistics for each label.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "stats_by_label = [(label, summary_by_label(raw_data, label)) for label in label_list]"
        }, 
        {
            "source": "Now we get the *duration* column, first in our dataset (i.e. index 0).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "duration_by_label = [ \n    (stat[0], np.array([float(stat[1].mean()[0]), float(sqrt(stat[1].variance()[0])), float(stat[1].min()[0]), float(stat[1].max()[0]), int(stat[1].count())])) \n    for stat in stats_by_label]"
        }, 
        {
            "source": "That we can put into a Pandas data frame.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\npd.set_option('display.max_columns', 50)\n\nstats_by_label_df = pd.DataFrame.from_items(duration_by_label, columns=[\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"], orient='index')"
        }, 
        {
            "source": "And print it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"Duration statistics, by label\"\nstats_by_label_df"
        }, 
        {
            "source": "In order to reuse this code and get a dataframe from any variable in our dataset we will define a function.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_variable_stats_df(stats_by_label, column_i):\n    column_stats_by_label = [\n        (stat[0], np.array([float(stat[1].mean()[column_i]), float(sqrt(stat[1].variance()[column_i])), float(stat[1].min()[column_i]), float(stat[1].max()[column_i]), int(stat[1].count())])) \n        for stat in stats_by_label\n    ]\n    return pd.DataFrame.from_items(column_stats_by_label, columns=[\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"], orient='index')"
        }, 
        {
            "source": "Let's try for *duration* again.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "get_variable_stats_df(stats_by_label,0)"
        }, 
        {
            "source": "Now for the next numeric column in the dataset, *src_bytes*.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"src_bytes statistics, by label\"\nget_variable_stats_df(stats_by_label,1)"
        }, 
        {
            "source": "And so on. By reusing the `summary_by_label` and `get_variable_stats_df` functions we can perform some exploratory data analysis in large datasets with Spark.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.1d) Correlations", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Spark's MLlib supports [Pearson\u2019s](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) and [Spearman\u2019s](http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) to calculate pairwise correlation methods among many series. Both of them are provided by the `corr` method in the `Statistics` package.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We have two options as input. Either two `RDD[Double]`s or an `RDD[Vector]`. In the first case the output will be a `Double` value, while in the second a whole correlation Matrix. Due to the nature of our data, we will obtain the second.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.stat import Statistics \ncorrelation_matrix = Statistics.corr(vector_data, method=\"spearman\")"
        }, 
        {
            "source": "Once we have the correlations ready, we can start inspecting their values.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\npd.set_option('display.max_columns', 50)\n\ncol_names = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n\ncorr_df = pd.DataFrame(correlation_matrix, index=col_names, columns=col_names)\n\ncorr_df"
        }, 
        {
            "source": "We have used a *Pandas* `DataFrame` here to render the correlation matrix in a more comprehensive way. Now we want those variables that are highly correlated. For that we do a bit of dataframe manipulation.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get a boolean dataframe where true means that a pair of variables is highly correlated\nhighly_correlated_df = (abs(corr_df) > .8) & (corr_df < 1.0)\n# get the names of the variables so we can use them to slice the dataframe\ncorrelated_vars_index = (highly_correlated_df==True).any()\ncorrelated_var_names = correlated_vars_index[correlated_vars_index==True].index\n# slice it\nhighly_correlated_df.loc[correlated_var_names,correlated_var_names]"
        }, 
        {
            "source": "##### Conclusions and posible model selection hints", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The previous dataframe showed us which variables are highly correlated. We have kept just those variables with at least one strong correlation. We can use as we please, but a good way could be to do some model selection. That is, if we have a group of variables that are highly correlated, we can keep just one of them to represent the group under the assumption that they convey similar information as predictors. Reducing the number of variables will not improve our model accuracy, but it will make it easier to understand and also more efficient to compute.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For example, from the description of the [KDD Cup 99 task](http://kdd.ics.uci.edu/databases/kddcup99/task.html) we know that the variable `dst_host_same_src_port_rate` references the percentage of the last 100 connections to the same port, for the same destination host. In our correlation matrix (and auxiliar dataframes) we find that this one is highly and positively correlated to `src_bytes` and `srv_count`. The former is the number of bytes sent form source to destination. The later is the number of connections to the same service as the current connection in the past 2 seconds. We might decide not to include `dst_host_same_src_port_rate` in our model if we include the other two, as a way to reduce the number of variables and later one better interpret our models.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Later on, in those notebooks dedicated to build predictive models, we will make use of this information to build more interpretable models.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"dt\"></a>\n### <span style=\"color:#0b486b\">2.2 MLlib: Classification with Decision Tree</span>\n\nDecision trees are a popular machine learning tool in part because they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. In this notebook, we will first train a classification tree including every single predictor. Then we will use our results to perform model selection. Once we find out the most important ones (the main splits in the tree) we will build a minimal tree using just three of them (the first two levels of the tree in order to compare performance and accuracy.   \n\n\nIn this section we will train a *classification tree* that, as we did with *logistic regression*, will predict if a network interaction is either `normal` or `attack`.  \n\nTraining a classification tree using [MLlib](https://spark.apache.org/docs/latest/mllib-decision-tree.html) requires some parameters:  \n- Training data  \n- Num classes  \n- Categorical features info: a map from column to categorical variables arity. This is optional, although it should increase model accuracy. However it requires that we know the levels in our categorical variables in advance. second we need to parse our data to convert labels to integer values within the arity range.  \n- Impurity metric  \n- Tree maximum depth  \n- And tree maximum number of bins  \n\nWe will use a complete dataset provided for the [KDD Cup 1999](http://kdd.ics.uciW.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import urllib\nf = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n\ndata_file = \"./kddcup.data.gz\"\nraw_data = sc.textFile(data_file)\n\nprint \"Train data size is {}\".format(raw_data.count())"
        }, 
        {
            "source": "The [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) also provide test data that we will load in a separate RDD.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n\ntest_data_file = \"./corrected.gz\"\ntest_raw_data = sc.textFile(test_data_file)\n\nprint \"Test data size is {}\".format(test_raw_data.count())"
        }, 
        {
            "source": "#### (2.2a) Preparing the data\n\nIn this section we will see how to obtain all the labels within a dataset and convert them to numerical factors.  \n\nAs we said, in order to benefits from trees hability to seamlessly with categporical variables, we need to convert them to numerical factors. But first we need to obtain all the possible levels. We will use *set* transformations on a csv parsed RDD.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.regression import LabeledPoint\nfrom numpy import array\n\ncsv_data = raw_data.map(lambda x: x.split(\",\"))\ntest_csv_data = test_raw_data.map(lambda x: x.split(\",\"))\n\nprotocols = csv_data.map(lambda x: x[1]).distinct().collect()\nservices = csv_data.map(lambda x: x[2]).distinct().collect()\nflags = csv_data.map(lambda x: x[3]).distinct().collect()"
        }, 
        {
            "source": "And now we can use this Python lists in our `create_labeled_point` function. If a factor level is not in the training data, we assign an especial level. Remember that we cannot use testing data for training our model, not even the factor levels. The testing data represents the unknown to us in a real case.     ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def create_labeled_point(line_split):\n    # leave_out = [41]\n    clean_line_split = line_split[0:41]\n    \n    # convert protocol to numeric categorical variable\n    try: \n        clean_line_split[1] = protocols.index(clean_line_split[1])\n    except:\n        clean_line_split[1] = len(protocols)\n        \n    # convert service to numeric categorical variable\n    try:\n        clean_line_split[2] = services.index(clean_line_split[2])\n    except:\n        clean_line_split[2] = len(services)\n    \n    # convert flag to numeric categorical variable\n    try:\n        clean_line_split[3] = flags.index(clean_line_split[3])\n    except:\n        clean_line_split[3] = len(flags)\n    \n    # convert label to binary label\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n        \n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ntraining_data = csv_data.map(create_labeled_point)\ntest_data = test_csv_data.map(create_labeled_point)"
        }, 
        {
            "source": "#### (2.2b) Training a classifier", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We are now ready to train our classification tree. We will keep the `maxDepth` value small. This will lead to smaller accuracy, but we will obtain less splits so later on we can better interpret the tree. In a production system we will try to increase this value in order to find a better accuracy.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\nfrom time import time\n\n# Build the model\nt0 = time()\ntree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n                                          impurity='gini', maxDepth=4, maxBins=100)\ntt = time() - t0\n\nprint \"Classifier trained in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "#### (2.2c) Evaluating the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In order to measure the classification error on our test data, we use `map` on the `test_data` RDD and the model to predict each test point class. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions = tree_model.predict(test_data.map(lambda p: p.features))\nlabels_and_preds = test_data.map(lambda p: p.label).zip(predictions)"
        }, 
        {
            "source": "Classification results are returned in pars, with the actual test label and the predicted one. This is used to calculate the classification error by using `filter` and `count` as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ntest_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(test_data.count())\ntt = time() - t0\n\nprint \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
        }, 
        {
            "source": "*NOTE: the zip transformation doesn't work properly with pySpark 1.2.1. It does in 1.3*", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.2d) Interpreting the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Understanding our tree splits is a great excersise in order to explain our classification labels in terms of predictors and the values they take. Using the `toDebugString` method in our three model we can obtain a lot of information regarding splits, nodes, etc.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"Learned classification tree model:\"\nprint tree_model.toDebugString()"
        }, 
        {
            "source": "For example, a network interaction with the following features (see description [here](http://kdd.ics.uci.edu/databases/kddcup99/task.html)) will be classified as an attack by our model:  \n- `count`, the number of connections to the same host as the current connection in the past two seconds, being greater than 32. \n- `dst_bytes`, the number of data bytes from destination to source, is 0.  \n- `service` is neither level 0 nor 52.  \n- `logged_in` is false.  \nFrom our services list we know that:  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print \"Service 0 is {}\".format(services[0])\nprint \"Service 52 is {}\".format(services[52])"
        }, 
        {
            "source": "So we can caracterise network interactions with more than 32 connections to the same server in the last 2 seconds, transferring zero bytes from destination to source, where service is neither *urp_i* nor *tftp_u*, and not logged in, as network attacks. A similar approach can be used for each tree terminal node.     ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can see that `count` is the first node split in the tree. Remember that each partition is chosen greedily by selecting the best split from a set of possible splits, in order to maximize the information gain at a tree node (see more [here](https://spark.apache.org/docs/latest/mllib-decision-tree.html#basic-algorithm)). At a second level we find variables `flag` (normal or error status of the connection) and `dst_bytes` (the number of data bytes from destination to source) and so on.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This explaining capability of a classification (or regression) tree is one of its main benefits. Understaining data is a key factor to build better models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.2e) Building a minimal model using the three main splits", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So now that we know the main features predicting a network attack, thanks to our classification tree splits, let's use them to build a minimal classification tree with just the main three variables: `count`, `dst_bytes`, and `flag`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We need to define the appropriate function to create labeled points.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def create_labeled_point_minimal(line_split):\n    # leave_out = [41]\n    clean_line_split = line_split[3:4] + line_split[5:6] + line_split[22:23]\n    \n    # convert flag to numeric categorical variable\n    try:\n        clean_line_split[0] = flags.index(clean_line_split[0])\n    except:\n        clean_line_split[0] = len(flags)\n    \n    # convert label to binary label\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n        \n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ntraining_data_minimal = csv_data.map(create_labeled_point_minimal)\ntest_data_minimal = test_csv_data.map(create_labeled_point_minimal)"
        }, 
        {
            "source": "That we use to train the model.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build the model\nt0 = time()\ntree_model_minimal = DecisionTree.trainClassifier(training_data_minimal, numClasses=2, \n                                          categoricalFeaturesInfo={0: len(flags)},\n                                          impurity='gini', maxDepth=3, maxBins=32)\ntt = time() - t0\n\nprint \"Classifier trained in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "Now we can predict on the testing data and calculate accuracy.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions_minimal = tree_model_minimal.predict(test_data_minimal.map(lambda p: p.features))\nlabels_and_preds_minimal = test_data_minimal.map(lambda p: p.label).zip(predictions_minimal)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ntest_accuracy = labels_and_preds_minimal.filter(lambda (v, p): v == p).count() / float(test_data_minimal.count())\ntt = time() - t0\n\nprint \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
        }, 
        {
            "source": "So we have trained a classification tree with just the three most important predictors, in half of the time, and with a not so bad accuracy. In fact, a classification tree is a very good model selection tool!    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"lr\"></a>\n### <span style=\"color:#0b486b\">2.3 MLlib: Classification with Logistic Regression</span>\n\n\n\nIn this section, we will use Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) to build a **Logistic Regression** classifier for network attack detection. We will use the complete [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets in order to test Spark capabilities with large datasets.  \n\nAdditionally, we will introduce two ways of performing **model selection**: by using a correlation matrix and by using hypothesis testing.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.3a) Getting the data and creating the RDD\n\nAs we said, this time we will use the complete dataset provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -l"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#import urllib\n#f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n\ndata_file = \"./kddcup.data.gz\"\nraw_data = sc.textFile(data_file)\n\nprint \"Train data size is {}\".format(raw_data.count())"
        }, 
        {
            "source": "The [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) also provide test data that we will load in a separate RDD.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n\ntest_data_file = \"./corrected.gz\"\ntest_raw_data = sc.textFile(test_data_file)\n\nprint \"Test data size is {}\".format(test_raw_data.count())"
        }, 
        {
            "source": "#### (2.3b) Labeled Points", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A labeled point is a local vector associated with a label/response. In [MLlib](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point), labeled points are used in supervised learning algorithms and they are stored as doubles. For binary classification, a label should be either 0 (negative) or 1 (positive).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Preparing the training data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In our case, we are intrested in detecting network attacks in general. We don't need to detect which type of attack we are dealing with. Therefore we will tag each network interaction as non attack (i.e. 'normal' tag) or attack (i.e. anything else but 'normal').  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.regression import LabeledPoint\nfrom numpy import array\n\ndef parse_interaction(line):\n    line_split = line.split(\",\")\n    # leave_out = [1,2,3,41]\n    clean_line_split = line_split[0:1]+line_split[4:41]\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ntraining_data = raw_data.map(parse_interaction)"
        }, 
        {
            "source": "##### Preparing the test data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Similarly, we process our test data file.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "test_data = test_raw_data.map(parse_interaction)"
        }, 
        {
            "source": "#### (2.3c) Detecting network attacks using Logistic Regression", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "[Logistic regression](http://en.wikipedia.org/wiki/Logistic_regression) is widely used to predict a binary response. Spark implements [two algorithms](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression) to solve logistic regression: mini-batch gradient descent and L-BFGS. L-BFGS is recommended over mini-batch gradient descent for faster convergence.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Training a classifier", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# use SparkSession instead? to be fixed\n\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\nfrom time import time\n\n# Build the model\nt0 = time()\nlogit_model = LogisticRegressionWithLBFGS.train(training_data)\ntt = time() - t0\n\nprint \"Classifier trained in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "##### Evaluating the model on new data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In order to measure the classification error on our test data, we use `map` on the `test_data` RDD and the model to predict each test point class. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "labels_and_preds = test_data.map(lambda p: (p.label, logit_model.predict(p.features)))"
        }, 
        {
            "source": "Classification results are returned in pars, with the actual test label and the predicted one. This is used to calculate the classification error by using `filter` and `count` as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t0 = time()\ntest_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(test_data.count())\ntt = time() - t0\n\nprint \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
        }, 
        {
            "source": "That's a decent accuracy. We know that there is space for improvement with a better variable selection and also by including categorical variables (e.g. we have excluded 'protocol' and 'service').", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"ms\"></a>\n### <span style=\"color:#0b486b\">2.4 Model selection</span>\n\n\n\nModel or feature selection helps us building more interpretable and efficient models (or a classifier in this case). For ilustrative purposes, we will follow two different approaches, correlation matrices and hypothesis testing.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.4a) Using a correlation matrix", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We calculated a correlation matrix in order to find predictors that are highly correlated. There are many possitble choices there in order to simplify our model. We can pick differen combinations of correlated variables and leave just those that represent them. The reader can try different combinations. Here we will choose the following for illustrative purposes:  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- From the description of the [KDD Cup 99 task](http://kdd.ics.uci.edu/databases/kddcup99/task.html) we know that the variable `dst_host_same_src_port_rate` references the percentage of the last 100 connections to the same port, for the same destination host. In our correlation matrix (and auxiliar dataframes) we find that this one is highly and positively correlated to `src_bytes` and `srv_count`. The former is the number of bytes sent form source to destination. The later is the number of connections to the same service as the current connection in the past 2 seconds. We decide not to include `dst_host_same_src_port_rate` in our model since we include the other two.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Variables `serror_rate` and `srv_error_rate` (% of connections that have *SYN* errors for same host and same service respectively) are highly positively correlated. Moreover, the set of variables that they highly correlate with are pretty much the same. They look like contributing very similarly to our model. We will keep just `serror_rate`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- A similar situation happens with `rerror_rate` and `srv_rerror_rate` (% of connections that have *REJ* errors) so we will keep just `rerror_rate`.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Same thing with variables prefixed with `dst_host_` for the previous ones (e.g. `dst_host_srv_serror_rate`).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We will stop here, although the reader can keep experimenting removing correlated variables has before (e.g. `same_srv_rate` and `diff_srv_rate` are good candidates. Our list of variables we will drop includes:  \n- `dst_host_same_src_port_rate`, (column 35).  \n- `srv_serror_rate` (column 25).  \n- `srv_rerror_rate` (column 27).  \n- `dst_host_srv_serror_rate` (column 38).  \n- `dst_host_srv_rerror_rate` (column 40).  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.4b) Evaluating the new model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's proceed with the evaluation of our reduced model. First we need to provide training and testing datasets containing just the selected variables. For that we will define a new function ot parse th raw data that keeps just what we need.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse_interaction_corr(line):\n    line_split = line.split(\",\")\n    # leave_out = [1,2,3,25,27,35,38,40,41]\n    clean_line_split = line_split[0:1]+line_split[4:25]+line_split[26:27]+line_split[28:35]+line_split[36:38]+line_split[39:40]\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ncorr_reduced_training_data = raw_data.map(parse_interaction_corr)\ncorr_reduced_test_data = test_raw_data.map(parse_interaction_corr)"
        }, 
        {
            "source": "*Note: when selecting elements in the split, a list comprehension with a `leave_out` list for filtering is more Pythonic than slicing and concatenation indeed, but we have found it less efficient. This is very important when dealing with large datasets. The `parse_interaction` functions will be called for every element in the RDD, so we need to make them as efficient as possible.*  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now we can train the model.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build the model\nt0 = time()\nlogit_model_2 = LogisticRegressionWithLBFGS.train(corr_reduced_training_data)\ntt = time() - t0\n\nprint \"Classifier trained in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "And evaluate its accuracy on the test data.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "labels_and_preds = corr_reduced_test_data.map(lambda p: (p.label, logit_model_2.predict(p.features)))\nt0 = time()\ntest_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(corr_reduced_test_data.count())\ntt = time() - t0\n\nprint \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
        }, 
        {
            "source": "As expected, we have reduced accuracy and also training time. However this doesn't seem a good trade! At least not for logistic regression and considering the predictors we decided to leave out. We have lost quite a lot of accuracy and have not gained a lot of execution time during training. Moreoever prediction time didn't improve.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.4c) Using hypothesis testing", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Hypothesis testing is a powerful tool in statistical inference and learning to determine whether a result is statistically significant. MLlib supports Pearson's chi-squared ( \u03c72) tests for goodness of fit and independence. The goodness of fit test requires an input type of `Vector`, whereas the independence test requires a `Matrix` as input. Moreover, MLlib also supports the input type `RDD[LabeledPoint]` to enable feature selection via chi-squared independence tests. Again, these methods are part of the [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics) package.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In our case we want to perform some sort of feature selection, so we will provide an RDD of `LabeledPoint`. Internally, MLlib will calculate a contingency matrix and perform the Persons's chi-squared (\u03c72) test. Features need to be categorical. Real-valued features will be treated as categorical in each of its different values. There is a limit of 1000 different values, so we need either to leave out some features or categorise them. In this case, we will consider just features that either take boolean values or just a few different numeric values in our dataset. We could overcome this limitation by defining a more complex `parse_interaction` function that categorises each feature properly.       ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "feature_names = [\"land\",\"wrong_fragment\",\n             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse_interaction_categorical(line):\n    line_split = line.split(\",\")\n    clean_line_split = line_split[6:41]\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ntraining_data_categorical = raw_data.map(parse_interaction_categorical)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.stat import Statistics\n\nchi = Statistics.chiSqTest(training_data_categorical)"
        }, 
        {
            "source": "Now we can check the resulting values after putting them into a *Pandas* data frame.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\npd.set_option('display.max_colwidth', 30)\n\nrecords = [(result.statistic, result.pValue) for result in chi]\n\nchi_df = pd.DataFrame(data=records, index= feature_names, columns=[\"Statistic\",\"p-value\"])\n\nchi_df "
        }, 
        {
            "source": "From that we conclude that predictors `land` and `num_outbound_cmds` could be removed from our model without affecting our accuracy dramatically. Let's try this.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### (2.4d) Evaluating the new model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So the only modification to our first `parse_interaction` function will be to remove columns 6 and 19, corresponding to the two predictors that we want not to be part of our model.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse_interaction_chi(line):\n    line_split = line.split(\",\")\n    # leave_out = [1,2,3,6,19,41]\n    clean_line_split = line_split[0:1] + line_split[4:6] + line_split[7:19] + line_split[20:41]\n    attack = 1.0\n    if line_split[41]=='normal.':\n        attack = 0.0\n    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n\ntraining_data_chi = raw_data.map(parse_interaction_chi)\ntest_data_chi = test_raw_data.map(parse_interaction_chi)"
        }, 
        {
            "source": "Now we build the logistic regression classifier again.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build the model\nt0 = time()\nlogit_model_chi = LogisticRegressionWithLBFGS.train(training_data_chi)\ntt = time() - t0\n\nprint \"Classifier trained in {} seconds\".format(round(tt,3))"
        }, 
        {
            "source": "And evaluate in test data.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "labels_and_preds = test_data_chi.map(lambda p: (p.label, logit_model_chi.predict(p.features)))\nt0 = time()\ntest_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(test_data_chi.count())\ntt = time() - t0\n\nprint \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
        }, 
        {
            "source": "So we can see that, by using hypothesis testing, we have been able to remove two predictors without diminishing testing accuracy at all. Training time improved a bit as well. This might now seem like a big model reduction, but it is something when dealing with big data sources. Moreover, we should be able to categorise those five predictors we have left out for different reasons and, either include them in the model or leave them out if they aren't statistically significant.  \n\nAdditionally, we could try to remove some of those predictors that are highly correlated, trying not to reduce accuracy too much. In the end, we should end up with a model easier to understand and use.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2.7", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}