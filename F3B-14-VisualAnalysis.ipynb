{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(03):  Deep Learning\n",
    "**(Module 01: Deep Learning)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session 14 - Visual Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In some of the previous tutorials on Convolutional Neural Networks, we showed the convolutional filter weights, see e.g. Tutorial #02 and #06. But it was impossible to determine what the convolutional filters might be recognizing in the input image from merely looking at the filter-weights.\n",
    "\n",
    "In this tutorial we will present a basic method for visually analysing the inner-workings of a neural network. The idea is to generate an image that maximizes individual features inside the neural network. The image is initialized with a little random noise and then gradually changed using the gradient of the given feature with regard to the input image.\n",
    "\n",
    "This method for visual analysis of a neural network is also known as *feature maximization* or *activation maximization*.\n",
    "\n",
    "This builds on the previous tutorials. You should be familiar with neural networks in general (e.g. Tutorial #01 and #02), and knowledge of the Inception model is also helpful (Tutorial #07)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Inception model from Tutorial #07. We want to find an input image that maximizes a given feature inside the neural network. The input image is initialized with a little noise and is then updated using the gradient of the given feature. After performing a number of these optimization iterations we get an image that this particular feature 'likes to see'.\n",
    "\n",
    "Because the Inception model is constructed from many basic mathematical operations that have been combined, TensorFlow allows us to easily find the gradient using the chain-rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image('images/13_visual_analysis_flowchart.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Functions and classes for loading and using the Inception model.\n",
    "import inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was developed using Python 3.5.2 (Anaconda) and TensorFlow version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception model from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inception model is downloaded from the internet. This is the default directory where you want to save the data-files. The directory will be created if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inception.data_dir = 'inception/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data for the Inception model if it doesn't already exist in the directory. It is 85 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inception.maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names of convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list of names for the convolutional layers in the Inception model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_layer_names():\n",
    "    # Load the Inception model.\n",
    "    model = inception.Inception()\n",
    "    \n",
    "    # Create a list of names for the operations in the graph\n",
    "    # for the Inception model where the operator-type is 'Conv2D'.\n",
    "    names = [op.name for op in model.graph.get_operations() if op.type=='Conv2D']\n",
    "\n",
    "    # Close the TensorFlow session inside the model-object.\n",
    "    model.close()\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_names = get_conv_layer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 94 convolutional layers in this Inception model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(conv_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the names of the first 5 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the names of the last 5 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_names[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for finding the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds the input image that maximizes a given feature in the network. It essentially just performs optimization with gradient ascent. The image is initialized with small random values and is then iteratively updated using the gradient for the given feature with regard to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_image(conv_id=None, feature=0,\n",
    "                   num_iterations=30, show_progress=True):\n",
    "    \"\"\"\n",
    "    Find an image that maximizes the feature\n",
    "    given by the conv_id and feature number.\n",
    "\n",
    "    Parameters:\n",
    "    conv_id: Integer identifying the convolutional layer to\n",
    "             maximize. It is an index into conv_names.\n",
    "             If None then use the last fully-connected layer\n",
    "             before the softmax output.\n",
    "    feature: Index into the layer for the feature to maximize.\n",
    "    num_iteration: Number of optimization iterations to perform.\n",
    "    show_progress: Boolean whether to show the progress.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Inception model. This is done for each call of\n",
    "    # this function because we will add a lot to the graph\n",
    "    # which will cause the graph to grow and eventually the\n",
    "    # computer will run out of memory.\n",
    "    model = inception.Inception()\n",
    "\n",
    "    # Reference to the tensor that takes the raw input image.\n",
    "    resized_image = model.resized_image\n",
    "\n",
    "    # Reference to the tensor for the predicted classes.\n",
    "    # This is the output of the final layer's softmax classifier.\n",
    "    y_pred = model.y_pred\n",
    "\n",
    "    # Create the loss-function that must be maximized.\n",
    "    if conv_id is None:\n",
    "        # If we want to maximize a feature on the last layer,\n",
    "        # then we use the fully-connected layer prior to the\n",
    "        # softmax-classifier. The feature no. is the class-number\n",
    "        # and must be an integer between 1 and 1000.\n",
    "        # The loss-function is just the value of that feature.\n",
    "        loss = model.y_logits[0, feature]\n",
    "    else:\n",
    "        # If instead we want to maximize a feature of a\n",
    "        # convolutional layer inside the neural network.\n",
    "\n",
    "        # Get the name of the convolutional operator.\n",
    "        conv_name = conv_names[conv_id]\n",
    "        \n",
    "        # Get a reference to the tensor that is output by the\n",
    "        # operator. Note that \":0\" is added to the name for this.\n",
    "        tensor = model.graph.get_tensor_by_name(conv_name + \":0\")\n",
    "\n",
    "        # Set the Inception model's graph as the default\n",
    "        # so we can add an operator to it.\n",
    "        with model.graph.as_default():\n",
    "            # The loss-function is the average of all the\n",
    "            # tensor-values for the given feature. This\n",
    "            # ensures that we generate the whole input image.\n",
    "            # You can try and modify this so it only uses\n",
    "            # a part of the tensor.\n",
    "            loss = tf.reduce_mean(tensor[:,:,:,feature])\n",
    "    \n",
    "    # Get the gradient for the loss-function with regard to\n",
    "    # the resized input image. This creates a mathematical\n",
    "    # function for calculating the gradient.\n",
    "    gradient = tf.gradients(loss, resized_image)\n",
    "\n",
    "    # Create a TensorFlow session so we can run the graph.\n",
    "    session = tf.Session(graph=model.graph)\n",
    "\n",
    "    # Generate a random image of the same size as the raw input.\n",
    "    # Each pixel is a small random value between 128 and 129,\n",
    "    # which is about the middle of the colour-range.\n",
    "    image_shape = resized_image.get_shape()\n",
    "    image = np.random.uniform(size=image_shape) + 128.0\n",
    "\n",
    "    # Perform a number of optimization iterations to find\n",
    "    # the image that maximizes the loss-function.\n",
    "    for i in range(num_iterations):\n",
    "        # Create a feed-dict. This feeds the image to the\n",
    "        # tensor in the graph that holds the resized image, because\n",
    "        # this is the final stage for inputting raw image data.\n",
    "        feed_dict = {model.tensor_name_resized_image: image}\n",
    "\n",
    "        # Calculate the predicted class-scores,\n",
    "        # as well as the gradient and the loss-value.\n",
    "        pred, grad, loss_value = session.run([y_pred, gradient, loss],\n",
    "                                             feed_dict=feed_dict)\n",
    "        \n",
    "        # Squeeze the dimensionality for the gradient-array.\n",
    "        grad = np.array(grad).squeeze()\n",
    "\n",
    "        # The gradient now tells us how much we need to change the\n",
    "        # input image in order to maximize the given feature.\n",
    "\n",
    "        # Calculate the step-size for updating the image.\n",
    "        # This step-size was found to give fast convergence.\n",
    "        # The addition of 1e-8 is to protect from div-by-zero.\n",
    "        step_size = 1.0 / (grad.std() + 1e-8)\n",
    "\n",
    "        # Update the image by adding the scaled gradient\n",
    "        # This is called gradient ascent.\n",
    "        image += step_size * grad\n",
    "\n",
    "        # Ensure all pixel-values in the image are between 0 and 255.\n",
    "        image = np.clip(image, 0.0, 255.0)\n",
    "\n",
    "        if show_progress:\n",
    "            print(\"Iteration:\", i)\n",
    "\n",
    "            # Convert the predicted class-scores to a one-dim array.\n",
    "            pred = np.squeeze(pred)\n",
    "\n",
    "            # The predicted class for the Inception model.\n",
    "            pred_cls = np.argmax(pred)\n",
    "\n",
    "            # Name of the predicted class.\n",
    "            cls_name = model.name_lookup.cls_to_name(pred_cls,\n",
    "                                               only_first_name=True)\n",
    "\n",
    "            # The score (probability) for the predicted class.\n",
    "            cls_score = pred[pred_cls]\n",
    "\n",
    "            # Print the predicted score etc.\n",
    "            msg = \"Predicted class-name: {0} (#{1}), score: {2:>7.2%}\"\n",
    "            print(msg.format(cls_name, pred_cls, cls_score))\n",
    "\n",
    "            # Print statistics for the gradient.\n",
    "            msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
    "            print(msg.format(grad.min(), grad.max(), step_size))\n",
    "\n",
    "            # Print the loss-value.\n",
    "            print(\"Loss:\", loss_value)\n",
    "\n",
    "            # Newline.\n",
    "            print()\n",
    "\n",
    "    # Close the TensorFlow session inside the model-object.\n",
    "    model.close()\n",
    "\n",
    "    return image.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting image and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function normalizes an image so its pixel-values are between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(x):\n",
    "    # Get the min and max values for all pixels in the input.\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "\n",
    "    # Normalize so all values are between 0.0 and 1.0\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    # Normalize the image so pixels are between 0.0 and 1.0\n",
    "    img_norm = normalize_image(image)\n",
    "    \n",
    "    # Plot the image.\n",
    "    plt.imshow(img_norm, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots 6 images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, show_size=100):\n",
    "    \"\"\"\n",
    "    The show_size is the number of pixels to show for each image.\n",
    "    The max value is 299.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(2, 3)\n",
    "\n",
    "    # Adjust vertical spacing.\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    # Use interpolation to smooth pixels?\n",
    "    smooth = True\n",
    "    \n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'spline16'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    # For each entry in the grid.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Get the i'th image and only use the desired pixels.\n",
    "        img = images[i, 0:show_size, 0:show_size, :]\n",
    "        \n",
    "        # Normalize the image so its pixels are between 0.0 and 1.0\n",
    "        img_norm = normalize_image(img)\n",
    "        \n",
    "        # Plot the image.\n",
    "        ax.imshow(img_norm, interpolation=interpolation)\n",
    "\n",
    "        # Remove ticks.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for optimizing and plotting images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function optimizes multiple images and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_images(conv_id=None, num_iterations=30, show_size=100):\n",
    "    \"\"\"\n",
    "    Find 6 images that maximize the 6 first features in the layer\n",
    "    given by the conv_id.\n",
    "    \n",
    "    Parameters:\n",
    "    conv_id: Integer identifying the convolutional layer to\n",
    "             maximize. It is an index into conv_names.\n",
    "             If None then use the last layer before the softmax output.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    show_size: Number of pixels to show for each image. Max 299.\n",
    "    \"\"\"\n",
    "\n",
    "    # Which layer are we using?\n",
    "    if conv_id is None:\n",
    "        print(\"Final fully-connected layer before softmax.\")\n",
    "    else:\n",
    "        print(\"Layer:\", conv_names[conv_id])\n",
    "\n",
    "    # Initialize the array of images.\n",
    "    images = []\n",
    "\n",
    "    # For each feature do the following. Note that the\n",
    "    # last fully-connected layer only supports numbers\n",
    "    # between 1 and 1000, while the convolutional layers\n",
    "    # support numbers between 0 and some other number.\n",
    "    # So we just use the numbers between 1 and 7.\n",
    "    for feature in range(1,7):\n",
    "        print(\"Optimizing image for feature no.\", feature)\n",
    "        \n",
    "        # Find the image that maximizes the given feature\n",
    "        # for the network layer identified by conv_id (or None).\n",
    "        image = optimize_image(conv_id=conv_id, feature=feature,\n",
    "                               show_progress=False,\n",
    "                               num_iterations=num_iterations)\n",
    "\n",
    "        # Squeeze the dim of the array.\n",
    "        image = image.squeeze()\n",
    "\n",
    "        # Append to the list of images.\n",
    "        images.append(image)\n",
    "\n",
    "    # Convert to numpy-array so we can index all dimensions easily.\n",
    "    images = np.array(images)\n",
    "\n",
    "    # Plot the images.\n",
    "    plot_images(images=images, show_size=show_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize a single image for an early convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, find an input image that maximizes feature no. 2 of the convolutional layer with the name `conv_names[conv_id]` where `conv_id=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = optimize_image(conv_id=5, feature=2,\n",
    "                       num_iterations=30, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize multiple images for convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we optimize and plot multiple images for convolutional layers inside the Inception model. These images show what the layers 'like to see'. Notice how the patterns become increasingly complex for deeper layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=0, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=3, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=4, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=5, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=6, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=7, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=8, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=9, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=10, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=20, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=30, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=40, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=50, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=60, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=70, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=80, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=90, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=93, num_iterations=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final fully-connected layer before Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we optimize and plot images for the final layer in the Inception model. This is the fully-connected layer right before the softmax-classifier. The features in this layer correspond to output classes.\n",
    "\n",
    "We might have hoped to see recognizable patterns in these images, e.g. monkeys and birds corresponding to the output classes, but the images just show complex, abstract patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_images(conv_id=None, num_iterations=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images only show 100x100 pixels but the images are actually 299x299 pixels. It is possible that there might be recognizable patterns if we optimize for more iterations and plot the full image. So let us optimize the first image again and plot it in full resolution.\n",
    "\n",
    "The Inception model classifies the resulting image as a 'kit fox' with about 100% certainty, but to the human eye the image just shows abstract patterns.\n",
    "\n",
    "If you want to try this for another feature number, note that it must be between 1 and 1000 because it must correspond to a valid class-number for the final output-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = optimize_image(conv_id=None, feature=1,\n",
    "                       num_iterations=100, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close TensorFlow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow session was already closed in the functions above that used the Inception model. This was done to save memory so the computer would not crash when adding many gradient-functions to the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showed how to optimize input images that maximize features inside a neural network. This allows us to visually analyze what the neural network 'likes to see', because the given feature (or neuron) inside the neural network reacts most strongly to that particular image.\n",
    "\n",
    "For the lower layers in the neural network, the images had simple patterns, e.g. different types of wavy lines. The image patterns become increasingly complex for deeper layers of the neural network. We might have expected or hoped that the image patterns would be recognizable for deeper layers, e.g. showing monkeys, foxes, cars, etc. But instead the image patterns become increasingly complex and abstract for the deeper layers.\n",
    "\n",
    "Why is that? Recall from Tutorial #11 that the Inception model can easily be fooled with a little adversarial noise, so it classifies any input image as another target-class. So it is not surprising that the Inception model recognizes these abstract image patterns, which are unclear to the human eye. There is probably an infinite number of images that maximize the features deep inside a neural network, and the images that are also recognizable by humans are only a small fraction of all these image patterns. This may be the reason why the optimization process only found abstract image patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods\n",
    "\n",
    "There are many proposals in the research literature for guiding the optimization process so as to find image patterns that are more recognizable to humans.\n",
    "\n",
    "[This paper](https://arxiv.org/abs/1506.06579) proposes a combination of heuristics for guiding the optimization process of the image patterns. The paper shows example images for several classes such as flamingo, pelican and black swan, all of which are somewhat recognizable to the human eye. The method is apparently implemented [here](https://github.com/yosinski/deep-visualization-toolbox/blob/master/optimize/gradient_optimizer.py#L313-L346) (the exact line-numbers could change in the future). It requires a combination of heuristics and their parameters must be finely tuned in order to generate these images. But the parameter choice is not entirely clear from the research paper. In spite of several attempts, I could not reproduce their results. Maybe I have misunderstood their paper, or maybe the heuristics were finely tuned to their network architecture, which is a variant of the so-called AlexNet, whereas this tutorial uses the more advanced Inception model.\n",
    "\n",
    "[This paper](https://arxiv.org/abs/1602.03616) proposes another method for producing images that are even more recognizable to the human eye. However, the method is actually cheating, because it goes through all the images in the training-set (e.g. ImageNet) and takes the images that maximally activate a given feature inside the neural network. Then it clusters and averages similar images. This produces the initial image for the optimization procedure. So it is no wonder that the method gives better results when it starts with an image that is constructed from real photos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n",
    "\n",
    "You may want to backup this Notebook and the other files before making any changes.\n",
    "\n",
    "* Try and run the optimization several times for features in lower layers of the network. Are the resulting images always the same?\n",
    "* Try and use fewer and more optimization iterations. How does it affect the image quality?\n",
    "* Try and change the loss-function for a convolutional feature. This can be done in different ways. How does it affect the image patterns? Why is that?\n",
    "* Do you think the optimizer also increases other features than the one we want maximized? How can you measure this? Can you ensure that the optimizer only maximizes one feature at a time?\n",
    "* Try maximizing multiple features simultaneously.\n",
    "* Try visualizing the features and layers in a smaller neural network trained on the MNIST data-set. Is it easier to see patterns in the images?\n",
    "* Try and implement the methods from the papers above.\n",
    "* Try your own ideas for improving the optimized images.\n",
    "* Explain to a friend how the program works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "Copyright (c) 2016 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
