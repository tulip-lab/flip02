{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# \u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\n\n---\n\n\u672c\u8282\u4ecb\u7ecd[\u7f16\u7801\u5668\u2014\u89e3\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236](seq2seq-attention.md)\u7684\u5e94\u7528\u3002\u6211\u4eec\u4ee5\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08neural machine translation\uff09\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Gluon\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7801\u5668\u2014\u89e3\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\u6a21\u578b\u3002\n\n\n## \u4f7f\u7528Gluon\u5b9e\u73b0\u7f16\u7801\u5668\u2014\u89e3\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\n\n\u6211\u4eec\u5148\u8f7d\u5165\u9700\u8981\u7684\u5305\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import mxnet as mx\nfrom mxnet import autograd, gluon, nd\nfrom mxnet.gluon import nn, rnn, Block\nfrom mxnet.contrib import text\n\nfrom io import open\nimport collections\nimport datetime"
        }, 
        {
            "source": "\u4e0b\u9762\u5b9a\u4e49\u4e00\u4e9b\u7279\u6b8a\u5b57\u7b26\u3002\u5176\u4e2dPAD (padding)\u7b26\u53f7\u4f7f\u6bcf\u4e2a\u5e8f\u5217\u7b49\u957f\uff1bBOS (beginning of sequence)\u7b26\u53f7\u8868\u793a\u5e8f\u5217\u7684\u5f00\u59cb\uff1b\u800cEOS (end of sequence)\u7b26\u53f7\u8868\u793a\u5e8f\u5217\u7684\u7ed3\u675f\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "PAD = '<pad>'\nBOS = '<bos>'\nEOS = '<eos>'"
        }, 
        {
            "source": "\u4ee5\u4e0b\u662f\u4e00\u4e9b\u53ef\u4ee5\u8c03\u8282\u7684\u6a21\u578b\u53c2\u6570\u3002\u6211\u4eec\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\u5206\u522b\u4f7f\u7528\u4e86\u4e00\u5c42\u548c\u4e24\u5c42\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "epochs = 50\nepoch_period = 10\n\nlearning_rate = 0.005\n# \u8f93\u5165\u6216\u8f93\u51fa\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\uff08\u542b\u53e5\u672b\u6dfb\u52a0\u7684EOS\u5b57\u7b26\uff09\u3002\nmax_seq_len = 5\n\nencoder_num_layers = 1\ndecoder_num_layers = 2\n\nencoder_drop_prob = 0.1\ndecoder_drop_prob = 0.1\n\nencoder_hidden_dim = 256\ndecoder_hidden_dim = 256\nalignment_dim = 25\n\nctx = mx.cpu(0)"
        }, 
        {
            "source": "### \u8bfb\u53d6\u6570\u636e\n\n\u6211\u4eec\u5b9a\u4e49\u51fd\u6570\u8bfb\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5f88\u5c0f\u7684\u6cd5\u8bed\u2014\u2014\u82f1\u8bed\u6570\u636e\u96c6\u3002\n\n\u8fd9\u91cc\u4f7f\u7528\u4e86[\u4e4b\u524d\u7ae0\u8282](pretrained-embedding.md)\u4ecb\u7ecd\u7684`mxnet.contrib.text`\u6765\u521b\u5efa\u6cd5\u8bed\u548c\u82f1\u8bed\u7684\u8bcd\u5178\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u4f1a\u5728\u53e5\u672b\u9644\u4e0aEOS\u7b26\u53f7\uff0c\u5e76\u53ef\u80fd\u901a\u8fc7\u6dfb\u52a0PAD\u7b26\u53f7\u4f7f\u6bcf\u4e2a\u5e8f\u5217\u7b49\u957f\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!wget https://github.com/mli/gluon-tutorials-zh/raw/master/data/fr-en-small.txt\n!mv fr-en-small.txt data/"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def read_data(max_seq_len):\n    input_tokens = []\n    output_tokens = []\n    input_seqs = []\n    output_seqs = []\n\n    with open('data/fr-en-small.txt') as f:\n        lines = f.readlines()\n        for line in lines:\n            input_seq, output_seq = line.rstrip().split('\\t')\n            cur_input_tokens = input_seq.split(' ')\n            cur_output_tokens = output_seq.split(' ')\n\n            if len(cur_input_tokens) < max_seq_len and \\\n                            len(cur_output_tokens) < max_seq_len:\n                input_tokens.extend(cur_input_tokens)\n                # \u53e5\u672b\u9644\u4e0aEOS\u7b26\u53f7\u3002\n                cur_input_tokens.append(EOS)\n                # \u6dfb\u52a0PAD\u7b26\u53f7\u4f7f\u6bcf\u4e2a\u5e8f\u5217\u7b49\u957f\uff08\u957f\u5ea6\u4e3amax_seq_len\uff09\u3002\n                while len(cur_input_tokens) < max_seq_len:\n                    cur_input_tokens.append(PAD)\n                input_seqs.append(cur_input_tokens)\n                output_tokens.extend(cur_output_tokens)\n                cur_output_tokens.append(EOS)\n                while len(cur_output_tokens) < max_seq_len:\n                    cur_output_tokens.append(PAD)\n                output_seqs.append(cur_output_tokens)\n\n        fr_vocab = text.vocab.Vocabulary(collections.Counter(input_tokens),\n                                         reserved_tokens=[PAD, BOS, EOS])\n        en_vocab = text.vocab.Vocabulary(collections.Counter(output_tokens),\n                                         reserved_tokens=[PAD, BOS, EOS])\n    return fr_vocab, en_vocab, input_seqs, output_seqs"
        }, 
        {
            "source": "\u4ee5\u4e0b\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u6bcf\u4e00\u4e2a\u6837\u672c\u5305\u542b\u6cd5\u8bed\u7684\u8f93\u5165\u5e8f\u5217\u548c\u82f1\u8bed\u7684\u8f93\u51fa\u5e8f\u5217\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "input_vocab, output_vocab, input_seqs, output_seqs = read_data(max_seq_len)\nX = nd.zeros((len(input_seqs), max_seq_len), ctx=ctx)\nY = nd.zeros((len(output_seqs), max_seq_len), ctx=ctx)\nfor i in range(len(input_seqs)):\n    X[i] = nd.array(input_vocab.to_indices(input_seqs[i]), ctx=ctx)\n    Y[i] = nd.array(output_vocab.to_indices(output_seqs[i]), ctx=ctx)\n\ndataset = gluon.data.ArrayDataset(X, Y)\n"
        }, 
        {
            "source": "### \u7f16\u7801\u5668\u3001\u542b\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u7801\u5668\u548c\u89e3\u7801\u5668\u521d\u59cb\u72b6\u6001\n\n\u4ee5\u4e0b\u5b9a\u4e49\u4e86\u57fa\u4e8e[GRU](../chapter_recurrent-neural-networks/gru-scratch.md)\u7684\u7f16\u7801\u5668\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class Encoder(Block):\n    \"\"\"\u7f16\u7801\u5668\"\"\"\n    def __init__(self, input_dim, hidden_dim, num_layers, drop_prob):\n        super(Encoder, self).__init__()\n        with self.name_scope():\n            self.embedding = nn.Embedding(input_dim, hidden_dim)\n            self.dropout = nn.Dropout(drop_prob)\n            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n                               input_size=hidden_dim)\n\n    def forward(self, inputs, state):\n        # inputs\u5c3a\u5bf8: (1, num_steps)\uff0cemb\u5c3a\u5bf8: (num_steps, 1, 256)\n        emb = self.embedding(inputs).swapaxes(0, 1)\n        emb = self.dropout(emb)\n        output, state = self.rnn(emb, state)\n        return output, state\n\n    def begin_state(self, *args, **kwargs):\n        return self.rnn.begin_state(*args, **kwargs)"
        }, 
        {
            "source": "\u4ee5\u4e0b\u5b9a\u4e49\u4e86\u57fa\u4e8e[GRU](../chapter_recurrent-neural-networks/gru-scratch.md)\u7684\u89e3\u7801\u5668\u3002\u5b83\u5305\u542b[\u4e0a\u4e00\u8282\u91cc\u4ecb\u7ecd\u7684\u6ce8\u610f\u529b\u673a\u5236](seq2seq-attention.md)\u7684\u5b9e\u73b0\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class Decoder(Block):\n    \"\"\"\u542b\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u7801\u5668\"\"\"\n    def __init__(self, hidden_dim, output_dim, num_layers, max_seq_len,\n                 drop_prob, alignment_dim, encoder_hidden_dim):\n        super(Decoder, self).__init__()\n        self.max_seq_len = max_seq_len\n        self.encoder_hidden_dim = encoder_hidden_dim\n        self.hidden_size = hidden_dim\n        self.num_layers = num_layers\n        with self.name_scope():\n            self.embedding = nn.Embedding(output_dim, hidden_dim)\n            self.dropout = nn.Dropout(drop_prob)\n            # \u6ce8\u610f\u529b\u673a\u5236\u3002\n            self.attention = nn.Sequential()\n            with self.attention.name_scope():\n                self.attention.add(nn.Dense(\n                    alignment_dim, in_units=hidden_dim + encoder_hidden_dim,\n                    activation=\"tanh\", flatten=False))\n                self.attention.add(nn.Dense(1, in_units=alignment_dim,\n                                            flatten=False))\n\n            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n                               input_size=hidden_dim)\n            self.out = nn.Dense(output_dim, in_units=hidden_dim)\n            self.rnn_concat_input = nn.Dense(\n                hidden_dim, in_units=hidden_dim + encoder_hidden_dim,\n                flatten=False)\n\n    def forward(self, cur_input, state, encoder_outputs):\n        # \u5f53RNN\u4e3a\u591a\u5c42\u65f6\uff0c\u53d6\u6700\u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u5355\u5c42\u9690\u542b\u72b6\u6001\u3002\n        single_layer_state = [state[0][-1].expand_dims(0)]\n        encoder_outputs = encoder_outputs.reshape((self.max_seq_len, 1,\n                                                   self.encoder_hidden_dim))\n        # single_layer_state\u5c3a\u5bf8: [(1, 1, decoder_hidden_dim)]\n        # hidden_broadcast\u5c3a\u5bf8: (max_seq_len, 1, decoder_hidden_dim)\n        hidden_broadcast = nd.broadcast_axis(single_layer_state[0], axis=0,\n                                             size=self.max_seq_len)\n\n        # encoder_outputs_and_hiddens\u5c3a\u5bf8:\n        # (max_seq_len, 1, encoder_hidden_dim + decoder_hidden_dim)\n        encoder_outputs_and_hiddens = nd.concat(encoder_outputs,\n                                                hidden_broadcast, dim=2)\n\n        # energy\u5c3a\u5bf8: (max_seq_len, 1, 1)\n        energy = self.attention(encoder_outputs_and_hiddens)\n\n        batch_attention = nd.softmax(energy, axis=0).reshape(\n            (1, 1, self.max_seq_len))\n\n        # batch_encoder_outputs\u5c3a\u5bf8: (1, max_seq_len, encoder_hidden_dim)\n        batch_encoder_outputs = encoder_outputs.swapaxes(0, 1)\n\n        # decoder_context\u5c3a\u5bf8: (1, 1, encoder_hidden_dim)\n        decoder_context = nd.batch_dot(batch_attention, batch_encoder_outputs)\n\n        # input_and_context\u5c3a\u5bf8: (1, 1, encoder_hidden_dim + decoder_hidden_dim)\n        input_and_context = nd.concat(self.embedding(cur_input).reshape(\n            (1, 1, self.hidden_size)), decoder_context, dim=2)\n        # concat_input\u5c3a\u5bf8: (1, 1, decoder_hidden_dim)\n        concat_input = self.rnn_concat_input(input_and_context)\n        concat_input = self.dropout(concat_input)\n\n        # \u5f53RNN\u4e3a\u591a\u5c42\u65f6\uff0c\u7528\u5355\u5c42\u9690\u542b\u72b6\u6001\u521d\u59cb\u5316\u5404\u4e2a\u5c42\u7684\u9690\u542b\u72b6\u6001\u3002\n        state = [nd.broadcast_axis(single_layer_state[0], axis=0,\n                                   size=self.num_layers)]\n\n        output, state = self.rnn(concat_input, state)\n        output = self.dropout(output)\n        output = self.out(output)\n        # output\u5c3a\u5bf8: (1, output_size)\uff0chidden\u5c3a\u5bf8: [(1, 1, decoder_hidden_dim)]\n        return output, state\n\n    def begin_state(self, *args, **kwargs):\n        return self.rnn.begin_state(*args, **kwargs)"
        }, 
        {
            "source": "\u4e3a\u4e86\u521d\u59cb\u5316\u89e3\u7801\u5668\u7684\u9690\u542b\u72b6\u6001\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u5c42\u5168\u8fde\u63a5\u7f51\u7edc\u6765\u8f6c\u5316\u7f16\u7801\u5668\u7684\u8f93\u51fa\u9690\u542b\u72b6\u6001\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class DecoderInitState(Block):\n    \"\"\"\u89e3\u7801\u5668\u9690\u542b\u72b6\u6001\u7684\u521d\u59cb\u5316\"\"\"\n    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n        super(DecoderInitState, self).__init__()\n        with self.name_scope():\n            self.dense = nn.Dense(decoder_hidden_dim,\n                                  in_units=encoder_hidden_dim,\n                                  activation=\"tanh\", flatten=False)\n\n    def forward(self, encoder_state):\n        return [self.dense(encoder_state)]"
        }, 
        {
            "source": "### \u8bad\u7ec3\u548c\u5e94\u7528\u6a21\u578b\n\n\u6211\u4eec\u5b9a\u4e49`translate`\u51fd\u6570\u6765\u5e94\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u8fc7\u8be5\u51fd\u6570\u7684\u524d\u4e09\u4e2a\u53c2\u6570\u4f20\u9012\u3002\u89e3\u7801\u5668\u7684\u6700\u521d\u65f6\u523b\u8f93\u5165\u6765\u81eaBOS\u5b57\u7b26\u3002\u5f53\u4efb\u4e00\u65f6\u523b\u7684\u8f93\u51fa\u4e3aEOS\u5b57\u7b26\u65f6\uff0c\u8f93\u51fa\u5e8f\u5217\u5373\u5b8c\u6210\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def translate(encoder, decoder, decoder_init_state, fr_ens, ctx, max_seq_len):\n    for fr_en in fr_ens:\n        print('Input :', fr_en[0])\n        input_tokens = fr_en[0].split(' ') + [EOS]\n        # \u6dfb\u52a0PAD\u7b26\u53f7\u4f7f\u6bcf\u4e2a\u5e8f\u5217\u7b49\u957f\uff08\u957f\u5ea6\u4e3amax_seq_len\uff09\u3002\n        while len(input_tokens) < max_seq_len:\n            input_tokens.append(PAD)\n        inputs = nd.array(input_vocab.to_indices(input_tokens), ctx=ctx)\n        encoder_state = encoder.begin_state(func=mx.nd.zeros, batch_size=1,\n                                            ctx=ctx)\n        encoder_outputs, encoder_state = encoder(inputs.expand_dims(0),\n                                                 encoder_state)\n        encoder_outputs = encoder_outputs.flatten()\n        # \u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u4e3aBOS\u5b57\u7b26\u3002\n        decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n        decoder_state = decoder_init_state(encoder_state[0])\n        output_tokens = []\n\n        for i in range(max_seq_len):\n            decoder_output, decoder_state = decoder(\n                decoder_input, decoder_state, encoder_outputs)\n            pred_i = int(decoder_output.argmax(axis=1).asnumpy())\n            # \u5f53\u4efb\u4e00\u65f6\u523b\u7684\u8f93\u51fa\u4e3aEOS\u5b57\u7b26\u65f6\uff0c\u8f93\u51fa\u5e8f\u5217\u5373\u5b8c\u6210\u3002\n            if pred_i == output_vocab.token_to_idx[EOS]:\n                break\n            else:\n                output_tokens.append(output_vocab.idx_to_token[pred_i])\n            decoder_input = nd.array([pred_i], ctx=ctx)\n\n        print('Output:', ' '.join(output_tokens))\n        print('Expect:', fr_en[1], '\\n')"
        }, 
        {
            "source": "\u4e0b\u9762\u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\u3002\u4e3a\u4e86\u521d\u59cb\u5316\u89e3\u7801\u5668\u7684\u9690\u542b\u72b6\u6001\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u5c42\u5168\u8fde\u63a5\u7f51\u7edc\u6765\u8f6c\u5316\u7f16\u7801\u5668\u6700\u65e9\u65f6\u523b\u7684\u8f93\u51fa\u9690\u542b\u72b6\u6001\u3002\u8fd9\u91cc\u7684\u89e3\u7801\u5668\u4f7f\u7528\u5f53\u524d\u65f6\u523b\u7684\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523b\u7684\u8f93\u5165\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def train(encoder, decoder, decoder_init_state, max_seq_len, ctx, eval_fr_ens):\n    # \u5bf9\u4e8e\u4e09\u4e2a\u7f51\u7edc\uff0c\u5206\u522b\u521d\u59cb\u5316\u5b83\u4eec\u7684\u6a21\u578b\u53c2\u6570\u5e76\u5b9a\u4e49\u5b83\u4eec\u7684\u4f18\u5316\u5668\u3002\n    encoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n    decoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n    decoder_init_state.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n    encoder_optimizer = gluon.Trainer(encoder.collect_params(), 'adam',\n                                      {'learning_rate': learning_rate})\n    decoder_optimizer = gluon.Trainer(decoder.collect_params(), 'adam',\n                                      {'learning_rate': learning_rate})\n    decoder_init_state_optimizer = gluon.Trainer(\n        decoder_init_state.collect_params(), 'adam',\n        {'learning_rate': learning_rate})\n\n    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    prev_time = datetime.datetime.now()\n    data_iter = gluon.data.DataLoader(dataset, 1, shuffle=True)\n\n    total_loss = 0.0\n    for epoch in range(1, epochs + 1):\n        for x, y in data_iter:\n            with autograd.record():\n                loss = nd.array([0], ctx=ctx)\n                encoder_state = encoder.begin_state(\n                    func=mx.nd.zeros, batch_size=1, ctx=ctx)\n                encoder_outputs, encoder_state = encoder(x, encoder_state)\n\n                # encoder_outputs\u5c3a\u5bf8: (max_seq_len, encoder_hidden_dim)\n                encoder_outputs = encoder_outputs.flatten()\n                # \u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u4e3aBOS\u5b57\u7b26\u3002\n                decoder_input = nd.array([output_vocab.token_to_idx[BOS]],\n                                         ctx=ctx)\n                decoder_state = decoder_init_state(encoder_state[0])\n                for i in range(max_seq_len):\n                    decoder_output, decoder_state = decoder(\n                        decoder_input, decoder_state, encoder_outputs)\n                    # \u89e3\u7801\u5668\u4f7f\u7528\u5f53\u524d\u65f6\u523b\u7684\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523b\u7684\u8f93\u5165\u3002\n                    decoder_input = nd.array(\n                        [decoder_output.argmax(axis=1).asscalar()], ctx=ctx)\n                    loss = loss + softmax_cross_entropy(decoder_output, y[0][i])\n                    if y[0][i].asscalar() == output_vocab.token_to_idx[EOS]:\n                        break\n\n            loss.backward()\n            encoder_optimizer.step(1)\n            decoder_optimizer.step(1)\n            decoder_init_state_optimizer.step(1)\n            total_loss += loss.asscalar() / max_seq_len\n\n        if epoch % epoch_period == 0 or epoch == 1:\n            cur_time = datetime.datetime.now()\n            h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n            m, s = divmod(remainder, 60)\n            time_str = 'Time %02d:%02d:%02d' % (h, m, s)\n            if epoch == 1:\n                print_loss_avg = total_loss / len(data_iter)\n            else:\n                print_loss_avg = total_loss / epoch_period / len(data_iter)\n            loss_str = 'Epoch %d, Loss %f, ' % (epoch, print_loss_avg)\n            print(loss_str + time_str)\n            if epoch != 1:\n                total_loss = 0.0\n            prev_time = cur_time\n\n            translate(encoder, decoder, decoder_init_state, eval_fr_ens, ctx,\n                      max_seq_len)"
        }, 
        {
            "source": "\u4ee5\u4e0b\u5206\u522b\u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u548c\u89e3\u7801\u5668\u521d\u59cb\u9690\u542b\u72b6\u6001\u7f51\u7edc\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "encoder = Encoder(len(input_vocab), encoder_hidden_dim, encoder_num_layers,\n                  encoder_drop_prob)\ndecoder = Decoder(decoder_hidden_dim, len(output_vocab),\n                  decoder_num_layers, max_seq_len, decoder_drop_prob,\n                  alignment_dim, encoder_hidden_dim)\ndecoder_init_state = DecoderInitState(encoder_hidden_dim, decoder_hidden_dim)"
        }, 
        {
            "source": "\u7ed9\u5b9a\u7b80\u5355\u7684\u6cd5\u8bed\u548c\u82f1\u8bed\u5e8f\u5217\uff0c\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u6a21\u578b\u7684\u8bad\u7ec3\u7ed3\u679c\u3002\u6253\u5370\u7684\u7ed3\u679c\u4e2d\uff0cInput\u3001Output\u548cExpect\u5206\u522b\u4ee3\u8868\u8f93\u5165\u5e8f\u5217\u3001\u8f93\u51fa\u5e8f\u5217\u548c\u6b63\u786e\u5e8f\u5217\u3002", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "eval_fr_ens =[['elle est japonaise .', 'she is japanese .'],\n              ['ils regardent .', 'they are watching .']]\ntrain(encoder, decoder, decoder_init_state, max_seq_len, ctx, eval_fr_ens)"
        }, 
        {
            "source": "## \u675f\u641c\u7d22\n\n\u5728\u4e0a\u4e00\u8282\u91cc\uff0c\u6211\u4eec\u63d0\u5230\u7f16\u7801\u5668\u6700\u7ec8\u8f93\u51fa\u4e86\u4e00\u4e2a\u80cc\u666f\u5411\u91cf$\\mathbf{c}$\uff0c\u8be5\u80cc\u666f\u5411\u91cf\u7f16\u7801\u4e86\u8f93\u5165\u5e8f\u5217$x_1, x_2, \\ldots, x_T$\u7684\u4fe1\u606f\u3002\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8f93\u51fa\u5e8f\u5217\u662f$y_1, y_2, \\ldots, y_{T^\\prime}$\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u751f\u6210\u6982\u7387\u662f\n\n$$\\mathbb{P}(y_1, \\ldots, y_{T^\\prime}) = \\prod_{t^\\prime=1}^{T^\\prime} \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n\n\n\u5bf9\u4e8e\u673a\u5668\u7ffb\u8bd1\u7684\u8f93\u51fa\u6765\u8bf4\uff0c\u5982\u679c\u8f93\u51fa\u8bed\u8a00\u7684\u8bcd\u6c47\u96c6\u5408$\\mathcal{Y}$\u7684\u5927\u5c0f\u4e3a$|\\mathcal{Y}|$\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a$T^\\prime$\uff0c\u90a3\u4e48\u53ef\u80fd\u7684\u8f93\u51fa\u5e8f\u5217\u79cd\u7c7b\u662f$\\mathcal{O}(|\\mathcal{Y}|^{T^\\prime})$\u3002\u4e3a\u4e86\u627e\u5230\u751f\u6210\u6982\u7387\u6700\u5927\u7684\u8f93\u51fa\u5e8f\u5217\uff0c\u4e00\u79cd\u65b9\u6cd5\u662f\u8ba1\u7b97\u6240\u6709$\\mathcal{O}(|\\mathcal{Y}|^{T^\\prime})$\u79cd\u53ef\u80fd\u5e8f\u5217\u7684\u751f\u6210\u6982\u7387\uff0c\u5e76\u8f93\u51fa\u6982\u7387\u6700\u5927\u7684\u5e8f\u5217\u3002\u6211\u4eec\u5c06\u8be5\u5e8f\u5217\u79f0\u4e3a\u6700\u4f18\u5e8f\u5217\u3002\u4f46\u662f\u8fd9\u79cd\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u8fc7\u9ad8\uff08\u4f8b\u5982\uff0c$10000^{10} = 1 \\times 10^{40}$\uff09\u3002\n\n\n\u6211\u4eec\u76ee\u524d\u6240\u4ecb\u7ecd\u7684\u89e3\u7801\u5668\u5728\u6bcf\u4e2a\u65f6\u523b\u53ea\u8f93\u51fa\u751f\u6210\u6982\u7387\u6700\u5927\u7684\u4e00\u4e2a\u8bcd\u6c47\u3002\u5bf9\u4e8e\u4efb\u4e00\u65f6\u523b$t^\\prime$\uff0c\u6211\u4eec\u4ece$|\\mathcal{Y}|$\u4e2a\u8bcd\u4e2d\u641c\u7d22\u51fa\u8f93\u51fa\u8bcd\n\n$$y_{t^\\prime} = \\text{argmax}_{y_{t^\\prime} \\in \\mathcal{Y}} \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n\n\u56e0\u6b64\uff0c\u641c\u7d22\u8ba1\u7b97\u5f00\u9500\uff08$\\mathcal{O}(|\\mathcal{Y}| \\times {T^\\prime})$\uff09\u663e\u8457\u4e0b\u964d\uff08\u4f8b\u5982\uff0c$10000 \\times 10 = 1 \\times 10^5$\uff09\uff0c\u4f46\u8fd9\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u4e00\u5b9a\u641c\u7d22\u5230\u6700\u4f18\u5e8f\u5217\u3002\n\n\u675f\u641c\u7d22\uff08beam search\uff09\u4ecb\u4e8e\u4e0a\u9762\u4e8c\u8005\u4e4b\u95f4\u3002\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u4f8b\u5b50\u3002\n\n\u5047\u8bbe\u8f93\u51fa\u5e8f\u5217\u7684\u8bcd\u5178\u4e2d\u53ea\u5305\u542b\u4e94\u4e2a\u8bcd\uff1a$\\mathcal{Y} = \\{A, B, C, D, E\\}$\u3002\u675f\u641c\u7d22\u7684\u4e00\u4e2a\u8d85\u53c2\u6570\u53eb\u505a\u675f\u5bbd\uff08beam width\uff09\u3002\u4ee5\u675f\u5bbd\u7b49\u4e8e2\u4e3a\u4f8b\uff0c\u5047\u8bbe\u8f93\u51fa\u5e8f\u5217\u957f\u5ea6\u4e3a3\uff0c\u5047\u5982\u65f6\u523b1\u751f\u6210\u6982\u7387$\\mathbb{P}(y_{t^\\prime} \\mid \\mathbf{c})$\u6700\u5927\u7684\u4e24\u4e2a\u8bcd\u4e3a$A$\u548c$C$\uff0c\u6211\u4eec\u5728\u65f6\u523b2\u5bf9\u4e8e\u6240\u6709\u7684$y_2 \\in \\mathcal{Y}$\u90fd\u5206\u522b\u8ba1\u7b97$\\mathbb{P}(y_2 \\mid A, \\mathbf{c})$\u548c$\\mathbb{P}(y_2 \\mid C, \\mathbf{c})$\uff0c\u4ece\u8ba1\u7b97\u51fa\u768410\u4e2a\u6982\u7387\u4e2d\u53d6\u6700\u5927\u7684\u4e24\u4e2a\uff0c\u5047\u8bbe\u4e3a$\\mathbb{P}(B \\mid A, \\mathbf{c})$\u548c$\\mathbb{P}(E \\mid C, \\mathbf{c})$\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u5728\u65f6\u523b3\u5bf9\u4e8e\u6240\u6709\u7684$y_3 \\in \\mathcal{Y}$\u90fd\u5206\u522b\u8ba1\u7b97$\\mathbb{P}(y_3 \\mid A, B, \\mathbf{c})$\u548c$\\mathbb{P}(y_3 \\mid C, E, \\mathbf{c})$\uff0c\u4ece\u8ba1\u7b97\u51fa\u768410\u4e2a\u6982\u7387\u4e2d\u53d6\u6700\u5927\u7684\u4e24\u4e2a\uff0c\u5047\u8bbe\u4e3a$\\mathbb{P}(D \\mid A, B, \\mathbf{c})$\u548c$\\mathbb{P}(D \\mid C, E, \\mathbf{c})$\u3002\n\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u8f93\u51fa\u5e8f\u5217\uff1a$A$\u3001$C$\u3001$AB$\u3001$CE$\u3001$ABD$\u3001$CED$\u4e2d\u7b5b\u9009\u51fa\u4ee5\u7279\u6b8a\u5b57\u7b26EOS\u7ed3\u5c3e\u7684\u5019\u9009\u5e8f\u5217\u3002\u518d\u5728\u5019\u9009\u5e8f\u5217\u4e2d\u53d6\u4ee5\u4e0b\u5206\u6570\u6700\u9ad8\u7684\u5e8f\u5217\u4f5c\u4e3a\u6700\u7ec8\u5019\u9009\u5e8f\u5217\uff1a\n\n$$ \\frac{1}{L^\\alpha} \\log \\mathbb{P}(y_1, \\ldots, y_{L}) = \\frac{1}{L^\\alpha} \\sum_{t^\\prime=1}^L \\log \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n\n\u5176\u4e2d$L$\u4e3a\u5019\u9009\u5e8f\u5217\u957f\u5ea6\uff0c$\\alpha$\u4e00\u822c\u53ef\u9009\u4e3a0.75\u3002\u5206\u6bcd\u4e0a\u7684$L^\\alpha$\u662f\u4e3a\u4e86\u60e9\u7f5a\u8f83\u957f\u5e8f\u5217\u7684\u5206\u6570\u4e2d\u7684\u5bf9\u6570\u76f8\u52a0\u9879\u3002\n\n## \u8bc4\u4ef7\u7ffb\u8bd1\u7ed3\u679c\n\n2002\u5e74\uff0cIBM\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4ef7\u7ffb\u8bd1\u7ed3\u679c\u7684\u6307\u6807\uff0c\u53eb\u505a[BLEU](https://www.aclweb.org/anthology/P02-1040.pdf) \uff08Bilingual Evaluation Understudy\uff09\u3002\n\n\u8bbe$k$\u4e3a\u6211\u4eec\u5e0c\u671b\u8bc4\u4ef7\u7684n-gram\u7684\u6700\u5927\u957f\u5ea6\uff0c\u4f8b\u5982$k=4$\u3002n-gram\u7684\u7cbe\u5ea6$p_n$\u4e3a\u6a21\u578b\u8f93\u51fa\u4e2d\u7684n-gram\u5339\u914d\u53c2\u8003\u8f93\u51fa\u7684\u6570\u91cf\u4e0e\u6a21\u578b\u8f93\u51fa\u4e2d\u7684n-gram\u7684\u6570\u91cf\u7684\u6bd4\u503c\u3002\u4f8b\u5982\uff0c\u53c2\u8003\u8f93\u51fa\uff08\u771f\u5b9e\u503c\uff09\u4e3aABCDEF\uff0c\u6a21\u578b\u8f93\u51fa\u4e3aABBCD\u3002\u90a3\u4e48$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$\u3002\u8bbe$len_{ref}$\u548c$len_{MT}$\u5206\u522b\u4e3a\u53c2\u8003\u8f93\u51fa\u548c\u6a21\u578b\u8f93\u51fa\u7684\u8bcd\u6570\u3002\u90a3\u4e48\uff0cBLEU\u7684\u5b9a\u4e49\u4e3a\n\n$$ \\exp(\\min(0, 1 - \\frac{len_{ref}}{len_{MT}})) \\prod_{i=1}^k p_n^{1/2^n}$$\n\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u968f\u7740$n$\u7684\u63d0\u9ad8\uff0cn-gram\u7684\u7cbe\u5ea6\u7684\u6743\u503c\u968f\u7740$p_n^{1/2^n}$\u4e2d\u7684\u6307\u6570\u51cf\u5c0f\u800c\u63d0\u9ad8\u3002\u4f8b\u5982$0.5^{1/2} \\approx 0.7, 0.5^{1/4} \\approx 0.84, 0.5^{1/8} \\approx 0.92, 0.5^{1/16} \\approx 0.96$\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u5339\u914d4-gram\u6bd4\u5339\u914d1-gram\u5e94\u8be5\u5f97\u5230\u66f4\u591a\u5956\u52b1\u3002\u53e6\u5916\uff0c\u6a21\u578b\u8f93\u51fa\u8d8a\u77ed\u5f80\u5f80\u8d8a\u5bb9\u6613\u5f97\u5230\u8f83\u9ad8\u7684n-gram\u7684\u7cbe\u5ea6\u3002\u56e0\u6b64\uff0cBLEU\u516c\u5f0f\u91cc\u8fde\u4e58\u9879\u524d\u9762\u7684\u7cfb\u6570\u4e3a\u4e86\u60e9\u7f5a\u8f83\u77ed\u7684\u8f93\u51fa\u3002\u4f8b\u5982\u5f53$k=2$\u65f6\uff0c\u53c2\u8003\u8f93\u51fa\u4e3aABCDEF\uff0c\u800c\u6a21\u578b\u8f93\u51fa\u4e3aAB\uff0c\u6b64\u65f6\u7684$p_1 = p_2 = 1$\uff0c\u800c$\\exp(1-6/3) \\approx 0.37$\uff0c\u56e0\u6b64BLEU=0.37\u3002\u5f53\u6a21\u578b\u8f93\u51fa\u4e5f\u4e3aABCDEF\u65f6\uff0cBLEU=1\u3002\n\n## \u7ed3\u8bba\n\n* \u6211\u4eec\u53ef\u4ee5\u5c06\u7f16\u7801\u5668\u2014\u89e3\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u4e8e\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u3002\n* \u675f\u641c\u7d22\u6709\u53ef\u80fd\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\u3002\n* BLEU\u53ef\u4ee5\u7528\u6765\u8bc4\u4ef7\u7ffb\u8bd1\u7ed3\u679c\u3002\n\n\n## \u7ec3\u4e60\n\n* \u8bd5\u7740\u4f7f\u7528\u66f4\u5927\u7684\u7ffb\u8bd1\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4f8b\u5982[WMT](http://www.statmt.org/wmt14/translation-task.html)\u548c[Tatoeba Project](http://www.manythings.org/anki/)\u3002\u8c03\u4e00\u8c03\u4e0d\u540c\u53c2\u6570\u5e76\u89c2\u5bdf\u5b9e\u9a8c\u7ed3\u679c\u3002\n* Teacher forcing\uff1a\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u8bd5\u7740\u8ba9\u89e3\u7801\u5668\u4f7f\u7528\u5f53\u524d\u65f6\u523b\u7684\u6b63\u786e\u7ed3\u679c\uff08\u800c\u4e0d\u662f\u9884\u6d4b\u7ed3\u679c\uff09\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523b\u7684\u8f93\u5165\u3002\u7ed3\u679c\u4f1a\u600e\u4e48\u6837\uff1f\n\n\n**\u5410\u69fd\u548c\u8ba8\u8bba\u6b22\u8fce\u70b9**[\u8fd9\u91cc](https://discuss.gluon.ai/t/topic/4689)", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6 (Unsupported)", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}