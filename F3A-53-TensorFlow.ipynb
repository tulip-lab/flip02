{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session A - Deep Learning with TensorFlow\n\n**The purpose of this session is to demonstrate how to work with an open source software library for developing deep neural networks apllications, called TensorFlow. In this practical session, we present the following topics:**\n<br>\n- <p><a href=\"#ref1\">1. Getting Started with TensorFlow</a></p>\n- <p><a href=\"#ref2\">2. How does TensorFlow work?</a></p>\n- <p><a href=\"#ref3\">3. Building a Graph</a></p>\n- <p><a href=\"#ref4\">4. Defining multidimensional arrays using TensorFlow</a></p>\n- <p><a href=\"#ref9\">5. Exercises</a></p>\n- <p><a href=\"#refA\">6. A Quick Tour</a></p>\n\n\n<p></p>\n\n\n** References and additional reading and resources**\n- [Installing Tensorflow on Windows](https://www.tensorflow.org/install/install_windows)\n- [Tensorflow API documentations](https://www.tensorflow.org/versions/master/api_docs/python/)\n- [Examples with Tensorflow](https://www.tensorflow.org/versions/master/get_started/)\n- https://www.tensorflow.org/versions/r0.9/get_started/index.html \n- http://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.html \n- https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html \n- https://www.tensorflow.org/versions/r0.9/resources/dims_types.html \n- https://en.wikipedia.org/wiki/Dimension \n- https://book.mql4.com/variables/arrays \n- https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx \n\n* This material is from [CognitiveClass.AI](https://cognitiveclass.ai)\n* Created by: <a href=\"https://ca.linkedin.com/in/rafaelblsilva\"> Rafael Belo Da Silva </a> \n* This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref1\"></a>\n## <span style=\"color:#0b486b\">1. Getting started with TensorFlow</span>\n\nTensorFlow is a powerful open source software library for numerical computation, particularly well-suited for large-scale Machine Learning and highly-optimized for Deep Learning. Its basic principle is simple: you first define in Python a graph of computations to perform, and then TensorFlow takes that graph and runs it efficiently using optimized C++ code.\n\nTensorFlow has many advanced features. The **coolest** are:\n* ***Programmer friendly***: The front-end high-level Python API of TensorFlow offers much more flexibility (at the cost of higher complexity) to efficiently create all sorts of computations, including any neural network architecture you can come up with. In addition, there are many higher-level packages that serve as TensorFlow's wrappers to provide even simpler APIs such as TF.Learn ([http://tflearn.org/](http://tflearn.org/), compatible with Scikit-Learn), Keras ([https://keras.io/](https://keras.io/)), to name a few. You can use these APIs to train various types of neural networks in just a few lines of code.\n* ***Machine learner friendly***: TensorFlow automatically takes care of computing the gradients of the functions you define. This is called automatic differentiating (or `autodiff`).\n* ***TensorBoard***: TensorFlow also comes with a great visualization tool called *TensorBoard* that allows you to browse through the computation graph, view learning curves, and more. This feature is extremely useful for researchers.\n* ***Highly-optimized back-end***: TensorFlow includes highly efficient C++ implementations of many machine learning operations, particularly those needed to build neural networks. There is also a C++ API to define your own high-performance operations. * TensorFlow can train a network with millions of parameters on a training set composed of billions of instances with millions of features each.\n* ***Device switchable***: You can switch between computation on CPU and GPU with one line of code: `tf.device('cpu')` or `tf.device('gpu:x')`.\n* ***Parallelized and Distributed***: It is possible to break up the graph into several chunks and run them in parallel across multiple CPUs or GPUs. Moreover, TensorFlow also supports distributed computing, so you can train colossal neural networks on humongous training sets in a reasonable amount of time by splitting the computations across hundreds of servers.\n* ***Cross-platform***: TensorFlow can run not only on Windows, Linux, and macOS, but also on mobile devices, including both iOS and Android.\n* Last but not least, TensorFlow was developed by **Google Brain** team, and have been being long-term supported and maintained by **Google**. It powers many of Google\u2019s large-scale services, such as Google Cloud Speech, Google Photos, and Google Search.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n### 1.1 How to install TensorFlow\n\n<span style=\"color:blue\">**Step 1.**</span> Install Anaconda (if you have Anaconda installed, you can skip this step)\n  \n<span style=\"color:blue\">**Step 2.**</span> Install TensorFlow on Windows with CPU:\n**`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl`**\n\nIf you want to install Tensorflow on other OSs and/or with GPU, you can follow the instructions in [this guide](https://www.tensorflow.org/install/).\n\n### 1.2<span style=\"color:#0b486b\"> Testing the installation</span>\n\n<span style=\"color:blue\">**Step 1.**</span> Import TensorFlow and print out the version", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nprint(tf.__version__)"
        }, 
        {
            "source": "<span style=\"color:blue\">**Step 2.**</span> Test TensorBoard", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\n\ng = tf.Graph()\n\nwith g.as_default():\n    a = tf.constant(5., name='a')\n    b = tf.constant(2., name='b')\n    c = tf.multiply(a, b, name='c')\n\nlog_dir = 'tf_logs/example00'\nwriter = tf.summary.FileWriter(log_dir, g)  # write the graph to a event file in folder log_dir\n\nwith tf.Session(graph=g) as sess:\n    print(sess.run(c))\n\nwriter.close()  # Close the writer when you're done with it"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls tf_logs/example00"
        }, 
        {
            "source": "* Now open Anaconda prompt and change to the directory that contains your notebook (and the `tf_logs` folder) then type:  \n`tensorboard --logdir=tf_logs/example00`\n\n* The terminal will display: Starting Tensorboard b'47' at http://0.0.0.0:6006 or http://[computer_name]:6006\n* Open your browser and go to http://localhost:6006/. Click \"Graphs\" and you will see:  \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example00/TensorBoard.jpg\", width=800>\n\n* If you want to open tensorboard at **another** port such as 9009, type the following in the terminal:  \n`tensorboard --logdir='tf_logs/example00' --port=9009`  \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"37\", align=\"left\"></img> *Some Windows users may have trouble with tensorboard, type the following in the terminal instead:*<br>\n`tensorboard --logdir=foo:tf_logs/example00`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref2\"></a>\n## 2. How does TensorFlow work?\n\nTensorFlow's capability to execute the code on different devices such as CPUs and GPUs is a consequence of it's specific structure:\n\nTensorFlow defines computations as Graphs, and these are made with operations (also know as \u201cops\u201d). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.\n\nTo execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU.\n\nFor example, the image below represents a graph in TensorFlow. _W_, _x_ and b are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where it's possible to get the wanted result.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img src='https://ibm.box.com/shared/static/a94cgezzwbkrq02jzfjjljrcaozu5s2q.png'>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Importing TensorFlow\n<p>To use TensorFlow, we need to import the library. We imported it and optionally gave it the name \"tf\", so the modules can be accessed by __tf.module-name__:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf"
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref3\"></a>\n## 3. Building a Graph\n\nAs we said before, TensorFlow works as a graph computational model. Let's create our first graph.\n\nTo create our first graph we will utilize __source operations__, which do not need any information input. These source operations or __source ops__ will pass their information to other operations which will execute computations.\n\nTo create two source operations which will output numbers we will define two constants:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = tf.constant([2])\nb = tf.constant([3])"
        }, 
        {
            "source": "After that, let's make an operation over these variables. The function __tf.add()__ adds two elements (you could also use `c = a + b`). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "c = tf.add(a,b)\n#c = a + b is also a way to define the sum of the terms"
        }, 
        {
            "source": "Then TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Let's define our session:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "session = tf.Session()"
        }, 
        {
            "source": "Let's run the session to get the result from the previous defined 'c' operation:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = session.run(c)\nprint(result)"
        }, 
        {
            "source": "Close the session to release resources:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "session.close()"
        }, 
        {
            "source": "To avoid having to close sessions every time, we can define them in a __with__ block, so after running the __with__ block the session will close automatically:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n    result = session.run(c)\n    print(result)"
        }, 
        {
            "source": "Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your edge (In this case our constants), include nodes (operations, like _tf.add_), and start a session to build a graph.\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### 3.1 What is the meaning of Tensor?\n\n<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n<font size = 3><strong>In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.</strong></font>\n<br>\n<br>\nThe word __tensor__ from new latin means \"that which stretches\". It is a mathematical object that is named __tensor__  because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. \n\n<p></p>\n\n</div>\n\nThat's great, but... what are these multidimensional arrays? \n\nGoing back a little bit to physics to understand the concept of dimensions:<br>\n<img src=\"https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg\"/>\n<div style=\"text-align:center\">[[Image Source]](https://en.wikipedia.org/wiki/Dimension) </div>\n\nThe zero dimension can be seen as a point, a single object or a single item.\n\nThe first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements.\n\nThe second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. \n\nThe third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.\n\nThe Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on...", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "As mathematical objects: <br><br>\n<img src=\"https://ibm.box.com/shared/static/kmxz570uai8eeg6i6ynqdz6kmlx1m422.png\">\n<div style=\"text-align:center\">[[Image Source]](https://book.mql4.com/variables/arrays)</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Summarizing:<br><br>\n<table style=\"width:100%\">\n  <tr>\n    <td><b>Dimension</b></td>\n    <td><b>Physical Representation</b></td> \n    <td><b>Mathematical Object</b></td>\n    <td><b>In Code</b></td>\n  </tr>\n  \n  <tr>\n    <td>Zero </td>\n    <td>Point</td> \n    <td>Scalar (Single Number)</td>\n    <td>[ 1 ]</td>\n  </tr>\n\n  <tr>\n    <td>One</td>\n    <td>Line</td> \n    <td>Vector (Series of Numbers) </td>\n    <td>[ 1,2,3,4,... ]</td>\n  </tr>\n  \n   <tr>\n    <td>Two</td>\n    <td>Surface</td> \n    <td>Matrix (Table of Numbers)</td>\n    <td>[ [1,2,3,4,...], [1,2,3,4,...], [1,2,3,4,...],... ]</td>\n  </tr>\n  \n   <tr>\n    <td>Three</td>\n    <td>Volume</td> \n    <td>Tensor (Cube of Numbers)</td>\n    <td>[ [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...] ,...]... ]</td>\n  </tr>\n  \n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref4\"></a>\n## 4. Defining multidimensional arrays using TensorFlow\nNow we will try to define such arrays using TensorFlow:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Scalar = tf.constant([2])\nVector = tf.constant([5,6,2])\nMatrix = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\nTensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )\nwith tf.Session() as session:\n    result = session.run(Scalar)\n    print (\"Scalar (1 entry):\\n %s \\n\" % result)\n    result = session.run(Vector)\n    print (\"Vector (3 entries) :\\n %s \\n\" % result)\n    result = session.run(Matrix)\n    print (\"Matrix (3x3 entries):\\n %s \\n\" % result)\n    result = session.run(Tensor)\n    print (\"Tensor (3x3x3 entries) :\\n %s \\n\" % result)"
        }, 
        {
            "source": "Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\nMatrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])\n\nfirst_operation = tf.add(Matrix_one, Matrix_two)\nsecond_operation = Matrix_one + Matrix_two\n\nwith tf.Session() as session:\n    result = session.run(first_operation)\n    print (\"Defined using tensorflow function :\")\n    print(result)\n    result = session.run(second_operation)\n    print (\"Defined using normal expressions :\")\n    print(result)"
        }, 
        {
            "source": "With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. <br>\n\nBut what if we want the regular matrix product?\n\nWe then need to use another TensorFlow function called __tf.matmul()__:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Matrix_one = tf.constant([[2,3],[3,4]])\nMatrix_two = tf.constant([[2,3],[3,4]])\n\nfirst_operation = tf.matmul(Matrix_one, Matrix_two)\n\nwith tf.Session() as session:\n    result = session.run(first_operation)\n    print (\"Defined using tensorflow function :\")\n    print(result)"
        }, 
        {
            "source": "We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref5\"></a>\n### 4.1 Why Tensors?\n\nThe Tensor structure helps us by giving the freedom to shape the dataset the way we want.\n\nAnd it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,\n\nThinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional strucutre (a matrix)... until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particulary helpful.\n\nImages are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this:\n\n<img src='https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png'>\n<div style=\"text-align:center\">[[Image Source]](https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131.aspx)</div>\n\nSo the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref6\"></a>\n### 4.2 Variables\n\nNow that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables.\n\nTo define variables we use the command __tf.variable()__.\nTo be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running __tf.global_variables_initializer()__.\n\nTo update the value of a variable, we simply run an assign operation that assigns a value to the variable:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "state = tf.Variable(0)"
        }, 
        {
            "source": "Let's first create a simple counter, a variable that increases one unit at a time:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "one = tf.constant(1)\nnew_value = tf.add(state, one)\nupdate = tf.assign(state, new_value)"
        }, 
        {
            "source": "Variables must be initialized by running an initialization operation after having launched the graph.  We first have to add the initialization operation to the graph:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "init_op = tf.global_variables_initializer()"
        }, 
        {
            "source": "We then start a session to run the graph, first initialize the variables, then print the initial value of the __state__ variable, and then run the operation of updating the __state__ variable and printing the result after each update:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n  session.run(init_op)\n  print(session.run(state))\n  for _ in range(3):\n    session.run(update)\n    print(session.run(state))"
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref7\"></a>\n### 4.3 Placeholders\n\nNow we know how to manipulate variables inside TensorFlow, but what about feeding data outside of a TensorFlow model? \n\nIf you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders.\n\nSo what are these placeholders and what do they do? \n\nPlaceholders can be seen as \"holes\" in your model, \"holes\" which you will pass the data to, you can create them using <br/> <b>tf.placeholder(_datatype_)</b>, where <b>_datatype_</b> specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits.\n\nThe definition of each data type with the respective python sintax is defined as:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "|Data type\t|Python type|Description|\n| --------- | --------- | --------- |\n|DT_FLOAT\t|tf.float32\t|32 bits floating point.|\n|DT_DOUBLE\t|tf.float64\t|64 bits floating point.|\n|DT_INT8\t|tf.int8\t|8 bits signed integer.|\n|DT_INT16\t|tf.int16\t|16 bits signed integer.|\n|DT_INT32\t|tf.int32\t|32 bits signed integer.|\n|DT_INT64\t|tf.int64\t|64 bits signed integer.|\n|DT_UINT8\t|tf.uint8\t|8 bits unsigned integer.|\n|DT_STRING\t|tf.string\t|Variable length byte arrays. Each element of a Tensor is a byte array.|\n|DT_BOOL\t|tf.bool\t|Boolean.|\n|DT_COMPLEX64\t|tf.complex64\t|Complex number made of two 32 bits floating points: real and imaginary parts.|\n|DT_COMPLEX128\t|tf.complex128\t|Complex number made of two 64 bits floating points: real and imaginary parts.|\n|DT_QINT8\t|tf.qint8\t|8 bits signed integer used in quantized Ops.|\n|DT_QINT32\t|tf.qint32\t|32 bits signed integer used in quantized Ops.|\n|DT_QUINT8\t|tf.quint8\t|8 bits unsigned integer used in quantized Ops.|\n\n<div style=\"text-align:center\">[[Table Source]](https://www.tensorflow.org/versions/r0.9/resources/dims_types.html)</div>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So we create a placeholder:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a=tf.placeholder(tf.float32)"
        }, 
        {
            "source": "And define a simple multiplication operation:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "b=a*2"
        }, 
        {
            "source": "Now we need to define and run the session, but since we created a \"hole\" in the model to pass the data, when we initialize the session we are obligated to pass an argument with the data, otherwise we would get an error.\n\nTo pass the data to the model we call the session with an extra argument <b> feed_dict</b> in which we should pass a dictionary with each placeholder name folowed by its respective data, just like this:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as sess:\n    result = sess.run(b,feed_dict={a:3.5})\n    print (result)"
        }, 
        {
            "source": "Since data in TensorFlow is passed in form of multidimensional arrays we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dictionary={a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] }\n\nwith tf.Session() as sess:\n    result = sess.run(b,feed_dict=dictionary)\n    print (result)"
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref8\"></a>\n## 5. Operations\n\nOperations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function.\n\n_tf.matmul_, _tf.add_, _tf.nn.sigmoid_ are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. \n<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">Other operations can be easily found in: https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = tf.constant([5])\nb = tf.constant([2])\nc = tf.add(a,b)\nd = tf.subtract(a,b)\n\nwith tf.Session() as session:\n    result = session.run(c)\n    print ('c =: %s' % result)\n    result = session.run(d)\n    print ('d =: %s' % result)"
        }, 
        {
            "source": "_tf.nn.sigmoid_ is an activiation function, it's a little more complicated, but this function helps learning models to evaluate what kind of information is good or not.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "-----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref9\"></a>\n\n\n## 5. Exercises\n\n\nBefore everything, let's import the TensorFlow library", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\nimport tensorflow as tf"
        }, 
        {
            "source": "First, try to add the two constants and print the result.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = tf.constant([5])\nb = tf.constant([2])"
        }, 
        {
            "source": "create another TensorFlow object applying the sum (+) operation:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Your code goes here\n\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#sum1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #1</a>\n<a href=\"#sum2\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #2</a>\n</div>\n<div id=\"sum1\" class=\"collapse\">\n```\nc=a+b\n```\n</div>\n<div id=\"sum2\" class=\"collapse\">\n```\nc=tf.add(a,b)\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n    result = session.run(c)\n    print (\"The addition of this two constants is: {0}\".format(result))"
        }, 
        {
            "source": "Now let's try to multiply them.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Your code goes here. Use the multiplication operator.\n\n\n\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#mult1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #1</a>\n<a href=\"#mult2\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #2</a>\n</div>\n<div id=\"mult1\" class=\"collapse\">\n```\nc=a*b\n```\n</div>\n<div id=\"mult2\" class=\"collapse\">\n```\nc=tf.multiply(a,b)\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n    result = session.run(c)\n    print (\"The Multiplication of this two constants is: {0}\".format(result))"
        }, 
        {
            "source": "### 5.1 Multiplication: element-wise or matrix multiplication\n\nLet's practice the different ways to multiply matrices:\n- **Element-wise** multiplication in the **first operation** ;\n- **Matrix multiplication** on the **second operation**  ;", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "matrixA = tf.constant([[2,3],[3,4]])\nmatrixB = tf.constant([[2,3],[3,4]])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Your code goes here\n\n\n\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#matmul1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n<div id=\"matmul1\" class=\"collapse\">\n```\nfirst_operation=tf.multiply(matrixA, matrixB)\nsecond_operation=tf.matmul(matrixA,matrixB)\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n    result = session.run(first_operation)\n    print (\"Element-wise multiplication: \\n\", result)\n\n    result = session.run(second_operation)\n    print (\"Matrix Multiplication: \\n\", result)"
        }, 
        {
            "source": "---\nModify the value of variable b to the value in constant a: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a=tf.constant(1000)\nb=tf.Variable(0)\ninit_op = tf.global_variables_initializer()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Your code goes here\n\n\n\n\n\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#assign\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n<div id=\"assign\" class=\"collapse\">\n```\na=tf.constant(1000)\nb=tf.Variable(0)\ninit_op = tf.global_variables_initializer()\nupdate = tf.assign(b,a)\nwith tf.Session() as session:\n    session.run(init_op) \n    session.run(update) \n    print(session.run(b))\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n### 5.2 Fibonacci sequence\n\nNow try to do something more advanced. Try to create a __fibonnacci sequence__ and print the first few values using TensorFlow:</b></h3>\n\nIf you don't know, the fibonnacci sequence is defined by the equation: <br><br>\n$$F_{n} = F_{n-1} + F_{n-2}$$<br>\nResulting in a sequence like: 1,1,2,3,5,8,13,21...\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#fibonacci-solution\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #1</a>\n<a href=\"#fibonacci-solution2\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution #2</a>\n</div>\n\n\n<div id=\"fibonacci-solution\" class=\"collapse\">\n```\na=tf.Variable(0)\nb=tf.Variable(1)\ntemp=tf.Variable(0)\nc=a+b\n\nupdate1=tf.assign(temp,c)\nupdate2=tf.assign(a,b)\nupdate3=tf.assign(b,temp)\n\ninit_op = tf.initialize_all_variables()\nwith tf.Session() as s:\n\ts.run(init_op)\n\tfor _ in range(15):\n\t\tprint(s.run(a))\n\t\ts.run(update1)\n\t\ts.run(update2)\n\t\ts.run(update3)\n```\n</div>\n\n\n<div id=\"fibonacci-solution2\" class=\"collapse\">\n```\nf = [tf.constant(1),tf.constant(1)]\n\nfor i in range(2,10):\n\ttemp = f[i-1] + f[i-2]\n\tf.append(temp)\n\nwith tf.Session() as sess:\n\tresult = sess.run(f)\n\tprint result\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n\n** Now try to create your own placeholders and define any kind of operation between them: **\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Your code goes here\n\n\n\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#placeholder\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n\n\n<div id=\"placeholder\" class=\"collapse\">\n```\n\na=tf.placeholder(tf.float32)\nb=tf.placeholder(tf.float32)\n\nc=2*a -b\n\ndictionary = {a:[2,2],b:[3,4]}\nwith tf.Session() as session:\n\tprint session.run(c,feed_dict=dictionary)\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "** Try changing our example with some other operations and see the result. **\n\n<font size = 3><strong>Some examples of functions:</strong></font>  \n- tf.multiply(x, y)<br />\n- tf.div(x, y)<br />\n- tf.square(x)<br />\n- tf.sqrt(x)<br />\n- tf.pow(x, y)<br />\n- tf.exp(x)<br />\n- tf.log(x)<br />\n- tf.cos(x)<br />\n- tf.sin(x)<br /> <br>\n\nYou can also take a look at [more operations]( https://www.tensorflow.org/versions/r0.9/api_docs/python/math_ops.html)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = tf.constant(5.)\nb = tf.constant(2.)"
        }, 
        {
            "source": "create a variable named **`c`** to receive the result an operation (at your choice):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#your code goes here\n\n\n"
        }, 
        {
            "source": "\n<div align=\"right\">\n<a href=\"#operations\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n\n\n<div id=\"operations\" class=\"collapse\">\n```\nc=tf.sin(a)\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as session:\n    result = session.run(c)\n    print (\"c =: {}\".format(result))"
        }, 
        {
            "source": "They're really similar to mathematical functions the only difference is that operations works over tensors.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n\n<a id=\"refA\"></a>\n\n\n## 6. A quick tour of TensorFlow\n\nAfter Tensorflow is installed, we can import the TensorFlow as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf"
        }, 
        {
            "source": "Let's go with a very simple piece of code first!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x * x * y + y + 2"
        }, 
        {
            "source": "The code does not actually perform any computation yet. It just creates a computation graph. In fact, even the variables are not initialized yet.\n\n### <span style=\"color:#0b486b\">6.1. TensorFlow session</span>\n\nTo evaluate this graph, you need to open a TensorFlow ***session*** and use it to initialize the variables and evaluate the function *`f`*. A TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them, and it holds all the variable values.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; In distributed TensorFlow, variable values are stored on the servers instead of the session.\n\nThe following code creates a session, initializes the variables, evaluates function *`f`* then closes the session (which frees up resources):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess = tf.Session()\nsess.run(x.initializer)\nsess.run(y.initializer)\nresult = sess.run(f)\nprint(result)\nsess.close()"
        }, 
        {
            "source": "* **`with`** block: If you don't want to repeat *`sess.run()`* all the time, you can use *`with`* statement as below:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()\n    print(result)"
        }, 
        {
            "source": "Inside the *`with`* block, the session is set as the default session. Calling *`x.initializer.run()`* is equivalent to calling *`tf.get_default_session().run(x.initializer)`*, and similarly *`f.eval()`* is equivalent to calling *`tf.get_default_session().run(f)`*. This makes the code easier to read.<br> \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; We don't need to call *`sess.close()`* here because the *`with`* context manager does it automatically.\n\n* **Interactive session**: Alternatively, you can even get rid of *`with`* block by creating an interactive session (*`InteractiveSession`*) when you are working with Jupyter notebook or a Python shell. The only difference from a regular Session is that when an *`InteractiveSession`* is created, it automatically sets itself as the default session. However, you do need to close the session manually when you are done.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess = tf.InteractiveSession()\nx.initializer.run()\ny.initializer.run()\nprint(f.eval())\nsess.close()"
        }, 
        {
            "source": "* **Global variables initialization**: Instead of manually running the initializer for every single variable, you can use the *`global_variables_initializer()`* function, and run it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "init = tf.global_variables_initializer()  # prepare an init node\nwith tf.Session() as sess:\n    init.run()  # actually initialize all the variables\n    result = f.eval()\n    print(result)"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">6.2. Managing TensorFlow graphs</span>\n\nAny node you create is automatically added to the default graph. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()"
        }, 
        {
            "source": "In most cases this is fine, but sometimes you may want to manage multiple independent graphs. You can do this by creating a new *`Graph`* and temporarily making it the default graph inside a *`with`* block, like so:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "graph = tf.Graph()\nwith graph.as_default():\n    x2 = tf.Variable(2)\nx2.graph is graph"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x2.graph is tf.get_default_graph()"
        }, 
        {
            "source": "Sometime the default graph could have duplicated nodes (variables). We can reset to clean the graph as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">6.3. Lifecycle of a node value</span>\n\nWhen you evaluate a node, TensorFlow automatically determines the set of nodes that it depends on and it evaluates these nodes first. Let's look at the following code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "w = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval())  # 10\n    print(z.eval())  # 15"
        }, 
        {
            "source": "The code evaluates *`y`* and *`z`*. For *`y`*, TensorFlow automatically detects that *`y`* depends on *`x`*, which depends on *`w`*, so it first evaluates *`w`*, then *`x`*, and then *`y`*. The schedule is: *`w -> x -> y`*. Likewise, the order for evaluating *`z`* is: *`w -> x -> z`*. It is important to note that it will ***not reuse*** the result of the previous evaluation of *`w`* and *`x`*. In short, the preceding code evaluates *`w`* and *`x`* ***twice***.<br>\n\nAll node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs. A variable starts its life when its initializer is run, and it ends when the session is closed.\n\nIf you want to evaluate *`y`* and *`z`* more efficiently, i.e., without evaluating *`w`* and *`x`* twice as in the previous code, you must ask TensorFlow to simultaneously evaluate both *`y`* and *`z`* in just one graph run as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as sess:\n    y_val, z_val = sess.run([y, z])\n    print(y_val)  # 10\n    print(z_val)  # 15"
        }, 
        {
            "source": "Sometimes, we will have two **independent** operations (ops) but you\u2019d like to specify which operation (op) should be run **first**. You can create context manager that specifies control dependencies for all operations constructed within the context. For example, the following codes call for an increment of *`global_step`* each time we compute *`learning_rate`*:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nwith tf.control_dependencies([incr]):\n    learning_rate = starter_lr * tf.pow(decay_rate, global_step)\n    \nwith tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(5):\n        print('Global Step %d: Learning rate = %f' % (global_step.eval(), learning_rate.eval()))"
        }, 
        {
            "source": "**Without** controlling dependences: *`global_step`* will stay at `0.0`, and *`learning_rate`* will be `1.0`:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nlearning_rate = starter_lr * tf.pow(decay_rate, global_step)\n    \nwith tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(5):\n        print('Global step %d: Learning rate = %f' % (global_step.eval(), learning_rate.eval()))\n        "
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">6.4. Placehoder</span>\n\nSuppose that you are evaluating a function *`y`* that takes an input *`x`*. You want to do this many times and change the input *`x`* at each time (as in an iterative algorithm when we want to replace the input data at every iteration). The simplest way to do this is to use *`placeholder`* nodes. These nodes are special because they don\u2019t actually perform any computation, they\njust output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training. Let's look at a simple example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport tensorflow as tf\ntf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, shape=[None, 3])\ny = x + 2\nwith tf.Session() as sess:\n    print(y.eval(feed_dict={x: np.ones([1, 3])}))  #  feed 1x3 array \n    print(y.eval(feed_dict={x: np.zeros([2, 3])})) #  feed 2x3 array "
        }, 
        {
            "source": "If you don\u2019t specify a value at runtime for a placeholder, you get an exception. Try this:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as sess:\n    print(y.eval())"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">6.5. Save and restore models</span>\n\nOnce you have trained your model, you should save its parameters to disk so that you can come back to it whenever you want, use it in another program, compare it to other models, and so on. Moreover, you probably want to save checkpoints at regular intervals during training so that if your computer crashes or encounters power-outage during training you can continue from the last checkpoint rather than start over from scratch.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> You might get into error if the folder path `models/example01` does not exist. You can create this folder in the directory containing this notebook.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!mkdir models/example01\n!ls models\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\ntf.reset_default_graph()\n\ntheta = tf.Variable(tf.zeros([5]), name='theta')\ntrain_op = tf.assign(theta, theta + 1.)\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nn_epochs = 200\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(n_epochs):\n        sess.run(train_op)\n        if (epoch + 1) % 100 == 0:  # checkpoint every 100 epochs\n            saver.save(sess, \"models/example01/save_and_restore.ckpt\")\n            \n        \n        \n    best_theta = theta.eval()\n    print(best_theta)"
        }, 
        {
            "source": "Restoring a model is just as easy: you create a `Saver` at the end of the construction phase just like before, but then at the beginning of the execution phase, instead of initializing the variables using the `init` node, you call the `restore()` method of the `Saver` object:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\ntheta = tf.Variable(tf.zeros([5]), name='theta')\ntrain_op = tf.assign(theta, theta + 1.)\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nn_epochs = 200\nwith tf.Session() as sess:\n    saver.restore(sess, \"models/example01/save_and_restore.ckpt\")\n    for epoch in range(n_epochs):\n        sess.run(train_op)\n        if (epoch + 1) % 100 == 0:  # checkpoint every 100 epochs\n            saver.save(sess, \"models/example01/save_and_restore_cont.ckpt\")\n        \n    best_theta = theta.eval()\n    print(best_theta)"
        }, 
        {
            "source": "After restoring and training for another 200 steps, best_theta is now [400.  400.  400.  400.  400.]\n\n### <span style=\"color:#0b486b\">6.6. Visualize computational graph and learning curves in TensorBoard</span>\n\nNormally, we rely on the `print()` function and `matplotlib` to visualize progress during training. There is a better way, i.e., using TensorBoard. If you feed it some training stats, it will display nice interactive visualizations of\nthese stats in your web browser (e.g., learning curves). You can also provide it the graph\u2019s definition and\nit will give you a great interface to browse through it. This is very useful to identify errors in the graph, to\nfind bottlenecks, and so on. Let's visualize learning rate and global step in the example above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# construction\ntf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nwith tf.control_dependencies([incr]):\n    learning_rate = starter_lr * tf.pow(decay_rate, global_step)"
        }, 
        {
            "source": "Here, we construct the graph as normal:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.summary.scalar('learning_rate', learning_rate)\ntf.summary.scalar('global_step', global_step)\nmerged = tf.summary.merge_all() # Merges all summaries collected in the default graph"
        }, 
        {
            "source": "The first two lines create two summary *`ops`* in the graph that will evaluate the *`learning_rate`* and *`global_step`* value and write them to a TensorBoard compatible binary log string called a summary. The third line creates a node that merges all summaries collected in the default graph. In the execution phase, you'll need to evaluate the merged node regularly during training (e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using the *`file_writer`*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import time\nlogdir = \"tf_logs/example01/model-at-{}\".format(time.strftime('%Y-%m-%d_%H.%M.%S'))\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
        }, 
        {
            "source": "You need to use a different log directory every time you run your program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. The simplest solution for this is to include a timestamp in the log directory name. Now's the execution phase:\n\nThe first line creates a node in the graph that will evaluate the *MSE* value and write it to a TensorBoard compatible binary log string called a summary. Then you need to update the execution phase to evaluate the *`mse_summary`* node regularly during training\n(e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using\nthe *`file_writer`*. Here is the updated code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(50):\n        merged_ = merged.eval()\n        file_writer.add_summary(merged_, i + 1)"
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> In actual Deep Learning implementation logging training stats at every single training step, as this would significantly slow down training. Instead, you should log 200 iterations for example.\n\nFinally, you want to close the FileWriter at the end of the program:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_writer.close()"
        }, 
        {
            "source": "Great! Now it\u2019s time to fire up the TensorBoard server. You need to activate your virtual environment\nif you created one, then start the server by running the *`tensorboard`* command, pointing it to the root log\ndirectory. This starts the TensorBoard web server, listening on port 6006 (which is \u201cgoog\u201d written upside\ndown :) )\nNext open a browser and go to http://0.0.0.0:6006/ (or http://localhost:6006/). Welcome to\nTensorBoard! In the Scalars tab, you'll see *`global_step`* and *`learning_rate`*:\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/learning_rate.png' width=300>\n\n### <span style=\"color:#0b486b\">6.7. Name Scopes</span>\n\nWhen dealing with more complex models such as neural networks, the graph can easily become cluttered with thousands of nodes. To avoid this, you can create name scopes to group related nodes. The following code defines two operations sum and product under two scopes:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\na = tf.placeholder(tf.float32, shape=(), name='a')\nb = tf.placeholder(tf.float32, shape=(), name='b')\n\nwith tf.name_scope('calc'):\n    c = a + b\n    d = a + b\n    e = a * b\n    f = tf.add(a, b, name='sum')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(c.op.name)\nprint(d.op.name)\nprint(e.op.name)\nprint(f.op.name)"
        }, 
        {
            "source": "The name of each op defined within the scope is now prefixed with \"calc/\". `c`, `d`, and `e` get generic names while `d` gets the name 'sum' that we passed into the operation. Note that `d` and `c` has the same generic name *`add`* but `d` is defined created later so it get the name *`calc/add_1`*.\n\nFor more detail about *`tf.name_scope`*, read: https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope.\n\n### <span style=\"color:#0b486b\">6.8. Modularity</span>\n\nSuppose you want to create a graph that adds the output of two [rectified linear units (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks). A ReLU computes a linear function of the inputs, and outputs the result if it is positive, and 0 otherwise, as shown in the following equation:\n\n$$h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{x})=\\max(\\mathbf{W}^T\\mathbf{x} + \\mathbf{b}, 0)$$\n\nThe following does the job but quit repetitive:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nw1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\nw2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n\nb1 = tf.Variable(0.0, name=\"bias1\")\nb2 = tf.Variable(0.0, name=\"bias2\")\n\nz1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\nz2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n\nrelu1 = tf.maximum(z1, 0., name=\"relu1\")\nrelu2 = tf.maximum(z1, 0., name=\"relu2\")\n\noutput = tf.add(relu1, relu2, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()"
        }, 
        {
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph01.PNG' width=500>\n\nThe graph looks unorganized and hard to follow! Suppose we want to creates many ReLUs and outputs their sum, the graph is very messy and is hard to follow.\n\nThe following code creates five ReLUs and outputs their sum (note that *`add_n()`* creates an operation that will compute the sum of a list of tensors):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    w_shape = (int(X.get_shape()[1]), 1)\n    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n    b = tf.Variable(0.0, name=\"bias\")\n    z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n    return tf.maximum(z, 0., name=\"relu\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity_clean'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()"
        }, 
        {
            "source": "Note that when you create a node, TensorFlow checks whether its name already exists, and if it does, TensorFlow appends an underscore followed by an index to make the name unique. So the first ReLU contains nodes named \"weights\", \"bias\", \"z\", and \"relu\" (plus many more nodes with their default name, such as \"MatMul\"); the second ReLU contains nodes named \"weights_1\", \"bias_1\", and so on; the third ReLU contains nodes named \"weights_2\", \"bias_2\", and so on. TensorBoard identifies such series and collapses them together to reduce clutter.\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph02.PNG' width=500>\n\nLet's try using name scopes to see if we can make the graph clearer!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    with tf.name_scope('relu'):\n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, 0., name=\"relu\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity_clearer'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()"
        }, 
        {
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph03.PNG' width=600>\n\nThe graph now looks much clearer. Notice that TensorFlow also gives the name scopes unique names by appending _1, _2, and so on. If we expand one of the relu, we'll see:\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph04.PNG' width=600>\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; Weights and bias are now within the name_scope relu2 so we don't see _2 appended to the name.\n\n### 6.9 Sharing Variables\n\nSuppose you want to control the ReLU threshold (currently hardcoded to 0) using a shared threshold variable for all ReLUs. You can use the *`get_variable()`* function to create the shared variable if it does not exist yet, or reuse it if it already exists. The desired behavior (creating or reusing) is controlled by an attribute of the current *`variable_scope()`*. For example, the following code will create a variable named *`\"relu/threshold\"`* (as a scalar, since *`shape=()`*, and using\n0.0 as the initial value):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.variable_scope(\"relu\"):\n    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))"
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> If the variable has already been created by an earlier call to *`get_variable()`*, this code will raise an exception. This behavior prevents reusing variables by mistake. If you want to reuse a variable, you need to explicitly say so by setting the variable scope\u2019s reuse attribute to *`True`*.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.variable_scope(\"relu\", reuse=True):\n    threshold = tf.get_variable(\"threshold\")"
        }, 
        {
            "source": "This code will fetch the existing *`\"relu/threshold\"`* variable, or raise an exception if it does not exist or if it was not created using *`get_variable()`*. Alternatively, you can set the reuse attribute to *`True`* inside the block by calling the scope\u2019s *`reuse_variables()`* method:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.variable_scope(\"relu\") as scope:\n    scope.reuse_variables()\n    threshold = tf.get_variable(\"threshold\")"
        }, 
        {
            "source": "Now you have all the pieces you need to make the *`relu()`* function access the threshold variable without having to pass it as a parameter:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    with tf.variable_scope('relu', reuse=True):\n        threshold = tf.get_variable(\"threshold\")\n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, threshold, name=\"max\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nwith tf.variable_scope(\"relu\"): # create the variable\n    threshold = tf.get_variable(\"threshold\", shape=(),\n                                initializer=tf.constant_initializer(0.0))\n    \nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/customized_relu'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()"
        }, 
        {
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph05.PNG' width=800>\n\nThis code first defines the *`relu()`* function, then creates the *`relu/threshold`* variable (as a scalar that will later be initialized to 0.0) and builds five ReLUs by calling the *`relu()`* function. The *`relu()`* function reuses the *`relu/threshold`* variable, and creates the other ReLU nodes. Therefore, we see the \"scalar\" connections between the first created ReLU and the other 5 ReLUs, meaning that 5 ReLUs reuse the threshold scalar from the first created ReLU.\n\nTo avoid creating the first unused ReLU. We can pass the reuse argument to the function *`relu()`*. We'll set *`reuse = True`* when after the first ReLU is created.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\ndef relu(X, reuse=False):\n    with tf.variable_scope('relu') as scope:\n        if reuse:\n            scope.reuse_variables()\n            \n        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n        \n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, threshold, name=\"max\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nrelus = [relu(X, reuse=i>0) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/customized_relu2'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()"
        }, 
        {
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph06.PNG' width=600>\n\nThis time, exactly 5 ReLUs are created, with *`ReLU_[1,2,3,4]`* reuse the threshold scalar from the first ReLU.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "widgets": {
            "state": {}, 
            "version": "1.1.2"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}