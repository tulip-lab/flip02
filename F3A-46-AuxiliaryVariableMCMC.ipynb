{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session P - Using Auxiliary Variables in MCMC proposals", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Slice sampling\n\nSlice sampling is a simple MCMC algorithm that introduces the idea of auxiliary variables. The motivation for slice sampling is that if we can sample uniformly from the region under the graph of the target distribution, we will have random samples from the target distribution. In the univariate case, the algorithm is as follows\n\n- start with some $x$ where $p(x) \\ne 0$\n- repeat\n    - sample $y$ (auxiliary variable) uniformly from 0 to $f(x)$\n    - draw a horizontal line at $y$ within $p(x)$ (this may consist of multiple intervals)\n    - sample $x$ from the horizontal segments\n\nThe auxiliary $y$ variable allows us to sample $(x, y)$ points that are in the region under the graph of the target distribution. Only the $x$ variable is used for the Monte Carlo samples - the $y$ variables are simply discarded. This works because the joint disribution is constructed so that it factors $p(x, y) = p(y \\mid x) p(x)$ and so projecting leaves just $p(x)$. The slice sampler is a Markov Chain Monte Carlo method since the next $(x, y)$ position depends only on the current position. Like Gibbs sampling, there is no tuning process and all proposals are accepted. For slice sampling, you either need the inverse distribution function or some way to estimate it. Later we will see that Hamiltonian Monte Carlo also uses auxiliary variables to generate a new proposal in an analogous way.\n\nA toy example illustrates the process - Suppose we want to draw random samples from the posterior distribution $\\mathcal{N}(0, 1)$ using slice sampling\n\nStart with some value $x$\n- sample $y$ from $\\mathcal{U}(0, f(x))$ - this is the horizontal \"slice\" that gives the method its name\n- sample the next $x$ from $f^{-1}(y)$ - this is typically done numerically\n- repeat\n\nWill sketch picture in class to show what is going on.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## A simple slice sampler example", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import scipy.stats as stats\nimport numpy as np\nimport matplotlib.pyplot as plt"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dist = stats.norm(5, 3)\nw = 0.5\nx = dist.rvs()\n\nniters = 1000\nxs = []\nwhile len(xs) < niters:\n    y = np.random.uniform(0, dist.pdf(x))\n    lb = x\n    rb = x\n    while y < dist.pdf(lb):\n        lb -= w\n    while y < dist.pdf(rb):\n        rb += w\n    x = np.random.uniform(lb, rb)\n    if y > dist.pdf(x):\n        if np.abs(x-lb) < np.abs(x-rb):\n            lb = x\n        else:\n            lb = y\n    else:\n        xs.append(x)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.hist(xs, 20)\npass"
        }, 
        {
            "source": "Notes on the slice sampler:\n\n- the slice may consist of disjoint pieces for multimodal distributions\n- the slice can be a rectangular hyperslab for multivariable posterior distributions\n- sampling from the slice (i.e. finding the boundaries at level $y$) is non-trivial and may involve iterative rejection steps - see figure below (from Wikimedia) for a typical approach - the blue bars represent disjoint pieces of the true slice through a bimodal distribution and the black lines are the proposal distribution approximating the true slice\n\n![Slice sampling algorithm from Wikipedia](http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Summary_of_slice_sampling.png/750px-Summary_of_slice_sampling.png)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Hamiltonian Monte Carlo (HMC)\n\nHMC uses an auxiliary variable corresponding to the momentum of particles in a potential energy well to generate proposal distributions that can make use of gradient information in the posterior distribution. For reversibility to be maintained, the total energy of the particle has to be conserved - hence we are interested in Hamiltonian systems. The main attraction of HMC is that it works much better than other methods when variables of interest are highly correlated. Because we have to solve problems involving momentum, we need to understand how to numerically solve differential equations in a way that is both accurate (i.e. second order) and preserves total energy (necessary for a Hamiltonian system).\n\nExample adapted from [MCMC: Hamiltonian Monte Carlo (a.k.a. Hybrid Monte Carlo)](https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Hamiltonian systems\n\nIn a Hamiltonian system, we consider particles with position $x$ and momentum (or velocity if we assume unit mass) $v$. The total energy of the system $H(x, v) = K(v) + U(x)$, where $K$ is the kinetic energy and $U$ is the potential energy, is conserved. Such a system satisfies the following Hamiltonian equations\n\n$$\n\\begin{align}\n\\frac{dx}{dt} &= & \\frac{\\delta H}{dv} \\\\\n\\frac{dv}{dt} &= & -\\frac{\\delta H}{dx} \n\\end{align}\n$$\n\nSince $K$ depends only on $v$ and $U$ depends only on $x$, we have\n$$\n\\begin{align}\n\\frac{dx}{dt} &= & \\frac{\\delta K}{dv} \\\\\n\\frac{dv}{dt} &= & -\\frac{\\delta U}{dx}\n\\end{align}\n$$\n\n### Harmonic oscillator\n\nWe will consider solving a classical Hamiltonian system - that of a undamped spring governed by the second order differential equation\n\n$$\nx'' + x = 0\n$$\n\nWe convert this to two first order ODEs by using a dummy variable $x' = v$ to get\n\n$$\n\\begin{align}\nx' &= v \\\\\nv' &= -x\n\\end{align}\n$$\n\nFrom the Hamiltonian equations above, this is equivalent to a system with kinetic energy $K(v) = \\frac{1}{2}v^2$ and potential energy $U(x) = \\frac{1}{2}x^2$.\n\nWriting in matrix form,\n\n$$\nA = \\pmatrix{ x' \\\\ v' } = \\pmatrix{0 & 1 \\\\ -1 & 0} \\pmatrix{x \\\\ v}\n$$\n\nand in general, for the state vector $x$,\n\n$$\nx' = Ax\n$$\n\nWe note that $A$ is anti- or skew-symmetric ($A^T = -A$), and hence has purely imaginary eigenvalues. Solving $|A - \\lambda I = 0$, we see that the eigenvalues and eigenvectors are $i, \\pmatrix{1\\\\i}$ and $-i, \\pmatrix{1\\\\-i}$. Since the eigenvalues are pure imaginary, we see that the solution for the initial conditions $(x,v) = (1, 0)$ is $x(t) = e^{it}$ and the orbit just goes around a circle with a period of $2\\pi$, neither growing nor decaying. Another weay of seeing this is that the Hamiltonian $H(u, v)$ or sum of potential ($U(x)) = \\frac{1}{2}x^2$) and kinetic energy ($K(v) = \\frac{1}{2}v^2$) is constant, i.e. in vector form, $(x^T x) = \\text{constant}$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Finite difference methods\n\nWe want to find a finite difference approximation to $u' = Au$ that is **accurate** and **preserves total energy**. If total energy is not preserved, the orbit will either spiral in towards zero or outwards away from the unit circle. If the accuracy is poor, the orbit will not be close to its starting value after $t = 2\\pi$. This gives us an easy way to visualize how good our numerical scheme is. We can also compare the numerical scheme to the Taylor series to evaluate its accuracy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Forward Euler\n\nThe simplest finite difference scheme for integrating ODEs is the forward Euler\n\n$$\n\\frac{u_{n+1} - u_n}{\\Delta t} = A u_n\n$$\n\nRearranging terms, we get\n\n$$\nu_{n+1} = u_n + \\Delta t A u_n = \\left( I + \\Delta t A \\right) u_n\n$$\n\nSince the eigenvalues of $A$ are $\\pm i$, we see that the eigenvalues of the forward Euler matrix are $1 \\pm i$. Since the absolute value of the eigenvalues is greater than 1, we expect **growing** solutions - i.e. the solution will spiral away from the unit circle.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import scipy.linalg as la"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def f_euler(A, u, N):\n    orbit = np.zeros((N,2))\n\n    dt = 2*np.pi/N\n    for i in range(N):\n        u = u + dt * A @ u\n        print(u)\n        orbit[i] = u\n    return orbit"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "A = np.array([[0,1],[-1,0]])\nu = np.array([1.0,0.0])\nN = 64\norbit = f_euler(A, u, N)"
        }, 
        {
            "source": "#### Accuracy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "la.norm(np.array([1.0,0.0]) - orbit[-1])"
        }, 
        {
            "source": "#### Conservation of energy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot([p @ p for p in orbit])\npass"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ax = plt.subplot(111)\nplt.plot(orbit[:, 0], orbit[:,1], 'o')\nax.axis('square')\nplt.axis([-1.5, 1.5, -1.5, 1.5])\npass"
        }, 
        {
            "source": "#### Accuracy and conservation of energy\n\nWe can see that forward Euler is not very accurate and also does not preserve energy since the orbit spirals away from the unit circle.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### The trapezoidal method\n\nThe trapezoidal method uses the following scheme\n\n$$\n\\frac{u_{n+1} - u_n}{\\Delta t} = \\frac{1}{2}  ( A u_{n+1} + A u_{n})\n$$\n\nThis is an implicit scheme (because $u_{n+1}$ appears on the RHS) whose solution is\n\n$$\nu_{n+1} = \\left(I - \\frac{\\Delta t}{2} A \\right)^{-1} \\left(I + \\frac{\\Delta t}{2} A \\right) u_{n} = B u_n\n$$\n\nBy inspection, we see that the eigenvalues are the complex conjugates of\n\n$$\n\\frac{1 + \\frac{\\Delta t}{2} i}{1 - \\frac{\\Delta t}{2} i}\n$$\n\nwhose absolute value is 1 - hence, energy is conserved. If we expand the matrix $B$ using the geometric series and compare with the Taylor expansion, we see that the trapezoidal method has local truncation error $O(h^3)$ and hence accuracy $O(h^2)$, where $h$ is the time step.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def trapezoidal(A, u, N):\n    p = len(u)\n    orbit = np.zeros((N,p))\n\n    dt = 2*np.pi/N\n    for i in range(N):\n        u = la.inv(np.eye(p) - dt/2 * A) @ (np.eye(p) + dt/2 * A) @ u\n        orbit[i] = u\n    return orbit"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "A = np.array([[0,1],[-1,0]])\nu = np.array([1.0,0.0])\nN = 64\norbit = trapezoidal(A, u, N)"
        }, 
        {
            "source": "#### Accuracy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "la.norm(np.array([1.0,0.0]) - orbit[-1])"
        }, 
        {
            "source": "#### Conservation of energy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot([p @ p for p in orbit])\npass"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ax = plt.subplot(111)\nplt.plot(orbit[:, 0], orbit[:,1], 'o')\nax.axis('square')\nplt.axis([-1.5, 1.5, -1.5, 1.5])\npass"
        }, 
        {
            "source": "### The leapfrog method\n\nThe leapfrog method uses a second order difference to update $u_n$. The algorithm simplifies to the following explicit scheme:\n\n- First take one half-step for v\n- Then take a full step for u\n- Then take one final half step for v\n\nIt performs as well as the trapezoidal method, with the advantage of being an explicit scheme and cheaper to calculate, so the leapfrog method is used in HMC.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def leapfrog(A, u, N):\n    orbit = np.zeros((N,2))\n\n    dt = 2*np.pi/N\n    for i in range(N):\n        u[1] = u[1] + dt/2 * A[1] @ u\n        u[0] = u[0] + dt * A[0] @ u\n        u[1] = u[1] + dt/2 * A[1] @ u\n        orbit[i] = u\n    return orbit"
        }, 
        {
            "source": "#### If we don't care about the intermediate steps, it is more efficient to just take 1/2 steps at the beginning and end", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def leapfrog2(A, u, N):\n    dt = 2*np.pi/N\n\n    u[1] = u[1] + dt/2 * A[1] @ u\n    for i in range(N-1):\n        u[0] = u[0] + dt * A[0] @ u\n        u[1] = u[1] + dt * A[1] @ u\n\n    u[0] = u[0] + dt * A[0] @ u\n    u[1] = u[1] + dt/2 * A[1] @ u   \n    return u"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "A = np.array([[0,1],[-1,0]])\nu = np.array([1.0,0.0])\nN = 64"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "orbit = leapfrog(A, u, N)"
        }, 
        {
            "source": "#### Accuracy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "la.norm(np.array([1.0,0.0]) - orbit[-1])"
        }, 
        {
            "source": "#### Conservation of energy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.plot([p @ p for p in orbit])\npass"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ax = plt.subplot(111)\nplt.plot(orbit[:, 0], orbit[:,1], 'o')\nax.axis('square')\nplt.axis([-1.5, 1.5, -1.5, 1.5])\npass"
        }, 
        {
            "source": "## From Hamiltonians to probability distributions\n\nThe physical analogy considers the negative log likelihood of the target distribution $p(x)$ to correspond to a potential energy well, with a collection of particles moving on the surface of the well. The state of each particle is given only by its position and momentum (or velocity if we assume unit mass for each particle). In a Hamiltonian system, the total energy $H(x,, v) = U(x) + K(v)$ is conserved. From statistical mechanics, the probability of each state is related to the total energy of the system\n\n$$\n\\begin{align}\np(x, v) & \\propto e^{-H(x, v)} \\\\\n&= e^{-U(x) - K(v)} \\\\\n&= e^{-P(x)}e^{-K(v)} \\\\\n& \\propto p(x) \\, p(v)\n\\end{align}\n$$\n\nSince the joint distribution factorizes $p(x, v) = p(x)\\, p(v)$, we can select an initial random $v$ for a particle, numerically integrate using a finite difference method such as the leapfrog and then use the updated $x^*$ as the new proposal. The acceptance ratio for the new $x^*$ is\n\n$$\n\\frac{ e^{ -U(x^*)-K(v^*) }} { e^{-U(x)-K(v)} } = e^{U(x)-U(x^*)+K(x)-K(x^*)}\n$$\n\nIf our finite difference scheme was exact, the acceptance ration would be 1 since energy is conserved with Hamiltonian dynamics. However, as we have seen, the leapfrog method does not conserve energy perfectly and an accept/reject step is still needed.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Example of HMC\n\nWe will explore how HMC works when the target distribution is bivariate normal centered at zero\n\n$$\nx \\sim N(0, \\Sigma)\n$$\n\nIn practice of course, the target distribution will be the posterior distribution and depend on both data and distributional parameters.\n\nThe potential energy or negative log likelihood is proportional to\n$$\nU(x) = \\frac{x^T\\Sigma^{-1} x}{2}\n$$\n\nThe kinetic energy is given by\n$$\nK(v) = \\frac{v^T v}{2}\n$$ \n\nwhere the initial $v_0$ is chosen at random from the unit normal at each step.\n\nTo find the time updates, we use the Hamiltonian equations and find the first derivatives of total energy with respect to $x$ and $v$\n\n$$\n\\begin{align}\nx' &= \\frac{\\delta K}{\\delta v} &= v \\\\\nv' &= -\\frac{\\delta U}{\\delta x} &= -\\Sigma^{-1} x \\\\\n\\end{align}\n$$\n\ngiving us the block matrix\n\n$$\nA = \\pmatrix{0 & 1 \\\\ -\\Sigma^{-1} & 0}\n$$\n\nBy using the first derivatives, we are making use of the gradient information on the log posterior to guide the proposal distribution.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#### This is what the target distribution should look like", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import seaborn as sns\nsigma = np.array([[1,0.8],[0.8,1]])\nmu = np.zeros(2)\nys = np.random.multivariate_normal(mu, sigma, 1000)\nsns.kdeplot(ys)\nplt.axis([-3.5,3.5,-3.5,3.5])\npass"
        }, 
        {
            "source": "#### This is the HMC posterior", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def E(A, u0, v0, u, v):\n    \"\"\"Total energy.\"\"\"\n    return (u0 @ tau @ u0 + v0 @ v0) - (u @ tau@u + v @ v)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def leapfrog(A, u, v, h, N):\n    \"\"\"Leapfrog finite difference scheme.\"\"\"\n    v = v - h/2 * A @ u\n    for i in range(N-1):\n        u = u + h * v\n        v = v - h * A @ u\n\n    u = u + h * v\n    v = v - h/2 * A @ u\n\n    return u, v"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "niter = 100\nh = 0.01\nN = 100\n\ntau = la.inv(sigma)\n\norbit = np.zeros((niter+1, 2))\nu = np.array([-3,3])\norbit[0] = u\nfor k in range(niter):\n    v0 = np.random.normal(0,1,2)\n    u, v = leapfrog(tau, u, v0, h, N)\n\n    # accept-reject\n    u0 = orbit[k]\n    a = np.exp(E(A, u0, v0, u, v))\n    r = np.random.rand()\n\n    if r < a:\n        orbit[k+1] = u\n    else:\n        orbit[k+1] = u0"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sns.kdeplot(orbit)\nplt.plot(orbit[:,0], orbit[:,1], alpha=0.2)\nplt.scatter(orbit[:1,0], orbit[:1,1],  c='red', s=30)\nplt.scatter(orbit[1:,0], orbit[1:,1],  c=np.arange(niter)[::-1], cmap='Reds')\nplt.axis([-3.5,3.5,-3.5,3.5])\npass"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}