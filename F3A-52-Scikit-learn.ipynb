{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 04: Machine Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session F - Scikit-learn", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Pipeline\n## 1. Pipeline and FeatureUnion: combining estimators\n\n### Pipeline: chaining estimators\n<font color = 'blue'><b>Pipeline</b></font> can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. \n\n<br/><font color = 'blue'><b>Pipeline</b></font> serves two purposes here:\n\n<br/><font size = 2.5><b>Convenience and encapsulation</b></font></br>\n    <br/><font size = 2>You only have to call fit and predict once on your data to fit a whole sequence of estimators.</br></font>\n<br/><font size = 2.5><b>Joint parameter selection</b></font></br>\n    <br/><font size = 2> You can grid search over parameters of all estimators in the pipeline at once.</br></font>    \n<font size = 2.5><b>Safety</b></font></br>\n    <br/><font size = 2> Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.</br></font>       \n\n<font size = 2>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).</font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<b>Usage</b>\n\nThe Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\npipe "
        }, 
        {
            "source": "The utility function make_pipeline is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import Binarizer\nmake_pipeline(Binarizer(), MultinomialNB()) "
        }, 
        {
            "source": "The estimators of a pipeline are stored as a list in the steps attribute:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipe.steps[0]"
        }, 
        {
            "source": "and as a dict in named_steps:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipe.named_steps['reduce_dim']"
        }, 
        {
            "source": "Parameters of the estimators in the pipeline can be accessed using the <estimator>__<parameter> syntax:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipe.set_params(clf__C=10) "
        }, 
        {
            "source": "Attributes of named_steps map to keys, enabling tab completion in interactive environments:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipe.named_steps.reduce_dim is pipe.named_steps['reduce_dim']"
        }, 
        {
            "source": "This is particularly important for doing grid searches:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import GridSearchCV\nparam_grid = dict(reduce_dim__n_components=[2, 5, 10],\n                  clf__C=[0.1, 10, 100])\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)"
        }, 
        {
            "source": "Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to None:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.linear_model import LogisticRegression\nparam_grid = dict(reduce_dim=[None, PCA(5), PCA(10)],\n                  clf=[SVC(), LogisticRegression()],\n                  clf__C=[0.1, 10, 100])\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)"
        }, 
        {
            "source": "<b>Notes</b>\n\nCalling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<b>Caching transformers: avoid repeated computation</b>\n\nFitting transformers may be computationally expensive. With its memory parameter set, <font color = 'blue'>Pipeline</font> will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.\n\nThe parameter memory is needed in order to cache the transformers. memory can be either a string containing the directory where to cache the transformers or a <font color = 'blue'>joblib.Memory</font> object:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from tempfile import mkdtemp\nfrom shutil import rmtree\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\ncachedir = mkdtemp()\npipe = Pipeline(estimators, memory=cachedir)\npipe \n\n# Clear the cache directory when you don't need it anymore\nrmtree(cachedir)"
        }, 
        {
            "source": "<font color = 'red'>Warning: Side effect of caching transformers</font>\n<br/><font color = 'red'>Using a </font>Pipeline <font color = 'red'>without cache enabled, it is possible to inspect the original instance such as:</font></br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.datasets import load_digits\ndigits = load_digits()\npca1 = PCA()\nsvm1 = SVC()\npipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\npipe.fit(digits.data, digits.target)\n\n# The pca instance can be inspected directly\nprint(pca1.components_) "
        }, 
        {
            "source": "<font color = 'red'>Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. In following example, accessing the </font>PCA <font color = 'red'>instance </font>pca2 <font color = 'red'>will raise an </font>AttributeError <font color = 'red'>since</font> pca2 <font color = 'red'>will be an unfitted transformer. Instead, use the attribute </font>named_steps <font color = 'red'>to inspect estimators within the pipeline:</font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cachedir = mkdtemp()\npca2 = PCA()\nsvm2 = SVC()\ncached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n                       memory=cachedir)\ncached_pipe.fit(digits.data, digits.target)\n\nprint(cached_pipe.named_steps['reduce_dim'].components_)\n\n# Remove the cache directory\nrmtree(cachedir)"
        }, 
        {
            "source": "## 2. FeatureUnion: composite feature spaces\n\n<br/><font color = 'blue'><b>FeatureUnion</b></font> combines several transformer objects into a new transformer that combines their output. A <font color = 'blue'><b>FeatureUnion</b></font> takes a list of transformer objects. During fitting, each of these is fit to the data independently. For transforming data, the transformers are applied in parallel, and the sample vectors they output are concatenated end-to-end into larger vectors.</br>\n\n<br/><font color = 'blue'><b>FeatureUnion</b></font> serves the same purposes as <font color = 'blue'><b>Pipeline</b></font> - convenience and joint parameter estimation and validation.\n\n<br/><font color = 'blue'><b>FeatureUnion</b></font> and <font color = 'blue'><b>Pipeline</b></font> can be combined to create complex models.\n\n(A <font color = 'blue'>FeatureUnion</font>has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller\u2019s responsibility.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<b>Usage</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A <font color = 'blue'>FeatureUnion</font> is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\nestimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\ncombined = FeatureUnion(estimators)\ncombined "
        }, 
        {
            "source": "Like pipelines, feature unions have a shorthand constructor called <font color = 'blue'>make_union</font> that does not require explicit naming of the components.\n\nLike Pipeline, individual steps may be replaced using set_params, and ignored by setting to None:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "combined.set_params(kernel_pca=None)"
        }, 
        {
            "source": "# Grid-search\n## 1.Tuning the hyper-parameters of an estimator\nHyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n\nIt is possible and recommended to search the hyper-parameter space for the best <font color = 'blue'>cross validation</font> score.\n\nAny parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "estimator.get_params()", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "A search consists of:\n\n    * an estimator (regressor or classifier such as sklearn.svm.SVC());\n    * a parameter space;\n    * a method for searching or sampling candidates;\n    * a cross-validation scheme; and\n    * a score function.\n    \nSome models allow for specialized, efficient parameter search strategies, <font color = 'blue'>outlined below</font>. Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, <font color = 'blue'>GridSearchCV</font> exhaustively considers all parameter combinations, while <font color = 'blue'>RandomizedSearchCV</font> can sample a given number of candidates from a parameter space with a specified distribution. After describing these tools we detail best practice applicable to both approaches.\n\nNote that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Exhaustive Grid Search\n\nThe grid search provided by <font color = 'blue'>GridSearchCV</font> exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "param_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]"
        }, 
        {
            "source": "specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].\n\nThe <font color = 'blue'>GridSearchCV</font> instance implements the usual estimator API: when \u201cfitting\u201d it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Randomized Parameter Optimization\n\nWhile using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. \n\n</br>This has two main benefits over an exhaustive search:<br/>\n\n    A budget can be chosen independent of the number of parameters and possible values.\n    Adding parameters that do not influence the performance does not decrease efficiency.\n\nSpecifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for <font color = 'blue'>GridSearchCV</font>. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import scipy"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}"
        }, 
        {
            "source": "This example uses the scipy.stats module, which contains many useful distributions for sampling parameters, such as expon, gamma, uniform or randint. In principle, any function can be passed that provides a rvs (random variate sample) method to sample a value. A call to the rvs function should provide independent random samples from possible parameter values on consecutive calls.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Tips for parameter search\n#### Specifying an objective metric\n\nBy default, parameter search uses the score function of the estimator to evaluate a parameter setting. These are the <font color = 'blue'>sklearn.metrics.accuracy_score</font> for classification and <font color = 'blue'>sklearn.metrics.r2_score</font> for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the scoring parameter to <font color = 'blue'>GridSearchCV, RandomizedSearchCV</font> and many of the specialized cross-validation tools described below. See The scoring parameter: defining model evaluation rules for more details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "sklearn.metrics.accuracy_score", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\naccuracy_score(y_true, y_pred)\n\naccuracy_score(y_true, y_pred, normalize=False)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
        }, 
        {
            "source": "sklearn.metrics.r2_score ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.metrics import r2_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nr2_score(y_true, y_pred)  \n\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nr2_score(y_true, y_pred, multioutput='variance_weighted')\n\n\ny_true = [1,2,3]\ny_pred = [1,2,3]\nr2_score(y_true, y_pred)\n\ny_true = [1,2,3]\ny_pred = [2,2,2]\nr2_score(y_true, y_pred)\n\ny_true = [1,2,3]\ny_pred = [3,2,1]\nr2_score(y_true, y_pred)\n"
        }, 
        {
            "source": "GridSearchCV", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = svm.SVC()\nclf = GridSearchCV(svc, parameters)\nclf.fit(iris.data, iris.target)\n \nsorted(clf.cv_results_.keys())"
        }, 
        {
            "source": "ParameterSampler", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import ParameterSampler\nfrom scipy.stats.distributions import expon\nimport numpy as np\nnp.random.seed(0)\nparam_grid = {'a':[1, 2], 'b': expon()}\nparam_list = list(ParameterSampler(param_grid, n_iter=4))\nrounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n                for d in param_list]\nrounded_list == [{'b': 0.89856, 'a': 1},\n                 {'b': 0.923223, 'a': 1},\n                 {'b': 1.878964, 'a': 2},\n                 {'b': 1.038159, 'a': 2}]"
        }, 
        {
            "source": "#### Specifying multiple metrics for evaluation\n\nGridSearchCV and RandomizedSearchCV allow specifying multiple metrics for the scoring parameter.\n\nMultimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details.\n\nWhen specifying multiple metrics, the refit parameter must be set to the metric (string) for which the best_params_ will be found and used to build the best_estimator_ on the whole dataset. If the search should not be refit, set refit=False. Leaving refit to the default value None will result in an error when using multiple metrics.\n\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Multiple metric parameter search can be done by setting the scoring parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.\n\nThe scores of all the scorers are available in the cv_results_ dict at keys ending in '_&lt;scorer_name&gt;' ('mean_test_precision', 'rank_test_precision', etc\u2026)\n\nThe best_estimator_, best_index_, best_score_ and best_params_ correspond to the scorer (key) that is set to the refit attribute.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nprint(__doc__)"
        }, 
        {
            "source": "Running GridSearchCV using multiple evaluation metrics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X, y = make_hastie_10_2(n_samples=8000, random_state=42)\n\n# The scorers can be either be one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and\n# ``gs.best_index_``\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={'min_samples_split': range(2, 403, 10)},\n                  scoring=scoring, cv=5, refit='AUC')\ngs.fit(X, y)\nresults = gs.cv_results_"
        }, 
        {
            "source": "Plotting the result", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "plt.figure(figsize=(13, 13))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n          fontsize=16)\n\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_min_samples_split'].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), ['g', 'k']):\n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = results['mean_test_%s' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show()"
        }, 
        {
            "source": "#### Composite estimators and parameter spaces\n\nPipeline: chaining estimators describes building composite estimators whose parameter space can be searched with these tools.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Model selection: development and evaluation\n\nModel selection by evaluating various parameter settings can be seen as a way to use the labeled data to \u201ctrain\u201d the parameters of the grid.\n\nWhen evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics.\n\nThis can be done by using the train_test_split utility function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nX, y = np.arange(10).reshape((5, 2)), range(5)\nX\n\nlist(y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_train"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_test"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_test"
        }, 
        {
            "source": "#### Parallelism\n\n<font color = 'blue'>GridSearchCV</font> and <font color = 'blue'>RandomizedSearchCV</font> evaluate each parameter setting independently. Computations can be run in parallel if your OS supports it, by using the keyword n_jobs=-1. See function signature for more details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = svm.SVC()\nclf = GridSearchCV(svc, parameters)\nclf.fit(iris.data, iris.target)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sorted(clf.cv_results_.keys())"
        }, 
        {
            "source": "#### Robustness to failure\n\nSome parameter settings may result in a failure to fit one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting error_score=0 (or =np.NaN) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or NaN), but completing the search.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Alternatives to brute force parameter search\n\n#### Model specific cross-validation\n\nSome models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.\n\nThe most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the regularization path of the estimator.\n\n##### linear_model.ElasticNetCV", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.linear_model import ElasticNetCV\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_features=2, random_state=0)\nregr = ElasticNetCV(cv=5, random_state=0)\nregr.fit(X, y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(regr.alpha_) \n\nprint(regr.intercept_) \n\nprint(regr.predict([[0, 0]])) "
        }, 
        {
            "source": "##### linear_model.LassoCV", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Now use lars_path and 1D linear interpolation to compute the\n# same path\nfrom sklearn.linear_model import lars_path\nalphas, active, coef_path_lars = lars_path(X, y, method='lasso')\nfrom scipy import interpolate\ncoef_path_continuous = interpolate.interp1d(alphas[::-1],\n                                            coef_path_lars[:, ::-1])\nprint(coef_path_continuous([5., 1., .5]))"
        }, 
        {
            "source": "#### Information Criterion\n\nSome models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation).\n\nHere is the list of models benefitting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import linear_model\nreg = linear_model.LassoLarsIC(criterion='bic')\nreg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n\nprint(reg.coef_) "
        }, 
        {
            "source": "#### Out of Bag Estimates\n\nWhen using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.\n\nThis left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes \u201cfor free\u201d as no additional data is needed and can be used for model selection.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### ensemble.RandomForestClassifier", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=4,\n                           n_informative=2, n_redundant=0,\n                           random_state=0, shuffle=False)\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(clf.feature_importances_)\n\nprint(clf.predict([[0, 0, 0, 0]]))"
        }, 
        {
            "source": "##### ensemble.RandomForestRegressor", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_features=4, n_informative=2,\n                       random_state=0, shuffle=False)\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr.fit(X, y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(regr.feature_importances_)\n\nprint(regr.predict([[0, 0, 0, 0]]))"
        }, 
        {
            "source": "# Validation curves\n## Validation curves: plotting scores to evaluate models\n\n### Validation curve\n\nEvery estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The <b>bias</b> of an estimator is its average error for different training sets. The <b>variance</b> of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.\n\nIn the following plot, we see a function $f(x) = cos (\\frac{3}{2} \\pi x)$ and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).\n\nBias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see Bias-variance dilemma). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.\n\nIn the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Underfitting vs. Overfitting", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This example demonstrates the problems of underfitting and overfitting and\nhow we can use linear regression with polynomial features to approximate\nnonlinear functions. The plot shows the function that we want to approximate,\nwhich is a part of the cosine function. In addition, the samples from the\nreal function and the approximations of different models are displayed. The\nmodels have polynomial features of different degrees. We can see that a\nlinear function (polynomial with degree 1) is not sufficient to fit the\ntraining samples. This is called **underfitting**. A polynomial of degree 4\napproximates the true function almost perfectly. However, for higher degrees\nthe model will **overfit** the training data, i.e. it learns the noise of the\ntraining data.\nWe evaluate quantitatively **overfitting** / **underfitting** by using\ncross-validation. We calculate the mean squared error (MSE) on the validation\nset, the higher, the less likely the model generalizes correctly from the\ntraining data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Ridge\n\nnp.random.seed(0)\niris = load_iris()\nX, y = iris.data, iris.target\nindices = np.arange(y.shape[0])\nnp.random.shuffle(indices)\nX, y = X[indices], y[indices]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_scores, valid_scores = validation_curve(Ridge(), X, y, \"alpha\",\n                                              np.logspace(-7, 3, 3)) "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_scores "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "valid_scores"
        }, 
        {
            "source": "#### Plotting Validation Curves", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this plot you can see the training scores and validation scores of an SVM\nfor different values of the kernel parameter gamma. For very low values of\ngamma, you can see that both the training score and the validation score are\nlow. This is called underfitting. Medium values of gamma will result in high\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\nis too high, the classifier will overfit, which means that the training score\nis good but the validation score is poor.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    SVC(), X, y, param_name=\"gamma\", param_range=param_range,\n    cv=10, scoring=\"accuracy\", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title(\"Validation Curve with SVM\")\nplt.xlabel(\"$\\gamma$\")\nplt.ylabel(\"Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()"
        }, 
        {
            "source": "### Learning curve", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can use the function learning_curve to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\ntrain_sizes            \n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_scores           "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "valid_scores "
        }, 
        {
            "source": "#### Plotting Learning Curves", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "On the left side the learning curve of a naive Bayes classifier is shown for\nthe digits dataset. Note that the training score and the cross-validation score\nare both not very good at the end. However, the shape of the curve can be found\nin more complex datasets very often: the training score is very high at the\nbeginning and decreases and the cross-validation score is very low at the\nbeginning and increases. On the right side we see the learning curve of an SVM\nwith RBF kernel. We can see clearly that the training score is still around\nthe maximum and the validation score could be increased with more training\nsamples.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n\ntitle = \"Learning Curves (Naive Bayes)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n\nplt.show()"
        }, 
        {
            "source": "# Polynomial feature generation\n## Generating polynomial features", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Often it\u2019s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features\u2019 high-order and interaction terms. It is implemented in PolynomialFeatures:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nX = np.arange(6).reshape(3, 2)\nX                                                 "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "poly = PolynomialFeatures(2)\npoly.fit_transform(X)  "
        }, 
        {
            "source": "The features of X have been transformed from $(X_1, X_2)$ to $(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$.\n\nIn some cases, only interaction terms among features are required, and it can be gotten with the setting interaction_only=True:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X = np.arange(9).reshape(3, 3)\nX    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "poly = PolynomialFeatures(degree=3, interaction_only=True)\npoly.fit_transform(X) "
        }, 
        {
            "source": "The features of X have been transformed from $(X_1, X_2, X_3)$ to $(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Sample generators\n## Generators for classification and clustering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "These generators produce a matrix of features and corresponding discrete targets.\n### Single label", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\n\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\n\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. produces Gaussian data with a spherical decision boundary for binary classification.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### make_blobs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=10, centers=3, n_features=2,\n                  random_state=0)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(X.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y"
        }, 
        {
            "source": "Plot several randomly generated 2D classification datasets.\nThis example illustrates the :func:`datasets.make_classification`\n:func:`datasets.make_blobs` and :func:`datasets.make_gaussian_quantiles`\nfunctions.\n\nFor ``make_classification``, three binary and two multi-class classification\ndatasets are generated, with different numbers of informative features and\nclusters per class.  \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_gaussian_quantiles\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n\nplt.subplot(321)\nplt.title(\"One informative feature, one cluster per class\", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n            s=25, edgecolor='k')\n\nplt.subplot(322)\nplt.title(\"Two informative features, one cluster per class\", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n            s=25, edgecolor='k')\n\nplt.subplot(323)\nplt.title(\"Two informative features, two clusters per class\",\n          fontsize='small')\nX2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\nplt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n            s=25, edgecolor='k')\n\nplt.subplot(324)\nplt.title(\"Multi-class, two informative features, one cluster\",\n          fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n            s=25, edgecolor='k')\n\nplt.subplot(325)\nplt.title(\"Three blobs\", fontsize='small')\nX1, Y1 = make_blobs(n_features=2, centers=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n            s=25, edgecolor='k')\n\nplt.subplot(326)\nplt.title(\"Gaussian divided into three quantiles\", fontsize='small')\nX1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n            s=25, edgecolor='k')\n\nplt.show()"
        }, 
        {
            "source": "### Multilabel", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\n\n    * Per-topic word distributions are independently drawn,where in reality all would be affected by a sparse base distribution, and would be correlated.\n    * For a document generated from multiple topics,all topics are weighted equally in generating its bag of words.\n    * Documents without labels words at random,rather than from a base distribution", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This illustrates the `datasets.make_multilabel_classification` dataset\ngenerator. Each sample consists of counts of two features (up to 50 in\ntotal), which are differently distributed in each of two classes.\n\nPoints are labeled as follows, where Y means the class is present:\n\n    =====  =====  =====  ======\n      1      2      3    Color\n    =====  =====  =====  ======\n      Y      N      N    Red\n      N      Y      N    Blue\n      N      N      Y    Yellow\n      Y      Y      N    Purple\n      Y      N      Y    Orange\n      Y      Y      N    Green\n      Y      Y      Y    Brown\n    =====  =====  =====  ======\n\nA star marks the expected sample for each class; its size reflects the\nprobability of selecting that class label.\n\nThe left and right examples highlight the ``n_labels`` parameter:\nmore of the samples in the right plot have 2 or 3 labels.\n\nNote that this two-dimensional example is very degenerate:\ngenerally the number of features would be much greater than the\n\"document length\", while here we have much larger documents than vocabulary.\nSimilarly, with ``n_classes > n_features``, it is much less likely that a\nfeature distinguishes a particular class.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\n\nprint(__doc__)\n\nCOLORS = np.array(['!',\n                   '#FF3333',  # red\n                   '#0198E1',  # blue\n                   '#BF5FFF',  # purple\n                   '#FCD116',  # yellow\n                   '#FF7216',  # orange\n                   '#4DBD33',  # green\n                   '#87421F'   # brown\n                   ])\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = np.random.randint(2 ** 10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,\n                                   n_classes=n_classes, n_labels=n_labels,\n                                   length=length, allow_unlabeled=False,\n                                   return_distributions=True,\n                                   random_state=RANDOM_SEED)\n\n    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]\n                                                    ).sum(axis=1)),\n               marker='.')\n    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,\n               marker='*', linewidth=.5, edgecolor='black',\n               s=20 + 1500 * p_c ** 2,\n               color=COLORS.take([1, 2, 4]))\n    ax.set_xlabel('Feature 0 count')\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))\nplt.subplots_adjust(bottom=.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title('n_labels=1, length=50')\nax1.set_ylabel('Feature 1 count')\n\nplot_2d(ax2, n_labels=3)\nax2.set_title('n_labels=3, length=50')\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\nplt.show()\n\nprint('The data was generated from (random_state=%d):' % RANDOM_SEED)\nprint('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\\t')\nfor k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):\n    print('%s\\t%0.2f\\t%0.2f\\t%0.2f' % (k, p, p_w[0], p_w[1]))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "widgets": {
            "state": {}, 
            "version": "1.1.2"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}